{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq with PyTorch\n",
    "====\n",
    "\n",
    "\n",
    "Simple RNN Encoder-Decoder network.\n",
    "\n",
    "\n",
    "### `pip` requirements: \n",
    "\n",
    "```\n",
    "gensim>=3.7.0\n",
    "torch>=1.0.0\n",
    "nltk>=3.4.0\n",
    "tqdm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython candies...\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports we need.\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kopi Problems\n",
    "====\n",
    "\n",
    "In this hands-on session, we want to **train an neural network to translate from Singlish Kopi orders to English.**\n",
    "\n",
    "\n",
    "**\"Singlish\" -> English**\n",
    "\n",
    "```\n",
    "\"Kopi\" -> Coffee with condensed milk\n",
    "\"Kopi O\" -> Coffee without milk or sugar\n",
    "\"Kopi dinosaur gau siew dai peng\" -> ???\n",
    "```\n",
    "\n",
    "(Image Source: http://www.straitstimes.com/lifestyle/food/get-your-kopi-kick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://static.straitstimes.com.sg/sites/default/files/160522_kopi.jpg\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://static.straitstimes.com.sg/sites/default/files/160522_kopi.jpg\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seriously?\n",
    "----\n",
    "\n",
    "Yes, we'll be translating Singlish Kopi orders to English using the [sequence-to-sequence network](https://arxiv.org/abs/1409.3215) (Sutskever et al. 2014).\n",
    "\n",
    "But first...\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Munging\n",
    "\n",
    "\n",
    "Before any machine/deep learning, we have to get some data and \"hammer\" it until we get it into the shape we want.\n",
    "\n",
    "> *Data scientists spend 60% of their time on cleaning and organizing data. Collecting data sets comes second at 19% of their time, meaning data scientists spend around 80% of their time on preparing and managing data for analysis.*\n",
    "\n",
    "> (Source: [Gil Press](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#3e4dc0416f63) Forbes article)\n",
    "\n",
    "**Step 1:** Take the data from somewhere, in this case: http://kaggle.com/alvations/sg-kopi.\n",
    "\n",
    "**Step 2:** Import your favorite dataframe and text processing library.\n",
    "\n",
    "**Step 3:** Munge the data till desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Singlish</th>\n",
       "      <th>English</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kopi O</td>\n",
       "      <td>Black Coffee with Sugar</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kopi</td>\n",
       "      <td>Black Coffee with Condensed Milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kopi C</td>\n",
       "      <td>Black Coffee with Evaporated Milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kopi Kosong</td>\n",
       "      <td>Black Coffee without sugar or milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kopi Gah Dai</td>\n",
       "      <td>Black Coffee with extra condensed milk</td>\n",
       "      <td>https://daneshd.com/2010/02/28/a-rough-guide-t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Singlish                                 English  \\\n",
       "0        Kopi O                 Black Coffee with Sugar   \n",
       "1          Kopi        Black Coffee with Condensed Milk   \n",
       "2        Kopi C       Black Coffee with Evaporated Milk   \n",
       "3   Kopi Kosong      Black Coffee without sugar or milk   \n",
       "4  Kopi Gah Dai  Black Coffee with extra condensed milk   \n",
       "\n",
       "                                              Source  \n",
       "0  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "1  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "2  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "3  https://daneshd.com/2010/02/28/a-rough-guide-t...  \n",
       "4  https://daneshd.com/2010/02/28/a-rough-guide-t...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Reads the tab-delimited data using Pandas.\n",
    "kopitiam = pd.read_csv('kopitiam.csv', sep=',')\n",
    "kopitiam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping the Data and Adding START and END Symbols\n",
    "\n",
    "To get the data in shape, we want to:\n",
    "  \n",
    "  1. normalize and tokenize of the input\n",
    "  2. pad the input with START (`<s>`) and END (`</s>`) symbols.\n",
    "\n",
    "If we look at the data carefully, sometimes we see that we have a mix of capitalized and lowered cased spellings, esp. in the \"Local Terms\" column. E.g. \"Kopi O\" and \"Kopi o\". For simplicity, we'll lowercase all the inputs and outputs so that our models don't think that the big \"O\" and the small \"o\" are different things.\n",
    "\n",
    "Additionally, we want to tokinze our input so that we pad the punctuations with spaces away from the preceeding or following word. There are many tokenization functions, we'll use the `word_tokenize()` function in `nltk`.\n",
    "\n",
    "As for padding the sentence with START and END symbols. It's an indication that we give to our Recurrent Neural Network (RNN) that denotes the start/end of our in/output sequences. \n",
    "\n",
    "\n",
    "(**Cut-away:** Here's some experts pitching in on why we need the START/END symbol. https://twitter.com/alvations/status/955770616648364037) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a unique string to indicate START and END of a sentence.\n",
    "# Assign a unique index to them.\n",
    "START, START_IDX = '<s>',  0\n",
    "END, END_IDX = '</s>', 1\n",
    "\n",
    "# We use this idiom to tokenize our sentences in the dataframe column:\n",
    "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
    "\n",
    "# Also we added the START and the END symbol to the sentences. \n",
    "singlish_sents = [START] + kopitiam['Singlish'].apply(str.lower).apply(word_tokenize) + [END]\n",
    "english_sents = [START] + kopitiam['English'].apply(str.lower).apply(word_tokenize) + [END]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Singlish sentence:\t ['<s>', 'kopi', 'o', '</s>']\n",
      "First English sentence:\t\t ['<s>', 'black', 'coffee', 'with', 'sugar', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# We're sort of getting into the data into the shape we want. \n",
    "# But now it's still too humanly readable and redundant.\n",
    "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
    "print('First Singlish sentence:\\t', singlish_sents[0])\n",
    "print('First English sentence:\\t\\t', english_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the PyTorch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KopitiamDataset(Dataset):\n",
    "    def __init__(self, src_sents, trg_sents, max_len=-1):\n",
    "        assert len(src_sents) == len(trg_sents), \"There should be the same no. of sentence for both source and target.\"\n",
    "        self.src_sents = src_sents\n",
    "        self.trg_sents = trg_sents\n",
    "\n",
    "        # Create the vocabulary for both the source and target.\n",
    "        self.src_vocab = Dictionary(src_sents)\n",
    "        self.trg_vocab = Dictionary(trg_sents)\n",
    "        \n",
    "        # Patch the vocabularies and add the <pad> and <unk> symbols.\n",
    "        special_tokens = {'<pad>': 0, '<unk>':1, '<s>':2, '</s>':3}\n",
    "        self.src_vocab.patch_with_special_tokens(special_tokens)\n",
    "        self.trg_vocab.patch_with_special_tokens(special_tokens)\n",
    "        \n",
    "        # Keep track of how many data points.\n",
    "        self._len = len(src_sents)\n",
    "        \n",
    "        if max_len < 0:\n",
    "            # If it's not set, find the longest text in the data.\n",
    "            max_src_len = max(len(sent) for sent in src_sents)\n",
    "            max_trg_len = max(len(sent) for sent in trg_sents)\n",
    "            self.max_len = max(max_src_len, max_trg_len)\n",
    "        else:\n",
    "            self.max_len = max_len\n",
    "        \n",
    "    def pad_sequence(self, vectorized_sent, max_len):\n",
    "        # To pad the sentence:\n",
    "        # Pad left = 0; Pad right = max_len - len of sent.\n",
    "        pad_dim = (0, max_len - len(vectorized_sent))\n",
    "        return F.pad(vectorized_sent, pad_dim, 'constant')\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        vectorized_src = self.vectorize(self.src_vocab, self.src_sents[index])\n",
    "        vectorized_trg = self.vectorize(self.trg_vocab, self.trg_sents[index])\n",
    "        \n",
    "        return {'x':self.pad_sequence(vectorized_src, self.max_len), \n",
    "                'y':self.pad_sequence(vectorized_trg, self.max_len), \n",
    "                'x_len':len(vectorized_src), \n",
    "                'y_len':len(vectorized_trg)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, vocab, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        # Lets just cast list of indices into torch tensors directly =)\n",
    "        return torch.tensor(vocab.doc2idx(tokens))\n",
    "    \n",
    "    def unvectorize(self, vocab, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kopi_data = KopitiamDataset(singlish_sents, english_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[ 2, 68, 10, 11,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [ 2, 13, 14,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [ 2, 13,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0]]), 'y': tensor([[  2,  21,  22, 116,   5,   6,   7,   3,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0],\n",
      "        [  2,  74,  92,  29, 115, 114,   3,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0],\n",
      "        [  2,  24,  29,   5,   6,   7,  23,   4,   3,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0]]), 'x_len': tensor([5, 4, 3]), 'y_len': tensor([8, 7, 9])}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "dataloader = DataLoader(dataset=kopi_data, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True)\n",
    "\n",
    "def sort_batch_by_len(data_dict):\n",
    "    # Sort indices of data in batch by lengths.\n",
    "    sorted_indices = np.array(data_dict['x_len']).argsort()[::-1].tolist()\n",
    "    data_batch = {name:_tensor[sorted_indices]\n",
    "                  for name, _tensor in data_dict.items()}\n",
    "    return data_batch\n",
    "\n",
    "for data_dict in dataloader:\n",
    "    data_batch = sort_batch_by_len(data_dict)\n",
    "    print(data_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Matrix Manipulation Magic\n",
    "\n",
    "Normally for a feed-forward network, we pad our variable length inputs so that the matrix operations can be done in the usual manner with fixed length matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !! All the variables start with 2 and end with 3 (start and stop symbol for RNN) !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, we have tensors with variable lengths.\n",
    "t1 = torch.tensor([2, 13, 5, 8, 3]).float()\n",
    "t2 = torch.tensor([2, 10, 3]).float()\n",
    "t3 = torch.tensor([2, 10, 1, 3]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 5 and 3 in dimension 1 at /Users/soumith/mc3build/conda-bld/pytorch_1549597882250/work/aten/src/TH/generic/THTensorMoreMath.cpp:1307",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-81fe738ed39f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If the dimensions are different, you can't even stack them into a matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 5 and 3 in dimension 1 at /Users/soumith/mc3build/conda-bld/pytorch_1549597882250/work/aten/src/TH/generic/THTensorMoreMath.cpp:1307"
     ]
    }
   ],
   "source": [
    "# If the dimensions are different, you can't even stack them into a matrix.\n",
    "torch.stack([t1, t2, t3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gotcha! You have to pad the tensors before stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., 13.,  5.,  8.,  3.],\n",
       "        [ 2., 10.,  3.,  0.,  0.],\n",
       "        [ 2., 10.,  1.,  3.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we have to pad them, e.g. \n",
    "_max_len = max(len(t1), len(t2), len(t3))\n",
    "\n",
    "t1 = F.pad(t1, (0, _max_len-len(t1)), 'constant')\n",
    "t2 = F.pad(t2, (0, _max_len-len(t2)), 'constant')\n",
    "t3 = F.pad(t3, (0, _max_len-len(t3)), 'constant')\n",
    "\n",
    "torch.stack([t1, t2, t3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, there's a way to handle the variable length when we try to send mini-batches of tensors to the layers (i.e. `nn.Module` objects).\n",
    "\n",
    "To do so, we use the `PackedSequence` object. We can use the `torch.nn.utils.rnn.pack_padded_sequence` function, to create the `PackedSequence` object from a normal fixed size matrix with padded zeros.\n",
    "\n",
    "Lets see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2, 68, 67,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0],\n",
       "        [ 2, 68,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0],\n",
       "        [ 2, 68,  4,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "dataloader = DataLoader(dataset=kopi_data, batch_size=batch_size, \n",
    "                        shuffle=False)\n",
    "\n",
    "# Hack to make dataloader give us the first batch.\n",
    "data_batch = next(iter(dataloader)) \n",
    "\n",
    "data_batch['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch['x_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "'lengths' array has to be sorted in decreasing order",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0f3409590a06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m packed_tensor = pack_padded_sequence(data_batch['x'],\n\u001b[1;32m      2\u001b[0m                                      \u001b[0mdata_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                      batch_first=True)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpacked_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first)\u001b[0m\n\u001b[1;32m    146\u001b[0m                       category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[1;32m    147\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 'lengths' array has to be sorted in decreasing order"
     ]
    }
   ],
   "source": [
    "packed_tensor = pack_padded_sequence(data_batch['x'],\n",
    "                                     data_batch['x_len'], \n",
    "                                     batch_first=True)\n",
    "packed_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gotcha! There's a caveat to `pack_padded_sequence`\n",
    "\n",
    "The batch of tensors that you send to `pack_padded_sequence` needs to be sorted in descreasing length first. Ah ha! Now we know why we were always sorting the batches when we use the dataloader =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([ 2,  2,  2, 68, 13, 13, 10, 14,  3, 11,  3,  3]), batch_sizes=tensor([3, 3, 3, 2, 1]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "dataloader = DataLoader(dataset=kopi_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def sort_batch_by_len(data_batch):\n",
    "    # Sort indices of data in batch by lengths.\n",
    "    sorted_indices = np.array(data_dict['x_len']).argsort()[::-1].tolist()\n",
    "    data_batch = {name:_tensor[sorted_indices]\n",
    "                  for name, _tensor in data_dict.items()}\n",
    "    return data_batch\n",
    "\n",
    "\n",
    "# Hack to make dataloader give us the first batch.\n",
    "data_batch = next(iter(dataloader)) \n",
    "\n",
    "# Apply the `pack_padded_sequence` to the batch.\n",
    "data_batch = sort_batch_by_len(data_batch)\n",
    "packed_tensor = pack_padded_sequence(data_batch['x'], data_batch['x_len'], batch_first=True)\n",
    "packed_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional]: Sorting tensor by value\n",
    "\n",
    "Here's some cut-away tensor kungfu (i.e. tensor-fu) to understand how we got the `sorted_indices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_padded_x_tensors = tensor(\n",
    "    [[ 2, 13,  6,  3,  0,  0,  0,  0,  0],\n",
    "     [ 2, 68,  3,  0,  0,  0,  0,  0,  0],\n",
    "     [ 2, 68, 67,  5,  3,  0,  0,  0,  0]\n",
    "    ])\n",
    "\n",
    "_tensor_lens = [4, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, to perform any operations within the tensors, \n",
    "# it's easiest to cast them into numpy arrays\n",
    "np.array(_tensor_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the `.argsort()` function in the numpy array\n",
    "# would return an array of the indices sorted by their\n",
    "# values in ascending order.\n",
    "np.array(_tensor_lens).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But for the `pack_padded_sequence`, we want our \n",
    "# tensor lengths to be sorted in a descending order, \n",
    "# so we do a reverse.\n",
    "\n",
    "# Normally you can use the native python `reversed()` \n",
    "# function and the idiom looks as below, but that will\n",
    "# lose the np.array object\n",
    "list(reversed(np.array(_tensor_lens).argsort()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To keep the np.array object, \n",
    "# we can use the [::-1] slice notion to reverse the array.\n",
    "# See https://stackoverflow.com/a/31633656/610569 \n",
    "np.array(_tensor_lens).argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, keeping the array as a numpy object can't \n",
    "# be used to change the order of the _padded_x_tensors\n",
    "# so we still have to cast it to a list to reorder \n",
    "# the _padded_x_tensors by descending length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 5]\n",
      "tensor([[ 2, 13,  6,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2, 68,  3,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 68, 67,  5,  3,  0,  0,  0,  0]])\n",
      "\n",
      "[5, 4, 3]\n",
      "tensor([[ 2, 68, 67,  5,  3,  0,  0,  0,  0],\n",
      "        [ 2, 13,  6,  3,  0,  0,  0,  0,  0],\n",
      "        [ 2, 68,  3,  0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "# Sort the indices by descending order.\n",
    "sorted_indices = np.array(_tensor_lens).argsort()[::-1].tolist()\n",
    "# Use the slice notation on the tensor to reorder the tensor.\n",
    "print(_tensor_lens)\n",
    "print(_padded_x_tensors)\n",
    "print()\n",
    "print(sorted(_tensor_lens, reverse=True))\n",
    "print(_padded_x_tensors[sorted_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undoing the PackedSequence\n",
    "\n",
    "If for some reason you'll need to undo the packing for the `PackedSeqeunce` object, you can use the `pad_packed_sequence` function, e.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2,  2,  2],\n",
       "         [68, 13, 13],\n",
       "         [10, 14,  3],\n",
       "         [11,  3,  0],\n",
       "         [ 3,  0,  0]]), tensor([5, 4, 3]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you didn't specify the batch_first argument, you see that our tensors got transposed.\n",
    "pad_packed_sequence(packed_tensor, batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2, 68, 10, 11,  3],\n",
       "        [ 2, 13, 14,  3,  0],\n",
       "        [ 2, 13,  3,  0,  0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get back the original tensor before packing, we set batch_first=True.\n",
    "unpacked_tensor, unpacked_tensor_len = pad_packed_sequence(packed_tensor, batch_first=True)\n",
    "unpacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 4, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked_tensor_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Seq2Seq Model\n",
    "\n",
    "\n",
    "A Recurrent Neural Network (RNN), is a network that operates on a sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "> *The general idea is to make **two recurrent neural network transform from one sequence to another**. An encoder network condenses an input sequence into a vector and a decoder netwrok unfolds the vector into a new sequence.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Encoder\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.\n",
    "\n",
    "\n",
    "<img src=\"http://pytorch.org/tutorials/_images/encoder-network.png\" align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # Set the no. of nodes for the hidden layer.\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of input (i.e. no. of words in input vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size,\n",
    "                                      padding_idx=0)\n",
    "        # Initialize the GRU with the \n",
    "        # - size of the hidden layer from the previous state\n",
    "        # - size of the hidden layer from the current state\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    " \n",
    "    def forward(self, inputs, inputs_lengths):\n",
    "        # Feed the input into the embedding layer.\n",
    "        embedded = self.embedding(inputs)\n",
    "        # Create PackedSequence\n",
    "        lengths = inputs_lengths.detach().cpu().numpy()\n",
    "        embedded_packed = pack_padded_sequence(embedded, lengths, \n",
    "                                               batch_first=True)\n",
    "        # Feed the embedded layer with the hidden layer to the GRU.\n",
    "        # Update the output and hidden layer.\n",
    "        output, hidden = self.gru(embedded_packed)\n",
    "        return output, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Decoder\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string `<s>` token, and the first hidden state is the context vector (the encoderâ€™s last hidden state).\n",
    "\n",
    "\n",
    "<img src=\"http://pytorch.org/tutorials/_images/decoder-network.png\" align='left'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # Set the no. of nodes for the hidden layer.\n",
    "        self.hidden_size = hidden_size\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of output (i.e. no. of words in output vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size,\n",
    "                                     padding_idx=0)\n",
    "        # Initialize the GRU with the \n",
    "        # - size of the hidden layer from the previous state\n",
    "        # - size of the hidden layer from the current state\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        # Initialize the \"classifier\" linear layer.\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        # Set the output layer to output a specific symbol \n",
    "        # from the output vocabulary\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input.size(0)\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = embedded.permute(1, 0, 2) # S=1 x B x N\n",
    "        # Transform the embedded output with a relu function. \n",
    "        output = F.relu(embedded)\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # Take the updated output and find the most appropriate\n",
    "        # output symbol. \n",
    "        output = self.softmax(self.out(output).squeeze(0))\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walking through the tensors \n",
    "\n",
    "Before we move on with the training data, we should take a look at the coolest feature of PyTorch (aka Tensorflow Eager mode way before eager mode is a thing). \n",
    "\n",
    "The fact that we can hijack the training process and start printing out layer output values or current parameters without needing to wait till the end of the training is pretty powerful. \n",
    "\n",
    "This is an artifact of how the PyTorch library designer allows users to probe and change the network at any point of time without first declaring and fixing a specific network. \n",
    "\n",
    "**Cut-away:** Here's some heated blogpost of imperative/declarative programming style.\n",
    "\n",
    "- Funny yet Informative: https://tylermcginnis.com/imperative-vs-declarative-programming/\n",
    "\n",
    "- Simple Python is de facto imperative but it's possible to do otherwise: http://www.benfrederickson.com/python-as-a-declarative-programming-language/\n",
    "\n",
    "<!--\n",
    "Pardon me being frank (not hotdog)\n",
    "----\n",
    "\n",
    "IMHO, I (Liling) don't really care how I write my network as long as the library allows me to have flexibility to alter networks and training mechanisms to suit what I'm trying to do. And for now, I write less code to do the same thing in PyTorch, so yeah... -->\n",
    "\n",
    "----\n",
    "\n",
    "Lets start from feeding a batch to an encoder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data.\n",
    "batch_size = 3\n",
    "kopi_data = KopitiamDataset(singlish_sents, english_sents)\n",
    "dataloader = DataLoader(dataset=kopi_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the network for encoder and decoder.\n",
    "hidden_size = 7\n",
    "_encoder = EncoderRNN(len(kopi_data.src_vocab), hidden_size)\n",
    "_decoder = DecoderRNN(hidden_size, len(kopi_data.trg_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_batch = sort_batch_by_len(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2, 68, 10, 11,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0],\n",
       "         [ 2, 13, 14,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0],\n",
       "         [ 2, 13,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0]]), tensor([5, 4, 3]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data_batch['x'], _data_batch['x_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 19])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The input is of shape:\n",
    "# batch_size * max_len\n",
    "_data_batch['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[-0.4365,  0.0405,  0.8090,  0.2884, -0.4206,  0.5598,  0.1304],\n",
       "         [-0.4365,  0.0405,  0.8090,  0.2884, -0.4206,  0.5598,  0.1304],\n",
       "         [-0.4365,  0.0405,  0.8090,  0.2884, -0.4206,  0.5598,  0.1304],\n",
       "         [-0.4013, -0.2246,  0.7697,  0.4100, -0.2742, -0.2106,  0.2832],\n",
       "         [ 0.1244,  0.1659,  0.8027,  0.5737, -0.0861,  0.2937,  0.2082],\n",
       "         [ 0.1244,  0.1659,  0.8027,  0.5737, -0.0861,  0.2937,  0.2082],\n",
       "         [-0.4328,  0.2636,  0.4947,  0.3177,  0.1898, -0.0414,  0.3013],\n",
       "         [ 0.1832,  0.0478,  0.7031,  0.4738, -0.2603,  0.2286,  0.3365],\n",
       "         [-0.4585,  0.1465,  0.7847,  0.2961, -0.3765,  0.1915,  0.2410],\n",
       "         [ 0.4353, -0.0459, -0.2597,  0.0734, -0.1335, -0.2561,  0.4762],\n",
       "         [-0.4229,  0.0814,  0.7481,  0.2363, -0.4675,  0.1786,  0.3419],\n",
       "         [-0.2900,  0.1559,  0.3424,  0.0213, -0.3576,  0.0399,  0.4032]],\n",
       "        grad_fn=<CatBackward>), batch_sizes=tensor([3, 3, 3, 2, 1])),\n",
       " tensor([[[-0.2900,  0.1559,  0.3424,  0.0213, -0.3576,  0.0399,  0.4032],\n",
       "          [-0.4229,  0.0814,  0.7481,  0.2363, -0.4675,  0.1786,  0.3419],\n",
       "          [-0.4585,  0.1465,  0.7847,  0.2961, -0.3765,  0.1915,  0.2410]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_encoder(_data_batch['x'], _data_batch['x_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.4365,  0.0405,  0.8090,  0.2884, -0.4206,  0.5598,  0.1304],\n",
       "        [-0.4365,  0.0405,  0.8090,  0.2884, -0.4206,  0.5598,  0.1304],\n",
       "        [-0.4365,  0.0405,  0.8090,  0.2884, -0.4206,  0.5598,  0.1304],\n",
       "        [-0.4013, -0.2246,  0.7697,  0.4100, -0.2742, -0.2106,  0.2832],\n",
       "        [ 0.1244,  0.1659,  0.8027,  0.5737, -0.0861,  0.2937,  0.2082],\n",
       "        [ 0.1244,  0.1659,  0.8027,  0.5737, -0.0861,  0.2937,  0.2082],\n",
       "        [-0.4328,  0.2636,  0.4947,  0.3177,  0.1898, -0.0414,  0.3013],\n",
       "        [ 0.1832,  0.0478,  0.7031,  0.4738, -0.2603,  0.2286,  0.3365],\n",
       "        [-0.4585,  0.1465,  0.7847,  0.2961, -0.3765,  0.1915,  0.2410],\n",
       "        [ 0.4353, -0.0459, -0.2597,  0.0734, -0.1335, -0.2561,  0.4762],\n",
       "        [-0.4229,  0.0814,  0.7481,  0.2363, -0.4675,  0.1786,  0.3419],\n",
       "        [-0.2900,  0.1559,  0.3424,  0.0213, -0.3576,  0.0399,  0.4032]],\n",
       "       grad_fn=<CatBackward>), batch_sizes=tensor([3, 3, 3, 2, 1]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_encoder(_data_batch['x'], _data_batch['x_len'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12)\n",
      "7\n",
      "torch.Size([12, 7])\n"
     ]
    }
   ],
   "source": [
    "# The sum of all tensors' lengths varies in every batch\n",
    "print(sum(_encoder(_data_batch['x'], _data_batch['x_len'])[0].batch_sizes))\n",
    "\n",
    "# The hidden dimension of the encoder. \n",
    "print(_encoder.hidden_size)\n",
    "\n",
    "# The output size of the tensor in the PackedSequence is\n",
    "# sum of tensors' lengths* hidden_dim\n",
    "print(_encoder(_data_batch['x'], _data_batch['x_len'])[0].data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the batch into encoder.\n",
    "_packed_encoder_output, _encoder_hidden = _encoder(_data_batch['x'], _data_batch['x_len'])\n",
    "\n",
    "# Unpack the PackedSequence.\n",
    "_encoder_output, _encoder_input_lens = pad_packed_sequence(_packed_encoder_output, \n",
    "                                                           batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.4365,  0.0405,  0.8090,  0.2884, -0.4206,  0.5598,  0.1304],\n",
       "        [-0.4365,  0.0405,  0.8090,  0.2884, -0.4206,  0.5598,  0.1304],\n",
       "        [-0.4365,  0.0405,  0.8090,  0.2884, -0.4206,  0.5598,  0.1304],\n",
       "        [-0.4013, -0.2246,  0.7697,  0.4100, -0.2742, -0.2106,  0.2832],\n",
       "        [ 0.1244,  0.1659,  0.8027,  0.5737, -0.0861,  0.2937,  0.2082],\n",
       "        [ 0.1244,  0.1659,  0.8027,  0.5737, -0.0861,  0.2937,  0.2082],\n",
       "        [-0.4328,  0.2636,  0.4947,  0.3177,  0.1898, -0.0414,  0.3013],\n",
       "        [ 0.1832,  0.0478,  0.7031,  0.4738, -0.2603,  0.2286,  0.3365],\n",
       "        [-0.4585,  0.1465,  0.7847,  0.2961, -0.3765,  0.1915,  0.2410],\n",
       "        [ 0.4353, -0.0459, -0.2597,  0.0734, -0.1335, -0.2561,  0.4762],\n",
       "        [-0.4229,  0.0814,  0.7481,  0.2363, -0.4675,  0.1786,  0.3419],\n",
       "        [-0.2900,  0.1559,  0.3424,  0.0213, -0.3576,  0.0399,  0.4032]],\n",
       "       grad_fn=<CatBackward>), batch_sizes=tensor([3, 3, 3, 2, 1]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_packed_encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_encoder_hidden.squeeze(0).squeeze(0).squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the decoder to start with the <s> symbol.\n",
    "decoder_input = kopi_data.vectorize(kopi_data.trg_vocab, ['<s>'])\n",
    "# Create no. of batches * the initial_state.\n",
    "first_state = torch.stack([_decoder.embedding(decoder_input)] * batch_size).permute(1, 0, 2)\n",
    "\n",
    "# Put the first state and the output of the encoder into the \n",
    "# GRU layer of the decoder.\n",
    "_decoder_out, _decoder_hidden = _decoder.gru(first_state, _encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.6609,  0.3348, -0.0317, -0.0727,  0.0593,  0.2016, -0.1152],\n",
       "          [-0.6783,  0.2977,  0.1865,  0.0786,  0.0873,  0.2576, -0.1324],\n",
       "          [-0.6877,  0.3536,  0.2014,  0.1250,  0.1201,  0.2556, -0.1895]]],\n",
       "        grad_fn=<StackBackward>),\n",
       " tensor([[[-0.6609,  0.3348, -0.0317, -0.0727,  0.0593,  0.2016, -0.1152],\n",
       "          [-0.6783,  0.2977,  0.1865,  0.0786,  0.0873,  0.2576, -0.1324],\n",
       "          [-0.6877,  0.3536,  0.2014,  0.1250,  0.1201,  0.2556, -0.1895]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_decoder_out, _decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 118])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(_decoder.out(_decoder_out).squeeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kopi_data.trg_vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'topping'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = int(torch.max(F.softmax(_decoder.out(_decoder_out).squeeze(0), dim=1), 1)[1][0])\n",
    "kopi_data.trg_vocab[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6696,  0.5149, -0.6238,  1.4911, -1.9001,  0.5816, -1.4319],\n",
       "         [-0.6696,  0.5149, -0.6238,  1.4911, -1.9001,  0.5816, -1.4319],\n",
       "         [-0.6696,  0.5149, -0.6238,  1.4911, -1.9001,  0.5816, -1.4319]]],\n",
       "       grad_fn=<PermuteBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = kopi_data.vectorize(kopi_data.trg_vocab, ['<s>'])\n",
    "_decoder.embedding(torch.stack([decoder_input] * batch_size)).permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To sum up the above, if we have a batch,\n",
    "# first put it into the encoder:\n",
    "_data_batch['x'], _data_batch['x_len']\n",
    "_packed_encoder_output, _encoder_hidden = _encoder(_data_batch['x'], _data_batch['x_len'])\n",
    "\n",
    "# Then create the initial state for the decoder.\n",
    "batch_size = _encoder_hidden.size(1) # Assumes `batch_first=True`\n",
    "decoder_input = kopi_data.vectorize(kopi_data.trg_vocab, ['<s>'])\n",
    "first_state = torch.stack([decoder_input] * batch_size)\n",
    "\n",
    "# Put the first state and the output of the encoder into the \n",
    "# GRU layer of the decoder.\n",
    "_decoder_out, _decoder_hidden = _decoder(first_state, _encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-4.2356, -4.2476, -4.2566], grad_fn=<MaxBackward0>),\n",
       " tensor([37, 37, 37]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally to get the predictions.\n",
    "torch.max(_decoder_out.squeeze(0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training the Model\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the `<s>` token as its first input, and the last hidden state of the encoder as its first hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As with all gradient methods in deep/machine learning, the basic idea is to:\n",
    "\n",
    " 1.\tInitialize the models and hyperparameters\n",
    " 2.\tIterate through the data points (Xs and Ys)\n",
    " 3.\tClear the gradient outputs\n",
    " 4.\tForward propagate, i.e. apply the model to the inputs to generate the predictions\n",
    " 5.\tCompute the loss for the predictions against the truth value\n",
    " 6.\tBackpropagate, i.e. compute the partial gradients \n",
    " 7.\tOptimizers update the model parameters\n",
    "\n",
    " \n",
    "**Note:** If you're unfamiliar with the steps above, I strongly encourage you to:\n",
    "\n",
    " - Watch @sirajraval on https://www.youtube.com/embed/q555kfIFUCM and \n",
    " - Spend some time going through this blogpost by @iamtrask http://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Forcing\n",
    "Teacher forcing is a method for quickly and efficiently training recurrent neural network models that use the output from a prior time step as input.\n",
    "\n",
    "It is a network training method critical to the development of deep learning language models used in machine translation, text summarization, and image captioning, among many other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the models and hyperparameters\n",
    "hidden_size = 1000\n",
    "learning_rate=0.03\n",
    "num_epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "MAX_LENGTH=20\n",
    "batch_size=5\n",
    "teacher_forcing_ratio = 0.3\n",
    "\n",
    "# Initialize the data.\n",
    "kopi_data = KopitiamDataset(singlish_sents, english_sents)\n",
    "dataloader = DataLoader(dataset=kopi_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the network for encoder and decoder.\n",
    "encoder = EncoderRNN(len(kopi_data.src_vocab), hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, len(kopi_data.trg_vocab))\n",
    "\n",
    "# Initialize the optimizer for encoder and decoder.\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:09<00:00,  3.66it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.31059265136719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:09<00:00,  3.50it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.54841995239258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.30it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.384401321411133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.29it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.236000061035156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:09<00:00,  3.40it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.43239688873291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.27it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.792956829071045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.30it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.849227905273438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:09<00:00,  3.40it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.189157485961914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:09<00:00,  3.41it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.14788818359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.36it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.853139877319336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.37it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.06199073791504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.25it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.195402145385742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.28it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.504268646240234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.26it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.954922676086426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.34it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.549238204956055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.25it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.410516738891602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.22it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9072847366333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:11<00:00,  3.04it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.712801218032837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.20it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.26157569885254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.24it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.422495365142822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.31it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.553847551345825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.29it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.89777946472168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.24it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.088063716888428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.24it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9115781784057617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.34it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1534504890441895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.31it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.851185321807861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:09<00:00,  3.42it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.324915885925293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.39it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0603291988372803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.16it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0293570756912231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.24it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.23015785217285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:11<00:00,  3.02it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.049266815185547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:11<00:00,  3.06it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.587512969970703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.33it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.749019622802734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.35it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.604101181030273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.28it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.611981391906738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.37it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.800962448120117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.37it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8060235977172852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.31it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0529727935791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.31it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.794147491455078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.23it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.160582542419434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.31it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.6376848220825195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.25it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.763915061950684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:09<00:00,  3.43it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3903098106384277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.39it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.150617599487305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.36it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800606966018677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.14it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.763169288635254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.14it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.74531364440918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.24it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.72944450378418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.20it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.258861541748047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.39it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.830611228942871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.10it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.462695598602295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.24it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.330860137939453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.37it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8048932552337646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.21it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.696951389312744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.34it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5801029205322266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.32it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.590264797210693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.22it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.872764587402344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.19it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.98143196105957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.20it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724222719669342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.19it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.784213066101074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:10<00:00,  3.13it/s]\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40239495038986206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 1/34 [00:00<00:12,  2.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-e6305760c95c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# We move forward through each state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# Find the predicted words from the decoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6571def610ca>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Get current hidden state from input word and last hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Take the updated output and find the most appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# output symbol.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for _e in range(num_epochs):\n",
    "    # 2.Iterate through the data points (Xs and Ys)\n",
    "    for data_dict in tqdm(dataloader):\n",
    "        data_batch = sort_batch_by_len(data_dict)\n",
    "        x, x_len = data_batch['x'], data_batch['x_len']\n",
    "        y, y_len = data_batch['y'], data_batch['y_len']\n",
    "\n",
    "        \n",
    "        # 3. Clear the gradient outputs\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        packed_encoder_output, encoder_hidden = encoder(x, x_len)\n",
    "        \n",
    "        \n",
    "        # Then create the initial state for the decoder.\n",
    "        batch_size = encoder_hidden.size(1) # Assumes `batch_first=True`\n",
    "        start_idx = kopi_data.vectorize(kopi_data.trg_vocab, ['<s>'])\n",
    "        end_idx = kopi_data.vectorize(kopi_data.trg_vocab, ['</s>'])\n",
    "\n",
    "        # Multiply the start_idx by batch_size no. of times.\n",
    "        decoder_input = torch.stack([start_idx] * batch_size)\n",
    "        # As the first state of the decoder, we take the last step of the encoder.\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "        # Keeps track of the loss per batch.\n",
    "        loss = 0\n",
    "        \n",
    "        # 4. Forward propagate\n",
    "        # Iterate through each state in the decoder.\n",
    "        # Note: when we are training we know the length of the decoder.\n",
    "        #       so we can use the trick to restrict the loop when decoding.\n",
    "        for di in range(max(y_len)):\n",
    "            # We move forward through each state.\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            # Find the predicted words from the decoder_output\n",
    "            top_scores, top_predictions = torch.max(decoder_output.squeeze(0), -1)\n",
    "            decoder_input = y.transpose(0, 1)[di].view(batch_size, 1) if use_teacher_forcing else top_predictions.unsqueeze(1)\n",
    "            ##decoder_input = top_predictions.unsqueeze(1)\n",
    "            \n",
    "            # 5. Compute the loss for the predictions against the truth value\n",
    "            # Note: for cross entropy loss, the criterion expects\n",
    "            # (i)  decoder output, float tensor, softmax probablities for all classes.\n",
    "            # (ii) the long tensor of the target class.\n",
    "            loss += criterion(decoder_output, y.transpose(0, 1)[di])\n",
    "            \n",
    "            # When all predictions are <pad>\n",
    "            ##if decoder_input.sum() == 0:\n",
    "            ##    break\n",
    "        \n",
    "        #decoder_input = kopi_data.pad_sequence(decoder_input, kopi_data.max_len)\n",
    "            \n",
    "        # 6. Backpropagate, i.e. compute the partial gradients \n",
    "        loss.backward()\n",
    "        # Keep track of the current batch loss.\n",
    "        losses.append(loss)\n",
    "        # 7. Optimizers update the model parameters\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "    print(loss.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.6 Getting the Model to Translate\n",
    "\n",
    "Remember that during training, our decode takes the `encoder_hidden` as in start state of the `decoder_hidden` and starts predicting the words as we move along the decoder states?\n",
    "\n",
    "Similarly, when translating input sentences with no target sentences, we'll do the same prediction in the decoder but the only difference is that we **DON'T** need to:\n",
    "\n",
    "- measure the difference between the prediction and the actual target sentence since we don't have it, (i.e. we don't need to do `criterion(decoder_output, y.transpose(0, 1)[di])`) and\n",
    "- backpropagate nor update the loss\n",
    "- do anything to the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PackedSequence(data=tensor([[-0.0565,  0.0557, -0.9634,  ..., -0.0144,  0.0474, -0.1354],\n",
       "         [-0.0565,  0.0557, -0.9634,  ..., -0.0144,  0.0474, -0.1354],\n",
       "         [-0.0565,  0.0557, -0.9634,  ..., -0.0144,  0.0474, -0.1354],\n",
       "         ...,\n",
       "         [ 0.4443, -0.1483, -0.8171,  ...,  0.9573,  0.3694, -0.9025],\n",
       "         [ 0.2033, -0.4848,  0.9678,  ..., -0.9914, -0.8758, -0.8300],\n",
       "         [ 0.4641, -0.1500, -0.8178,  ...,  0.9429,  0.3739, -0.8733]],\n",
       "        grad_fn=<CatBackward>), batch_sizes=tensor([5, 5, 5, 5, 3, 2, 1])),\n",
       " tensor([[[ 0.4641, -0.1500, -0.8178,  ...,  0.9429,  0.3739, -0.8733],\n",
       "          [ 0.2033, -0.4848,  0.9678,  ..., -0.9914, -0.8758, -0.8300],\n",
       "          [ 0.2649, -0.2997, -0.9404,  ...,  0.0093, -0.4636, -0.2900],\n",
       "          [-0.1935, -0.0763, -0.7118,  ...,  0.9007, -0.2745,  0.1660],\n",
       "          [-0.2272,  0.2700, -0.4377,  ..., -0.5931,  0.4415,  0.0769]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_encoder_output, encoder_hidden \n",
    "encoder(x, x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2, 68,  4, 10,  8,  6,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0],\n",
       "         [ 2, 68, 10,  5, 11,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0],\n",
       "         [ 2, 13,  8,  6,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0],\n",
       "         [ 2, 68,  9,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0],\n",
       "         [ 2, 24, 25,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0]]), tensor([7, 6, 5, 4, 4]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sent):\n",
    "    sent_tensor = kopi_data.vectorize(kopi_data.src_vocab, sent)\n",
    "    sent_len = len(sent_tensor)\n",
    "    sent_tensor = kopi_data.pad_sequence(sent_tensor, kopi_data.max_len)\n",
    "\n",
    "    # Encoder!!\n",
    "    packed_encoder_output, encoder_hidden = encoder(sent_tensor.unsqueeze(0), tensor([sent_len]))\n",
    "\n",
    "    start_idx = kopi_data.vectorize(kopi_data.trg_vocab, ['<s>'])\n",
    "    # Multiply the start_idx by batch_size no. of times.\n",
    "    decoder_input = torch.tensor([start_idx])\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    indices = []\n",
    "    for di in range(kopi_data.max_len):\n",
    "        # Create no. of batches * the initial_state.\n",
    "        embedded = torch.stack([decoder.embedding(decoder_input)]).permute(1, 0, 2)\n",
    "        decoder_out, decoder_hidden = decoder.gru(embedded, decoder_hidden)\n",
    "\n",
    "        _, pred = torch.max(F.softmax(decoder.out(decoder_out).squeeze(0), dim=1), -1)\n",
    "        decoder_input = pred\n",
    "        indices.append(int(pred))\n",
    "\n",
    "    translation = []\n",
    "    for token in kopi_data.unvectorize(kopi_data.trg_vocab, indices):\n",
    "        if token in ['</s>', '<pad>']:\n",
    "            break\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        translation.append(token)\n",
    "    ##print(kopi_data.unvectorize(kopi_data.trg_vocab, indices))\n",
    "    return ' '.join(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iced coffee ,'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'kopi peng'.split()\n",
    "translate(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
