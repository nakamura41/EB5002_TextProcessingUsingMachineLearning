{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tsundoku in /anaconda3/lib/python3.6/site-packages (0.0.3)\n",
      "Requirement already satisfied: gensim in /anaconda3/lib/python3.6/site-packages (from tsundoku) (3.4.0)\n",
      "Requirement already satisfied: torch in /anaconda3/lib/python3.6/site-packages (from tsundoku) (0.4.1)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (from tsundoku) (1.15.0)\n",
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.6/site-packages (from tsundoku) (3.2.5)\n",
      "Requirement already satisfied: IPython in /anaconda3/lib/python3.6/site-packages (from tsundoku) (6.2.1)\n",
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.6/site-packages (from tsundoku) (4.23.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /anaconda3/lib/python3.6/site-packages (from gensim->tsundoku) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /anaconda3/lib/python3.6/site-packages (from gensim->tsundoku) (1.11.0)\n",
      "Requirement already satisfied: smart_open>=1.2.1 in /anaconda3/lib/python3.6/site-packages (from gensim->tsundoku) (1.6.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (38.4.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (0.11.1)\n",
      "Requirement already satisfied: decorator in /anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (4.2.1)\n",
      "Requirement already satisfied: pickleshare in /anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (0.7.4)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (0.8.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (4.3.2)\n",
      "Requirement already satisfied: prompt_toolkit<2.0.0,>=1.0.4 in /anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (1.0.15)\n",
      "Requirement already satisfied: pygments in /anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (2.2.0)\n",
      "Requirement already satisfied: appnope in /anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (0.1.0)\n",
      "Requirement already satisfied: pexpect in /anaconda3/lib/python3.6/site-packages (from IPython->tsundoku) (4.3.1)\n",
      "Requirement already satisfied: boto>=2.32 in /anaconda3/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim->tsundoku) (2.48.0)\n",
      "Requirement already satisfied: bz2file in /anaconda3/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim->tsundoku) (0.98)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim->tsundoku) (2.18.4)\n",
      "Requirement already satisfied: boto3 in /anaconda3/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim->tsundoku) (1.7.84)\n",
      "Requirement already satisfied: parso==0.1.* in /anaconda3/lib/python3.6/site-packages (from jedi>=0.10->IPython->tsundoku) (0.1.1)\n",
      "Requirement already satisfied: ipython_genutils in /anaconda3/lib/python3.6/site-packages (from traitlets>=4.2->IPython->tsundoku) (0.2.0)\n",
      "Requirement already satisfied: wcwidth in /anaconda3/lib/python3.6/site-packages (from prompt_toolkit<2.0.0,>=1.0.4->IPython->tsundoku) (0.1.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim->tsundoku) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim->tsundoku) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim->tsundoku) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests->smart_open>=1.2.1->gensim->tsundoku) (2018.11.29)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.84 in /anaconda3/lib/python3.6/site-packages (from boto3->smart_open>=1.2.1->gensim->tsundoku) (1.10.84)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.6/site-packages (from boto3->smart_open>=1.2.1->gensim->tsundoku) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /anaconda3/lib/python3.6/site-packages (from boto3->smart_open>=1.2.1->gensim->tsundoku) (0.1.13)\n",
      "Requirement already satisfied: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.84->boto3->smart_open>=1.2.1->gensim->tsundoku) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.84->boto3->smart_open>=1.2.1->gensim->tsundoku) (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tsundoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: torch in /anaconda3/lib/python3.6/site-packages (0.4.1)\n",
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.6/site-packages (4.23.3)\n",
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.6/site-packages (3.2.5)\n",
      "Collecting lazyme\n",
      "  Downloading https://files.pythonhosted.org/packages/19/02/f61d1772afbba87874005a91ebe03b874976e720b402280c5eb2b1a30435/lazyme-0.0.22.tar.gz\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.6/site-packages (2.18.4)\n",
      "Requirement already satisfied: gensim in /anaconda3/lib/python3.6/site-packages (3.4.0)\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/lib/python3.6/site-packages (from sklearn) (0.19.1)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests) (2018.11.29)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: smart_open>=1.2.1 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.6.0)\n",
      "Requirement already satisfied: boto>=2.32 in /anaconda3/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: bz2file in /anaconda3/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: boto3 in /anaconda3/lib/python3.6/site-packages (from smart_open>=1.2.1->gensim) (1.7.84)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.84 in /anaconda3/lib/python3.6/site-packages (from boto3->smart_open>=1.2.1->gensim) (1.10.84)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.6/site-packages (from boto3->smart_open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /anaconda3/lib/python3.6/site-packages (from boto3->smart_open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.84->boto3->smart_open>=1.2.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.84->boto3->smart_open>=1.2.1->gensim) (2.6.1)\n",
      "Building wheels for collected packages: sklearn, lazyme\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/davidleonardi/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "  Building wheel for lazyme (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/davidleonardi/Library/Caches/pip/wheels/85/7a/38/7af821d7ef46e04c7c74cc033e26fd7487572c4609331c437c\n",
      "Successfully built sklearn lazyme\n",
      "Installing collected packages: sklearn, lazyme\n",
      "Successfully installed lazyme-0.0.22 sklearn-0.0\n",
      "/anaconda3/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/davidleonardi/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn torch tqdm nltk lazyme requests gensim\n",
    "!python -m nltk.downloader movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "from tsundoku.word2vec_hints import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "- <a href=\"#section-3-0\">**3.0. Data Preparation**</a>\n",
    "  - <a href=\"#section-3-0-1\">3.0.1. *Vocabulary*</a>\n",
    "    - <a href=\"#section-3-0-1-a\"> Pet Peeve: using `gensim`</a>\n",
    "  - <a href=\"#section-3-0-2\">3.0.2. *Dataset*</a>  (<a href=\"#section-3-0-2-hints\">Hints</a>)\n",
    "    - <a href=\"#section-3-0-2-return-dict\">Return `dict` in `__getitem__()`</a>\n",
    "    - <a href=\"#section-3-0-2-labeleddata\">Try `LabeledDataset`</a>\n",
    "<br><br>\n",
    "- <a href=\"#section-3-1\">**3.1. Word2Vec from Scratch**</a>\n",
    "  - <a href=\"#section-3-1-1\">3.1.1. *CBOW*</a>\n",
    "  - <a href=\"#section-3-1-2\">3.1.2. *Skipgram*</a>\n",
    "  - <a href=\"#section-3-1-3\">3.1.3. *Word2Vec Dataset*</a> (<a href=\"#section-3-1-3-hint\">Hints</a>)\n",
    "  - <a href=\"#section-3-1-4-hint\">3.1.4. *Train a CBOW model*</a>\n",
    "    - <a href=\"#section-3-1-4-fill-cbow\">Fill in the CBOW model</a>\n",
    "    - <a href=\"#section-3-1-4-train-cbow\">Train the model (*for real*)</a>\n",
    "    - <a href=\"#section-3-1-4-evaluate-cbow\">Evaluate the model</a>\n",
    "    - <a href=\"#section-3-1-4-load-model\">Load model at specific epoch</a>\n",
    "  - <a href=\"#section-3-1-5\">3.1.5. *Train a Skipgram model*</a>\n",
    "    - <a href=\"#section-3-1-5-forward\">Take a closer look at `forward()`</a>\n",
    "    - <a href=\"#section-3-1-5-train\">Train the model (*for real*)</a>\n",
    "    - <a href=\"section-3-1-5-evaluate\">Evaluate the model</a>\n",
    "  - <a href=\"#section-3-1-6\">3.1.6. *Loading Pre-trained Embeddings*</a>\n",
    "    - <a href=\"#section-3-1-6-vocab\">Override the Embedding vocabulary</a>\n",
    "    - <a href=\"#section-3-1-6-pretrained\">Override the Embedding weights</a>\n",
    "    - <a href=\"#section-3-1-6-eval-skipgram\">Evaluate on the Skipgram task</a>\n",
    "    - <a href=\"#section-3-1-6-eval-cbow\">Evaluate on the CBOW task</a>\n",
    "    - <a href=\"#section-3-1-6-unfreeze-finetune\">Unfreeeze and finetune</a>\n",
    "    - <a href=\"#section-3-1-6-reval-cbow\">Re-evaluate on the CBOW task</a>\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0\"></a>\n",
    "# 3.0. Data Preparation\n",
    "\n",
    "Before we train our own embeddings, lets first understand how to read text data into pytorch.\n",
    "The native pytorch way to load datasets is to use the `torch.utils.data.Dataset` object.\n",
    "\n",
    "There are already several other libraries that help with loading text datasets, e.g. \n",
    "\n",
    " - FastAI https://docs.fast.ai/text.data.html\n",
    " - AllenNLP https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset.html\n",
    " - Torch Text https://github.com/pytorch/text#data\n",
    " - Texar https://texar.readthedocs.io/en/latest/code/data.html#id4 \n",
    " - SpaCy https://github.com/explosion/thinc\n",
    " \n",
    "\n",
    "But to truly understand and use it for the custom datasets you'll see at work, lets learn it the native way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-1\"></a>\n",
    "## 3.0.1  Vocabulary\n",
    "\n",
    "Given a text, the first thing to do is to build a vocabulary (i.e. a dictionary of unique words) and assign an index to each unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"Language users never choose words randomly, and language is essentially\n",
    "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
    "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
    "the null hypothesis will never be true. Moreover, where there is enough\n",
    "data, we shall (almost) always be able to establish that it is not true. In\n",
    "corpus studies, we frequently do have enough data, so the fact that a relation \n",
    "between two phenomena is demonstrably non-random, does not support the inference \n",
    "that it is not arbitrary. We present experimental evidence\n",
    "of how arbitrary associations between word frequencies and corpora are\n",
    "systematically non-random. We review literature in which hypothesis testing \n",
    "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
    "\n",
    "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "uniq_tokens = set(chain(*tokenized_text))\n",
    "\n",
    "vocab = {}   # Assign indices to every word.\n",
    "idx2tok = {} # Also keep an dict of index to words.\n",
    "for i, token in enumerate(uniq_tokens):\n",
    "    vocab[token] = i\n",
    "    idx2tok[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(': 44,\n",
       " ')': 52,\n",
       " ',': 34,\n",
       " '.': 47,\n",
       " 'a': 18,\n",
       " 'able': 31,\n",
       " 'almost': 27,\n",
       " 'always': 19,\n",
       " 'and': 8,\n",
       " 'arbitrary': 3,\n",
       " 'are': 56,\n",
       " 'associations': 38,\n",
       " 'at': 12,\n",
       " 'be': 2,\n",
       " 'been': 76,\n",
       " 'between': 62,\n",
       " 'choose': 1,\n",
       " 'corpora': 10,\n",
       " 'corpus': 75,\n",
       " 'data': 73,\n",
       " 'demonstrably': 60,\n",
       " 'do': 11,\n",
       " 'does': 37,\n",
       " 'enough': 69,\n",
       " 'essentially': 79,\n",
       " 'establish': 82,\n",
       " 'evidence': 81,\n",
       " 'experimental': 0,\n",
       " 'fact': 48,\n",
       " 'frequencies': 40,\n",
       " 'frequently': 9,\n",
       " 'has': 70,\n",
       " 'have': 74,\n",
       " 'hence': 28,\n",
       " 'how': 25,\n",
       " 'hypothesis': 33,\n",
       " 'in': 50,\n",
       " 'inference': 54,\n",
       " 'is': 85,\n",
       " 'it': 46,\n",
       " 'language': 68,\n",
       " 'led': 45,\n",
       " 'linguistic': 21,\n",
       " 'literature': 58,\n",
       " 'look': 22,\n",
       " 'misleading': 5,\n",
       " 'moreover': 80,\n",
       " 'never': 35,\n",
       " 'non-random': 4,\n",
       " 'not': 32,\n",
       " 'null': 72,\n",
       " 'of': 71,\n",
       " 'often': 53,\n",
       " 'or': 14,\n",
       " 'phenomena': 30,\n",
       " 'posits': 65,\n",
       " 'present': 13,\n",
       " 'randomly': 55,\n",
       " 'randomness': 86,\n",
       " 'relation': 77,\n",
       " 'results': 66,\n",
       " 'review': 41,\n",
       " 'shall': 26,\n",
       " 'show': 7,\n",
       " 'so': 36,\n",
       " 'statistical': 61,\n",
       " 'studies': 59,\n",
       " 'support': 49,\n",
       " 'systematically': 51,\n",
       " 'testing': 84,\n",
       " 'that': 39,\n",
       " 'the': 16,\n",
       " 'there': 17,\n",
       " 'to': 15,\n",
       " 'true': 83,\n",
       " 'two': 63,\n",
       " 'unhelpful': 64,\n",
       " 'used': 42,\n",
       " 'users': 6,\n",
       " 'uses': 23,\n",
       " 'we': 67,\n",
       " 'when': 29,\n",
       " 'where': 20,\n",
       " 'which': 78,\n",
       " 'will': 57,\n",
       " 'word': 43,\n",
       " 'words': 24}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the index of the word 'corpora'\n",
    "vocab['corpora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68, 6, 35, 1, 24, 55, 34, 8, 68, 85, 79, 4, 47]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The indexed representation of the first sentence.\n",
    "\n",
    "sent0 = tokenized_text[0]\n",
    "\n",
    "[vocab[token] for token in sent0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-1-a\"></a>\n",
    "\n",
    "### Pet Peeve (Gensim)\n",
    "\n",
    "I (Liling) don't really like to write my own vectorizer the `gensim` has functions that are optimized for such operations. In fact, I've written a [whole preprocessing pipeline library for me to use for language modelling and machine translation purposes](https://github.com/alvations/komorebi/blob/master/komorebi/text.py) =)\n",
    "\n",
    "Using `gensim`, I would have written the above as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "vocab = Dictionary(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ',',\n",
       " 1: '.',\n",
       " 2: 'and',\n",
       " 3: 'choose',\n",
       " 4: 'essentially',\n",
       " 5: 'is',\n",
       " 6: 'language',\n",
       " 7: 'never',\n",
       " 8: 'non-random',\n",
       " 9: 'randomly',\n",
       " 10: 'users',\n",
       " 11: 'words',\n",
       " 12: 'a',\n",
       " 13: 'hypothesis',\n",
       " 14: 'null',\n",
       " 15: 'posits',\n",
       " 16: 'randomness',\n",
       " 17: 'statistical',\n",
       " 18: 'testing',\n",
       " 19: 'uses',\n",
       " 20: 'which',\n",
       " 21: 'at',\n",
       " 22: 'be',\n",
       " 23: 'corpora',\n",
       " 24: 'hence',\n",
       " 25: 'in',\n",
       " 26: 'linguistic',\n",
       " 27: 'look',\n",
       " 28: 'phenomena',\n",
       " 29: 'the',\n",
       " 30: 'true',\n",
       " 31: 'we',\n",
       " 32: 'when',\n",
       " 33: 'will',\n",
       " 34: '(',\n",
       " 35: ')',\n",
       " 36: 'able',\n",
       " 37: 'almost',\n",
       " 38: 'always',\n",
       " 39: 'data',\n",
       " 40: 'enough',\n",
       " 41: 'establish',\n",
       " 42: 'it',\n",
       " 43: 'moreover',\n",
       " 44: 'not',\n",
       " 45: 'shall',\n",
       " 46: 'that',\n",
       " 47: 'there',\n",
       " 48: 'to',\n",
       " 49: 'where',\n",
       " 50: 'arbitrary',\n",
       " 51: 'between',\n",
       " 52: 'corpus',\n",
       " 53: 'demonstrably',\n",
       " 54: 'do',\n",
       " 55: 'does',\n",
       " 56: 'fact',\n",
       " 57: 'frequently',\n",
       " 58: 'have',\n",
       " 59: 'inference',\n",
       " 60: 'relation',\n",
       " 61: 'so',\n",
       " 62: 'studies',\n",
       " 63: 'support',\n",
       " 64: 'two',\n",
       " 65: 'are',\n",
       " 66: 'associations',\n",
       " 67: 'evidence',\n",
       " 68: 'experimental',\n",
       " 69: 'frequencies',\n",
       " 70: 'how',\n",
       " 71: 'of',\n",
       " 72: 'present',\n",
       " 73: 'systematically',\n",
       " 74: 'word',\n",
       " 75: 'been',\n",
       " 76: 'has',\n",
       " 77: 'led',\n",
       " 78: 'literature',\n",
       " 79: 'misleading',\n",
       " 80: 'often',\n",
       " 81: 'or',\n",
       " 82: 'results',\n",
       " 83: 'review',\n",
       " 84: 'show',\n",
       " 85: 'unhelpful',\n",
       " 86: 'used'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the key-value order is different of gensim from the native Python's\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token2id['corpora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.doc2idx(sent0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"indexed form\" of the tokens in the sentence forms the ***vectorized*** input to the `nn.Embedding` layer in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2\"></a>\n",
    "\n",
    "# 3.0.2 Dataset\n",
    "\n",
    "Lets try creating a `torch.utils.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Text(Dataset):\n",
    "    def __init__(self, tokenized_texts):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.vocab = Dictionary(tokenized_texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        # Hint: You want to return a vectorized sentence here.\n",
    "        return {'x': self.vectorize(self.sents[index])}\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2-hints\"></a>\n",
    "## Hints to the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
    "##hint_dataset_vectorize()\n",
    "##code_text_dataset_vectorize()\n",
    "\n",
    "# Option 2: \"I give up just, run the code for me\" \n",
    "# Uncomment the next two lines, if you really gave up... \n",
    "#full_code_text_dataset_vectorize()\n",
    "##from tsundoku.word2vec import Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'present',\n",
       " 'experimental',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'how',\n",
       " 'arbitrary',\n",
       " 'associations',\n",
       " 'between',\n",
       " 'word',\n",
       " 'frequencies',\n",
       " 'and',\n",
       " 'corpora',\n",
       " 'are',\n",
       " 'systematically',\n",
       " 'non-random',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = Text(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [31, 72, 68, 67, 71, 70, 50, 66, 51, 74, 69, 2, 23, 65, 73, 8, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset[5] # First sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2-return-dict\"></a>\n",
    "\n",
    "### Return `dict` in `__getitem__()`\n",
    "\n",
    "This is nice if we're just representing sentences/documents by their indices but when we're doing machine learning, we usually have `X` and `Y`. \n",
    "\n",
    "If we have labels for the each sentence, we can also put it into to `__getitem__()` by having it return a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LabeledText(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.labels = labels # Sentence level labels.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        return {'X': self.vectorize(self.sents[index]), 'Y': self.labels[index]}\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2-labeleddata\"></a>\n",
    "\n",
    "### Lets try the `LabeledDataset` on a movie review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:11<00:00, 175.02it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "for fileid in tqdm(movie_reviews.fileids()):\n",
    "    label = fileid.split('/')[0]\n",
    "    doc = word_tokenize(movie_reviews.open(fileid).read())\n",
    "    documents.append(doc)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.',\n",
       " 'they',\n",
       " 'get',\n",
       " 'into',\n",
       " 'an',\n",
       " 'accident',\n",
       " '.',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guys',\n",
       " 'dies',\n",
       " ',',\n",
       " 'but',\n",
       " 'his',\n",
       " 'girlfriend',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'see',\n",
       " 'him',\n",
       " 'in',\n",
       " 'her',\n",
       " 'life',\n",
       " ',',\n",
       " 'and',\n",
       " 'has',\n",
       " 'nightmares',\n",
       " '.',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'deal',\n",
       " '?',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'and',\n",
       " '``',\n",
       " 'sorta',\n",
       " '``',\n",
       " 'find',\n",
       " 'out',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'critique',\n",
       " ':',\n",
       " 'a',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'for',\n",
       " 'the',\n",
       " 'teen',\n",
       " 'generation',\n",
       " 'that',\n",
       " 'touches',\n",
       " 'on',\n",
       " 'a',\n",
       " 'very',\n",
       " 'cool',\n",
       " 'idea',\n",
       " ',',\n",
       " 'but',\n",
       " 'presents',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'package',\n",
       " '.',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'review',\n",
       " 'an',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'one',\n",
       " 'to',\n",
       " 'write',\n",
       " ',',\n",
       " 'since',\n",
       " 'i',\n",
       " 'generally',\n",
       " 'applaud',\n",
       " 'films',\n",
       " 'which',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'break',\n",
       " 'the',\n",
       " 'mold',\n",
       " ',',\n",
       " 'mess',\n",
       " 'with',\n",
       " 'your',\n",
       " 'head',\n",
       " 'and',\n",
       " 'such',\n",
       " '(',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '&',\n",
       " 'memento',\n",
       " ')',\n",
       " ',',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'good',\n",
       " 'and',\n",
       " 'bad',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'making',\n",
       " 'all',\n",
       " 'types',\n",
       " 'of',\n",
       " 'films',\n",
       " ',',\n",
       " 'and',\n",
       " 'these',\n",
       " 'folks',\n",
       " 'just',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'snag',\n",
       " 'this',\n",
       " 'one',\n",
       " 'correctly',\n",
       " '.',\n",
       " 'they',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'this',\n",
       " 'pretty',\n",
       " 'neat',\n",
       " 'concept',\n",
       " ',',\n",
       " 'but',\n",
       " 'executed',\n",
       " 'it',\n",
       " 'terribly',\n",
       " '.',\n",
       " 'so',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'well',\n",
       " ',',\n",
       " 'its',\n",
       " 'main',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'simply',\n",
       " 'too',\n",
       " 'jumbled',\n",
       " '.',\n",
       " 'it',\n",
       " 'starts',\n",
       " 'off',\n",
       " '``',\n",
       " 'normal',\n",
       " '``',\n",
       " 'but',\n",
       " 'then',\n",
       " 'downshifts',\n",
       " 'into',\n",
       " 'this',\n",
       " '``',\n",
       " 'fantasy',\n",
       " '``',\n",
       " 'world',\n",
       " 'in',\n",
       " 'which',\n",
       " 'you',\n",
       " ',',\n",
       " 'as',\n",
       " 'an',\n",
       " 'audience',\n",
       " 'member',\n",
       " ',',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'on',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'dreams',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'characters',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'from',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'others',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'strange',\n",
       " 'apparitions',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'disappearances',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'looooot',\n",
       " 'of',\n",
       " 'chase',\n",
       " 'scenes',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tons',\n",
       " 'of',\n",
       " 'weird',\n",
       " 'things',\n",
       " 'that',\n",
       " 'happen',\n",
       " ',',\n",
       " 'and',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'not',\n",
       " 'explained',\n",
       " '.',\n",
       " 'now',\n",
       " 'i',\n",
       " 'personally',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mind',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'unravel',\n",
       " 'a',\n",
       " 'film',\n",
       " 'every',\n",
       " 'now',\n",
       " 'and',\n",
       " 'then',\n",
       " ',',\n",
       " 'but',\n",
       " 'when',\n",
       " 'all',\n",
       " 'it',\n",
       " 'does',\n",
       " 'is',\n",
       " 'give',\n",
       " 'me',\n",
       " 'the',\n",
       " 'same',\n",
       " 'clue',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " ',',\n",
       " 'i',\n",
       " 'get',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'fed',\n",
       " 'up',\n",
       " 'after',\n",
       " 'a',\n",
       " 'while',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'this',\n",
       " 'film',\n",
       " \"'s\",\n",
       " 'biggest',\n",
       " 'problem',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'obviously',\n",
       " 'got',\n",
       " 'this',\n",
       " 'big',\n",
       " 'secret',\n",
       " 'to',\n",
       " 'hide',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'it',\n",
       " 'completely',\n",
       " 'until',\n",
       " 'its',\n",
       " 'final',\n",
       " 'five',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'and',\n",
       " 'do',\n",
       " 'they',\n",
       " 'make',\n",
       " 'things',\n",
       " 'entertaining',\n",
       " ',',\n",
       " 'thrilling',\n",
       " 'or',\n",
       " 'even',\n",
       " 'engaging',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meantime',\n",
       " '?',\n",
       " 'not',\n",
       " 'really',\n",
       " '.',\n",
       " 'the',\n",
       " 'sad',\n",
       " 'part',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'arrow',\n",
       " 'and',\n",
       " 'i',\n",
       " 'both',\n",
       " 'dig',\n",
       " 'on',\n",
       " 'flicks',\n",
       " 'like',\n",
       " 'this',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'actually',\n",
       " 'figured',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'out',\n",
       " 'by',\n",
       " 'the',\n",
       " 'half-way',\n",
       " 'point',\n",
       " ',',\n",
       " 'so',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'after',\n",
       " 'that',\n",
       " 'did',\n",
       " 'start',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'sense',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'still',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'the',\n",
       " 'make',\n",
       " 'the',\n",
       " 'film',\n",
       " 'all',\n",
       " 'that',\n",
       " 'more',\n",
       " 'entertaining',\n",
       " '.',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'the',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'with',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'this',\n",
       " 'is',\n",
       " 'that',\n",
       " 'you',\n",
       " 'should',\n",
       " 'always',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'the',\n",
       " 'audience',\n",
       " 'is',\n",
       " '``',\n",
       " 'into',\n",
       " 'it',\n",
       " '``',\n",
       " 'even',\n",
       " 'before',\n",
       " 'they',\n",
       " 'are',\n",
       " 'given',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'password',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'your',\n",
       " 'world',\n",
       " 'of',\n",
       " 'understanding',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'showing',\n",
       " 'melissa',\n",
       " 'sagemiller',\n",
       " 'running',\n",
       " 'away',\n",
       " 'from',\n",
       " 'visions',\n",
       " 'for',\n",
       " 'about',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'just',\n",
       " 'plain',\n",
       " 'lazy',\n",
       " '!',\n",
       " '!',\n",
       " 'okay',\n",
       " ',',\n",
       " 'we',\n",
       " 'get',\n",
       " 'it',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'chasing',\n",
       " 'her',\n",
       " 'and',\n",
       " 'we',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'who',\n",
       " 'they',\n",
       " 'are',\n",
       " '.',\n",
       " 'do',\n",
       " 'we',\n",
       " 'really',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " '?',\n",
       " 'how',\n",
       " 'about',\n",
       " 'giving',\n",
       " 'us',\n",
       " 'different',\n",
       " 'scenes',\n",
       " 'offering',\n",
       " 'further',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'going',\n",
       " 'down',\n",
       " 'in',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'apparently',\n",
       " ',',\n",
       " 'the',\n",
       " 'studio',\n",
       " 'took',\n",
       " 'this',\n",
       " 'film',\n",
       " 'away',\n",
       " 'from',\n",
       " 'its',\n",
       " 'director',\n",
       " 'and',\n",
       " 'chopped',\n",
       " 'it',\n",
       " 'up',\n",
       " 'themselves',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'shows',\n",
       " '.',\n",
       " 'there',\n",
       " 'might',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'decent',\n",
       " 'teen',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'in',\n",
       " 'here',\n",
       " 'somewhere',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'guess',\n",
       " '``',\n",
       " 'the',\n",
       " 'suits',\n",
       " '``',\n",
       " 'decided',\n",
       " 'that',\n",
       " 'turning',\n",
       " 'it',\n",
       " 'into',\n",
       " 'a',\n",
       " 'music',\n",
       " 'video',\n",
       " 'with',\n",
       " 'little',\n",
       " 'edge',\n",
       " ',',\n",
       " 'would',\n",
       " 'make',\n",
       " 'more',\n",
       " 'sense',\n",
       " '.',\n",
       " 'the',\n",
       " 'actors',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'for',\n",
       " 'the',\n",
       " 'most',\n",
       " 'part',\n",
       " ',',\n",
       " 'although',\n",
       " 'wes',\n",
       " 'bentley',\n",
       " 'just',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'playing',\n",
       " 'the',\n",
       " 'exact',\n",
       " 'same',\n",
       " 'character',\n",
       " 'that',\n",
       " 'he',\n",
       " 'did',\n",
       " 'in',\n",
       " 'american',\n",
       " 'beauty',\n",
       " ',',\n",
       " 'only',\n",
       " 'in',\n",
       " 'a',\n",
       " 'new',\n",
       " 'neighborhood',\n",
       " '.',\n",
       " 'but',\n",
       " 'my',\n",
       " 'biggest',\n",
       " 'kudos',\n",
       " 'go',\n",
       " 'out',\n",
       " 'to',\n",
       " 'sagemiller',\n",
       " ',',\n",
       " 'who',\n",
       " 'holds',\n",
       " 'her',\n",
       " 'own',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'film',\n",
       " ',',\n",
       " 'and',\n",
       " 'actually',\n",
       " 'has',\n",
       " 'you',\n",
       " 'feeling',\n",
       " 'her',\n",
       " 'character',\n",
       " \"'s\",\n",
       " 'unraveling',\n",
       " '.',\n",
       " 'overall',\n",
       " ',',\n",
       " 'the',\n",
       " 'film',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'stick',\n",
       " 'because',\n",
       " 'it',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'entertain',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'confusing',\n",
       " ',',\n",
       " 'it',\n",
       " 'rarely',\n",
       " 'excites',\n",
       " 'and',\n",
       " 'it',\n",
       " 'feels',\n",
       " 'pretty',\n",
       " 'redundant',\n",
       " 'for',\n",
       " 'most',\n",
       " 'of',\n",
       " 'its',\n",
       " 'runtime',\n",
       " ',',\n",
       " 'despite',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'ending',\n",
       " 'and',\n",
       " 'explanation',\n",
       " 'to',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'craziness',\n",
       " 'that',\n",
       " 'came',\n",
       " 'before',\n",
       " 'it',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'and',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'horror',\n",
       " 'or',\n",
       " 'teen',\n",
       " 'slasher',\n",
       " 'flick',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'just',\n",
       " 'packaged',\n",
       " 'to',\n",
       " 'look',\n",
       " 'that',\n",
       " 'way',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'is',\n",
       " 'apparently',\n",
       " 'assuming',\n",
       " 'that',\n",
       " 'the',\n",
       " 'genre',\n",
       " 'is',\n",
       " 'still',\n",
       " 'hot',\n",
       " 'with',\n",
       " 'the',\n",
       " 'kids',\n",
       " '.',\n",
       " 'it',\n",
       " 'also',\n",
       " 'wrapped',\n",
       " 'production',\n",
       " 'two',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'has',\n",
       " 'been',\n",
       " 'sitting',\n",
       " 'on',\n",
       " 'the',\n",
       " 'shelves',\n",
       " 'ever',\n",
       " 'since',\n",
       " '.',\n",
       " 'whatever',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'skip',\n",
       " 'it',\n",
       " '!',\n",
       " 'where',\n",
       " \"'s\",\n",
       " 'joblo',\n",
       " 'coming',\n",
       " 'from',\n",
       " '?',\n",
       " 'a',\n",
       " 'nightmare',\n",
       " 'of',\n",
       " 'elm',\n",
       " 'street',\n",
       " '3',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'blair',\n",
       " 'witch',\n",
       " '2',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " ':',\n",
       " 'salvation',\n",
       " '(',\n",
       " '4/10',\n",
       " ')',\n",
       " '-',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'memento',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'others',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'stir',\n",
       " 'of',\n",
       " 'echoes',\n",
       " '(',\n",
       " '8/10',\n",
       " ')']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = LabeledText(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': [243,\n",
       "  17,\n",
       "  314,\n",
       "  294,\n",
       "  77,\n",
       "  140,\n",
       "  307,\n",
       "  20,\n",
       "  68,\n",
       "  237,\n",
       "  6,\n",
       "  97,\n",
       "  34,\n",
       "  299,\n",
       "  98,\n",
       "  8,\n",
       "  302,\n",
       "  135,\n",
       "  167,\n",
       "  33,\n",
       "  22,\n",
       "  8,\n",
       "  226,\n",
       "  220,\n",
       "  297,\n",
       "  145,\n",
       "  87,\n",
       "  6,\n",
       "  60,\n",
       "  158,\n",
       "  136,\n",
       "  74,\n",
       "  307,\n",
       "  262,\n",
       "  157,\n",
       "  165,\n",
       "  153,\n",
       "  179,\n",
       "  6,\n",
       "  34,\n",
       "  149,\n",
       "  214,\n",
       "  8,\n",
       "  333,\n",
       "  2,\n",
       "  297,\n",
       "  82,\n",
       "  18,\n",
       "  326,\n",
       "  297,\n",
       "  204,\n",
       "  34,\n",
       "  19,\n",
       "  280,\n",
       "  19,\n",
       "  124,\n",
       "  230,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  79,\n",
       "  17,\n",
       "  20,\n",
       "  199,\n",
       "  204,\n",
       "  129,\n",
       "  297,\n",
       "  294,\n",
       "  133,\n",
       "  296,\n",
       "  311,\n",
       "  225,\n",
       "  20,\n",
       "  322,\n",
       "  75,\n",
       "  164,\n",
       "  6,\n",
       "  60,\n",
       "  245,\n",
       "  169,\n",
       "  165,\n",
       "  20,\n",
       "  322,\n",
       "  46,\n",
       "  234,\n",
       "  8,\n",
       "  337,\n",
       "  168,\n",
       "  333,\n",
       "  188,\n",
       "  304,\n",
       "  253,\n",
       "  33,\n",
       "  108,\n",
       "  148,\n",
       "  226,\n",
       "  307,\n",
       "  345,\n",
       "  6,\n",
       "  272,\n",
       "  163,\n",
       "  132,\n",
       "  37,\n",
       "  122,\n",
       "  337,\n",
       "  42,\n",
       "  307,\n",
       "  59,\n",
       "  297,\n",
       "  201,\n",
       "  6,\n",
       "  196,\n",
       "  341,\n",
       "  348,\n",
       "  152,\n",
       "  34,\n",
       "  290,\n",
       "  4,\n",
       "  185,\n",
       "  156,\n",
       "  1,\n",
       "  195,\n",
       "  5,\n",
       "  6,\n",
       "  60,\n",
       "  300,\n",
       "  38,\n",
       "  142,\n",
       "  34,\n",
       "  46,\n",
       "  328,\n",
       "  220,\n",
       "  189,\n",
       "  28,\n",
       "  315,\n",
       "  220,\n",
       "  122,\n",
       "  6,\n",
       "  34,\n",
       "  301,\n",
       "  128,\n",
       "  173,\n",
       "  86,\n",
       "  208,\n",
       "  276,\n",
       "  304,\n",
       "  226,\n",
       "  76,\n",
       "  8,\n",
       "  302,\n",
       "  263,\n",
       "  307,\n",
       "  150,\n",
       "  293,\n",
       "  304,\n",
       "  246,\n",
       "  209,\n",
       "  72,\n",
       "  6,\n",
       "  60,\n",
       "  113,\n",
       "  169,\n",
       "  295,\n",
       "  8,\n",
       "  277,\n",
       "  333,\n",
       "  38,\n",
       "  297,\n",
       "  248,\n",
       "  341,\n",
       "  297,\n",
       "  204,\n",
       "  18,\n",
       "  331,\n",
       "  6,\n",
       "  170,\n",
       "  186,\n",
       "  247,\n",
       "  168,\n",
       "  296,\n",
       "  169,\n",
       "  2,\n",
       "  271,\n",
       "  309,\n",
       "  172,\n",
       "  8,\n",
       "  169,\n",
       "  282,\n",
       "  221,\n",
       "  19,\n",
       "  216,\n",
       "  19,\n",
       "  60,\n",
       "  299,\n",
       "  95,\n",
       "  167,\n",
       "  304,\n",
       "  19,\n",
       "  116,\n",
       "  19,\n",
       "  342,\n",
       "  165,\n",
       "  337,\n",
       "  347,\n",
       "  6,\n",
       "  40,\n",
       "  33,\n",
       "  43,\n",
       "  194,\n",
       "  6,\n",
       "  150,\n",
       "  215,\n",
       "  164,\n",
       "  333,\n",
       "  2,\n",
       "  141,\n",
       "  225,\n",
       "  8,\n",
       "  300,\n",
       "  38,\n",
       "  96,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  64,\n",
       "  70,\n",
       "  45,\n",
       "  130,\n",
       "  297,\n",
       "  81,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  229,\n",
       "  339,\n",
       "  183,\n",
       "  180,\n",
       "  297,\n",
       "  81,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  286,\n",
       "  36,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  91,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  20,\n",
       "  184,\n",
       "  220,\n",
       "  65,\n",
       "  260,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  308,\n",
       "  220,\n",
       "  330,\n",
       "  303,\n",
       "  296,\n",
       "  147,\n",
       "  6,\n",
       "  34,\n",
       "  203,\n",
       "  220,\n",
       "  169,\n",
       "  168,\n",
       "  271,\n",
       "  217,\n",
       "  114,\n",
       "  8,\n",
       "  218,\n",
       "  163,\n",
       "  240,\n",
       "  92,\n",
       "  208,\n",
       "  198,\n",
       "  312,\n",
       "  307,\n",
       "  317,\n",
       "  20,\n",
       "  121,\n",
       "  110,\n",
       "  218,\n",
       "  34,\n",
       "  299,\n",
       "  6,\n",
       "  60,\n",
       "  335,\n",
       "  28,\n",
       "  169,\n",
       "  93,\n",
       "  168,\n",
       "  137,\n",
       "  190,\n",
       "  297,\n",
       "  259,\n",
       "  69,\n",
       "  231,\n",
       "  34,\n",
       "  231,\n",
       "  26,\n",
       "  6,\n",
       "  163,\n",
       "  135,\n",
       "  175,\n",
       "  220,\n",
       "  117,\n",
       "  320,\n",
       "  25,\n",
       "  20,\n",
       "  338,\n",
       "  6,\n",
       "  337,\n",
       "  168,\n",
       "  304,\n",
       "  121,\n",
       "  2,\n",
       "  54,\n",
       "  247,\n",
       "  8,\n",
       "  169,\n",
       "  2,\n",
       "  219,\n",
       "  143,\n",
       "  304,\n",
       "  53,\n",
       "  261,\n",
       "  307,\n",
       "  155,\n",
       "  6,\n",
       "  60,\n",
       "  169,\n",
       "  265,\n",
       "  307,\n",
       "  325,\n",
       "  307,\n",
       "  155,\n",
       "  169,\n",
       "  71,\n",
       "  319,\n",
       "  170,\n",
       "  123,\n",
       "  125,\n",
       "  200,\n",
       "  8,\n",
       "  34,\n",
       "  92,\n",
       "  302,\n",
       "  187,\n",
       "  303,\n",
       "  106,\n",
       "  6,\n",
       "  305,\n",
       "  228,\n",
       "  108,\n",
       "  103,\n",
       "  6,\n",
       "  165,\n",
       "  297,\n",
       "  192,\n",
       "  18,\n",
       "  217,\n",
       "  251,\n",
       "  8,\n",
       "  297,\n",
       "  256,\n",
       "  236,\n",
       "  168,\n",
       "  296,\n",
       "  297,\n",
       "  39,\n",
       "  34,\n",
       "  163,\n",
       "  57,\n",
       "  89,\n",
       "  225,\n",
       "  127,\n",
       "  180,\n",
       "  304,\n",
       "  6,\n",
       "  277,\n",
       "  329,\n",
       "  24,\n",
       "  120,\n",
       "  203,\n",
       "  220,\n",
       "  169,\n",
       "  230,\n",
       "  61,\n",
       "  297,\n",
       "  146,\n",
       "  244,\n",
       "  6,\n",
       "  277,\n",
       "  28,\n",
       "  220,\n",
       "  297,\n",
       "  287,\n",
       "  25,\n",
       "  296,\n",
       "  86,\n",
       "  281,\n",
       "  307,\n",
       "  187,\n",
       "  20,\n",
       "  182,\n",
       "  55,\n",
       "  220,\n",
       "  266,\n",
       "  6,\n",
       "  60,\n",
       "  169,\n",
       "  284,\n",
       "  86,\n",
       "  208,\n",
       "  297,\n",
       "  187,\n",
       "  297,\n",
       "  121,\n",
       "  28,\n",
       "  296,\n",
       "  202,\n",
       "  106,\n",
       "  8,\n",
       "  163,\n",
       "  144,\n",
       "  297,\n",
       "  58,\n",
       "  181,\n",
       "  341,\n",
       "  205,\n",
       "  180,\n",
       "  304,\n",
       "  168,\n",
       "  296,\n",
       "  347,\n",
       "  268,\n",
       "  31,\n",
       "  187,\n",
       "  292,\n",
       "  296,\n",
       "  297,\n",
       "  43,\n",
       "  168,\n",
       "  19,\n",
       "  167,\n",
       "  169,\n",
       "  19,\n",
       "  108,\n",
       "  51,\n",
       "  302,\n",
       "  38,\n",
       "  138,\n",
       "  297,\n",
       "  261,\n",
       "  238,\n",
       "  307,\n",
       "  104,\n",
       "  348,\n",
       "  342,\n",
       "  220,\n",
       "  316,\n",
       "  8,\n",
       "  163,\n",
       "  191,\n",
       "  6,\n",
       "  269,\n",
       "  193,\n",
       "  257,\n",
       "  254,\n",
       "  44,\n",
       "  130,\n",
       "  324,\n",
       "  129,\n",
       "  21,\n",
       "  11,\n",
       "  200,\n",
       "  306,\n",
       "  297,\n",
       "  204,\n",
       "  168,\n",
       "  173,\n",
       "  241,\n",
       "  178,\n",
       "  0,\n",
       "  0,\n",
       "  224,\n",
       "  6,\n",
       "  329,\n",
       "  135,\n",
       "  169,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  300,\n",
       "  38,\n",
       "  239,\n",
       "  66,\n",
       "  153,\n",
       "  34,\n",
       "  329,\n",
       "  92,\n",
       "  208,\n",
       "  176,\n",
       "  339,\n",
       "  302,\n",
       "  38,\n",
       "  8,\n",
       "  92,\n",
       "  329,\n",
       "  251,\n",
       "  210,\n",
       "  307,\n",
       "  262,\n",
       "  169,\n",
       "  231,\n",
       "  34,\n",
       "  231,\n",
       "  26,\n",
       "  18,\n",
       "  162,\n",
       "  21,\n",
       "  139,\n",
       "  321,\n",
       "  88,\n",
       "  260,\n",
       "  222,\n",
       "  131,\n",
       "  166,\n",
       "  167,\n",
       "  28,\n",
       "  220,\n",
       "  297,\n",
       "  287,\n",
       "  141,\n",
       "  94,\n",
       "  165,\n",
       "  297,\n",
       "  204,\n",
       "  18,\n",
       "  35,\n",
       "  6,\n",
       "  297,\n",
       "  289,\n",
       "  310,\n",
       "  304,\n",
       "  121,\n",
       "  44,\n",
       "  130,\n",
       "  170,\n",
       "  90,\n",
       "  34,\n",
       "  67,\n",
       "  169,\n",
       "  320,\n",
       "  298,\n",
       "  6,\n",
       "  34,\n",
       "  169,\n",
       "  270,\n",
       "  8,\n",
       "  300,\n",
       "  197,\n",
       "  3,\n",
       "  50,\n",
       "  20,\n",
       "  246,\n",
       "  83,\n",
       "  294,\n",
       "  199,\n",
       "  204,\n",
       "  165,\n",
       "  154,\n",
       "  279,\n",
       "  6,\n",
       "  60,\n",
       "  163,\n",
       "  144,\n",
       "  19,\n",
       "  297,\n",
       "  291,\n",
       "  19,\n",
       "  84,\n",
       "  296,\n",
       "  313,\n",
       "  169,\n",
       "  167,\n",
       "  20,\n",
       "  206,\n",
       "  323,\n",
       "  341,\n",
       "  182,\n",
       "  100,\n",
       "  6,\n",
       "  343,\n",
       "  187,\n",
       "  202,\n",
       "  266,\n",
       "  8,\n",
       "  297,\n",
       "  23,\n",
       "  38,\n",
       "  246,\n",
       "  142,\n",
       "  129,\n",
       "  297,\n",
       "  203,\n",
       "  236,\n",
       "  6,\n",
       "  30,\n",
       "  332,\n",
       "  52,\n",
       "  173,\n",
       "  264,\n",
       "  307,\n",
       "  47,\n",
       "  242,\n",
       "  297,\n",
       "  111,\n",
       "  259,\n",
       "  63,\n",
       "  296,\n",
       "  151,\n",
       "  86,\n",
       "  165,\n",
       "  32,\n",
       "  48,\n",
       "  6,\n",
       "  227,\n",
       "  165,\n",
       "  20,\n",
       "  212,\n",
       "  211,\n",
       "  8,\n",
       "  60,\n",
       "  207,\n",
       "  54,\n",
       "  177,\n",
       "  140,\n",
       "  230,\n",
       "  307,\n",
       "  257,\n",
       "  6,\n",
       "  339,\n",
       "  159,\n",
       "  153,\n",
       "  233,\n",
       "  306,\n",
       "  297,\n",
       "  107,\n",
       "  121,\n",
       "  6,\n",
       "  34,\n",
       "  24,\n",
       "  149,\n",
       "  347,\n",
       "  118,\n",
       "  153,\n",
       "  63,\n",
       "  2,\n",
       "  318,\n",
       "  8,\n",
       "  232,\n",
       "  6,\n",
       "  297,\n",
       "  121,\n",
       "  93,\n",
       "  208,\n",
       "  283,\n",
       "  49,\n",
       "  169,\n",
       "  93,\n",
       "  208,\n",
       "  105,\n",
       "  6,\n",
       "  169,\n",
       "  2,\n",
       "  73,\n",
       "  6,\n",
       "  169,\n",
       "  250,\n",
       "  112,\n",
       "  34,\n",
       "  169,\n",
       "  119,\n",
       "  246,\n",
       "  252,\n",
       "  129,\n",
       "  203,\n",
       "  220,\n",
       "  170,\n",
       "  255,\n",
       "  6,\n",
       "  85,\n",
       "  20,\n",
       "  246,\n",
       "  75,\n",
       "  102,\n",
       "  34,\n",
       "  115,\n",
       "  307,\n",
       "  28,\n",
       "  220,\n",
       "  297,\n",
       "  78,\n",
       "  296,\n",
       "  62,\n",
       "  51,\n",
       "  169,\n",
       "  8,\n",
       "  223,\n",
       "  6,\n",
       "  34,\n",
       "  61,\n",
       "  297,\n",
       "  327,\n",
       "  6,\n",
       "  304,\n",
       "  168,\n",
       "  217,\n",
       "  20,\n",
       "  160,\n",
       "  228,\n",
       "  294,\n",
       "  275,\n",
       "  126,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  169,\n",
       "  2,\n",
       "  173,\n",
       "  235,\n",
       "  307,\n",
       "  183,\n",
       "  296,\n",
       "  327,\n",
       "  49,\n",
       "  278,\n",
       "  168,\n",
       "  35,\n",
       "  41,\n",
       "  296,\n",
       "  297,\n",
       "  134,\n",
       "  168,\n",
       "  284,\n",
       "  161,\n",
       "  341,\n",
       "  297,\n",
       "  174,\n",
       "  8,\n",
       "  169,\n",
       "  29,\n",
       "  344,\n",
       "  249,\n",
       "  314,\n",
       "  346,\n",
       "  27,\n",
       "  34,\n",
       "  149,\n",
       "  50,\n",
       "  273,\n",
       "  225,\n",
       "  297,\n",
       "  267,\n",
       "  109,\n",
       "  272,\n",
       "  8,\n",
       "  334,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  274,\n",
       "  169,\n",
       "  0,\n",
       "  336,\n",
       "  2,\n",
       "  171,\n",
       "  70,\n",
       "  130,\n",
       "  18,\n",
       "  20,\n",
       "  213,\n",
       "  220,\n",
       "  101,\n",
       "  288,\n",
       "  12,\n",
       "  4,\n",
       "  14,\n",
       "  5,\n",
       "  7,\n",
       "  56,\n",
       "  340,\n",
       "  10,\n",
       "  4,\n",
       "  14,\n",
       "  5,\n",
       "  7,\n",
       "  297,\n",
       "  80,\n",
       "  4,\n",
       "  16,\n",
       "  5,\n",
       "  7,\n",
       "  297,\n",
       "  80,\n",
       "  17,\n",
       "  258,\n",
       "  4,\n",
       "  13,\n",
       "  5,\n",
       "  7,\n",
       "  185,\n",
       "  156,\n",
       "  4,\n",
       "  9,\n",
       "  5,\n",
       "  7,\n",
       "  195,\n",
       "  4,\n",
       "  9,\n",
       "  5,\n",
       "  7,\n",
       "  297,\n",
       "  229,\n",
       "  4,\n",
       "  16,\n",
       "  5,\n",
       "  7,\n",
       "  285,\n",
       "  220,\n",
       "  99,\n",
       "  4,\n",
       "  15,\n",
       "  5],\n",
       " 'Y': 'neg'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset[0]  # First review in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[243,\n",
       " 17,\n",
       " 314,\n",
       " 294,\n",
       " 77,\n",
       " 140,\n",
       " 307,\n",
       " 20,\n",
       " 68,\n",
       " 237,\n",
       " 6,\n",
       " 97,\n",
       " 34,\n",
       " 299,\n",
       " 98,\n",
       " 8,\n",
       " 302,\n",
       " 135,\n",
       " 167,\n",
       " 33,\n",
       " 22,\n",
       " 8,\n",
       " 226,\n",
       " 220,\n",
       " 297,\n",
       " 145,\n",
       " 87,\n",
       " 6,\n",
       " 60,\n",
       " 158,\n",
       " 136,\n",
       " 74,\n",
       " 307,\n",
       " 262,\n",
       " 157,\n",
       " 165,\n",
       " 153,\n",
       " 179,\n",
       " 6,\n",
       " 34,\n",
       " 149,\n",
       " 214,\n",
       " 8,\n",
       " 333,\n",
       " 2,\n",
       " 297,\n",
       " 82,\n",
       " 18,\n",
       " 326,\n",
       " 297,\n",
       " 204,\n",
       " 34,\n",
       " 19,\n",
       " 280,\n",
       " 19,\n",
       " 124,\n",
       " 230,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 79,\n",
       " 17,\n",
       " 20,\n",
       " 199,\n",
       " 204,\n",
       " 129,\n",
       " 297,\n",
       " 294,\n",
       " 133,\n",
       " 296,\n",
       " 311,\n",
       " 225,\n",
       " 20,\n",
       " 322,\n",
       " 75,\n",
       " 164,\n",
       " 6,\n",
       " 60,\n",
       " 245,\n",
       " 169,\n",
       " 165,\n",
       " 20,\n",
       " 322,\n",
       " 46,\n",
       " 234,\n",
       " 8,\n",
       " 337,\n",
       " 168,\n",
       " 333,\n",
       " 188,\n",
       " 304,\n",
       " 253,\n",
       " 33,\n",
       " 108,\n",
       " 148,\n",
       " 226,\n",
       " 307,\n",
       " 345,\n",
       " 6,\n",
       " 272,\n",
       " 163,\n",
       " 132,\n",
       " 37,\n",
       " 122,\n",
       " 337,\n",
       " 42,\n",
       " 307,\n",
       " 59,\n",
       " 297,\n",
       " 201,\n",
       " 6,\n",
       " 196,\n",
       " 341,\n",
       " 348,\n",
       " 152,\n",
       " 34,\n",
       " 290,\n",
       " 4,\n",
       " 185,\n",
       " 156,\n",
       " 1,\n",
       " 195,\n",
       " 5,\n",
       " 6,\n",
       " 60,\n",
       " 300,\n",
       " 38,\n",
       " 142,\n",
       " 34,\n",
       " 46,\n",
       " 328,\n",
       " 220,\n",
       " 189,\n",
       " 28,\n",
       " 315,\n",
       " 220,\n",
       " 122,\n",
       " 6,\n",
       " 34,\n",
       " 301,\n",
       " 128,\n",
       " 173,\n",
       " 86,\n",
       " 208,\n",
       " 276,\n",
       " 304,\n",
       " 226,\n",
       " 76,\n",
       " 8,\n",
       " 302,\n",
       " 263,\n",
       " 307,\n",
       " 150,\n",
       " 293,\n",
       " 304,\n",
       " 246,\n",
       " 209,\n",
       " 72,\n",
       " 6,\n",
       " 60,\n",
       " 113,\n",
       " 169,\n",
       " 295,\n",
       " 8,\n",
       " 277,\n",
       " 333,\n",
       " 38,\n",
       " 297,\n",
       " 248,\n",
       " 341,\n",
       " 297,\n",
       " 204,\n",
       " 18,\n",
       " 331,\n",
       " 6,\n",
       " 170,\n",
       " 186,\n",
       " 247,\n",
       " 168,\n",
       " 296,\n",
       " 169,\n",
       " 2,\n",
       " 271,\n",
       " 309,\n",
       " 172,\n",
       " 8,\n",
       " 169,\n",
       " 282,\n",
       " 221,\n",
       " 19,\n",
       " 216,\n",
       " 19,\n",
       " 60,\n",
       " 299,\n",
       " 95,\n",
       " 167,\n",
       " 304,\n",
       " 19,\n",
       " 116,\n",
       " 19,\n",
       " 342,\n",
       " 165,\n",
       " 337,\n",
       " 347,\n",
       " 6,\n",
       " 40,\n",
       " 33,\n",
       " 43,\n",
       " 194,\n",
       " 6,\n",
       " 150,\n",
       " 215,\n",
       " 164,\n",
       " 333,\n",
       " 2,\n",
       " 141,\n",
       " 225,\n",
       " 8,\n",
       " 300,\n",
       " 38,\n",
       " 96,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 64,\n",
       " 70,\n",
       " 45,\n",
       " 130,\n",
       " 297,\n",
       " 81,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 229,\n",
       " 339,\n",
       " 183,\n",
       " 180,\n",
       " 297,\n",
       " 81,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 286,\n",
       " 36,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 91,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 20,\n",
       " 184,\n",
       " 220,\n",
       " 65,\n",
       " 260,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 308,\n",
       " 220,\n",
       " 330,\n",
       " 303,\n",
       " 296,\n",
       " 147,\n",
       " 6,\n",
       " 34,\n",
       " 203,\n",
       " 220,\n",
       " 169,\n",
       " 168,\n",
       " 271,\n",
       " 217,\n",
       " 114,\n",
       " 8,\n",
       " 218,\n",
       " 163,\n",
       " 240,\n",
       " 92,\n",
       " 208,\n",
       " 198,\n",
       " 312,\n",
       " 307,\n",
       " 317,\n",
       " 20,\n",
       " 121,\n",
       " 110,\n",
       " 218,\n",
       " 34,\n",
       " 299,\n",
       " 6,\n",
       " 60,\n",
       " 335,\n",
       " 28,\n",
       " 169,\n",
       " 93,\n",
       " 168,\n",
       " 137,\n",
       " 190,\n",
       " 297,\n",
       " 259,\n",
       " 69,\n",
       " 231,\n",
       " 34,\n",
       " 231,\n",
       " 26,\n",
       " 6,\n",
       " 163,\n",
       " 135,\n",
       " 175,\n",
       " 220,\n",
       " 117,\n",
       " 320,\n",
       " 25,\n",
       " 20,\n",
       " 338,\n",
       " 6,\n",
       " 337,\n",
       " 168,\n",
       " 304,\n",
       " 121,\n",
       " 2,\n",
       " 54,\n",
       " 247,\n",
       " 8,\n",
       " 169,\n",
       " 2,\n",
       " 219,\n",
       " 143,\n",
       " 304,\n",
       " 53,\n",
       " 261,\n",
       " 307,\n",
       " 155,\n",
       " 6,\n",
       " 60,\n",
       " 169,\n",
       " 265,\n",
       " 307,\n",
       " 325,\n",
       " 307,\n",
       " 155,\n",
       " 169,\n",
       " 71,\n",
       " 319,\n",
       " 170,\n",
       " 123,\n",
       " 125,\n",
       " 200,\n",
       " 8,\n",
       " 34,\n",
       " 92,\n",
       " 302,\n",
       " 187,\n",
       " 303,\n",
       " 106,\n",
       " 6,\n",
       " 305,\n",
       " 228,\n",
       " 108,\n",
       " 103,\n",
       " 6,\n",
       " 165,\n",
       " 297,\n",
       " 192,\n",
       " 18,\n",
       " 217,\n",
       " 251,\n",
       " 8,\n",
       " 297,\n",
       " 256,\n",
       " 236,\n",
       " 168,\n",
       " 296,\n",
       " 297,\n",
       " 39,\n",
       " 34,\n",
       " 163,\n",
       " 57,\n",
       " 89,\n",
       " 225,\n",
       " 127,\n",
       " 180,\n",
       " 304,\n",
       " 6,\n",
       " 277,\n",
       " 329,\n",
       " 24,\n",
       " 120,\n",
       " 203,\n",
       " 220,\n",
       " 169,\n",
       " 230,\n",
       " 61,\n",
       " 297,\n",
       " 146,\n",
       " 244,\n",
       " 6,\n",
       " 277,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 287,\n",
       " 25,\n",
       " 296,\n",
       " 86,\n",
       " 281,\n",
       " 307,\n",
       " 187,\n",
       " 20,\n",
       " 182,\n",
       " 55,\n",
       " 220,\n",
       " 266,\n",
       " 6,\n",
       " 60,\n",
       " 169,\n",
       " 284,\n",
       " 86,\n",
       " 208,\n",
       " 297,\n",
       " 187,\n",
       " 297,\n",
       " 121,\n",
       " 28,\n",
       " 296,\n",
       " 202,\n",
       " 106,\n",
       " 8,\n",
       " 163,\n",
       " 144,\n",
       " 297,\n",
       " 58,\n",
       " 181,\n",
       " 341,\n",
       " 205,\n",
       " 180,\n",
       " 304,\n",
       " 168,\n",
       " 296,\n",
       " 347,\n",
       " 268,\n",
       " 31,\n",
       " 187,\n",
       " 292,\n",
       " 296,\n",
       " 297,\n",
       " 43,\n",
       " 168,\n",
       " 19,\n",
       " 167,\n",
       " 169,\n",
       " 19,\n",
       " 108,\n",
       " 51,\n",
       " 302,\n",
       " 38,\n",
       " 138,\n",
       " 297,\n",
       " 261,\n",
       " 238,\n",
       " 307,\n",
       " 104,\n",
       " 348,\n",
       " 342,\n",
       " 220,\n",
       " 316,\n",
       " 8,\n",
       " 163,\n",
       " 191,\n",
       " 6,\n",
       " 269,\n",
       " 193,\n",
       " 257,\n",
       " 254,\n",
       " 44,\n",
       " 130,\n",
       " 324,\n",
       " 129,\n",
       " 21,\n",
       " 11,\n",
       " 200,\n",
       " 306,\n",
       " 297,\n",
       " 204,\n",
       " 168,\n",
       " 173,\n",
       " 241,\n",
       " 178,\n",
       " 0,\n",
       " 0,\n",
       " 224,\n",
       " 6,\n",
       " 329,\n",
       " 135,\n",
       " 169,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 300,\n",
       " 38,\n",
       " 239,\n",
       " 66,\n",
       " 153,\n",
       " 34,\n",
       " 329,\n",
       " 92,\n",
       " 208,\n",
       " 176,\n",
       " 339,\n",
       " 302,\n",
       " 38,\n",
       " 8,\n",
       " 92,\n",
       " 329,\n",
       " 251,\n",
       " 210,\n",
       " 307,\n",
       " 262,\n",
       " 169,\n",
       " 231,\n",
       " 34,\n",
       " 231,\n",
       " 26,\n",
       " 18,\n",
       " 162,\n",
       " 21,\n",
       " 139,\n",
       " 321,\n",
       " 88,\n",
       " 260,\n",
       " 222,\n",
       " 131,\n",
       " 166,\n",
       " 167,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 287,\n",
       " 141,\n",
       " 94,\n",
       " 165,\n",
       " 297,\n",
       " 204,\n",
       " 18,\n",
       " 35,\n",
       " 6,\n",
       " 297,\n",
       " 289,\n",
       " 310,\n",
       " 304,\n",
       " 121,\n",
       " 44,\n",
       " 130,\n",
       " 170,\n",
       " 90,\n",
       " 34,\n",
       " 67,\n",
       " 169,\n",
       " 320,\n",
       " 298,\n",
       " 6,\n",
       " 34,\n",
       " 169,\n",
       " 270,\n",
       " 8,\n",
       " 300,\n",
       " 197,\n",
       " 3,\n",
       " 50,\n",
       " 20,\n",
       " 246,\n",
       " 83,\n",
       " 294,\n",
       " 199,\n",
       " 204,\n",
       " 165,\n",
       " 154,\n",
       " 279,\n",
       " 6,\n",
       " 60,\n",
       " 163,\n",
       " 144,\n",
       " 19,\n",
       " 297,\n",
       " 291,\n",
       " 19,\n",
       " 84,\n",
       " 296,\n",
       " 313,\n",
       " 169,\n",
       " 167,\n",
       " 20,\n",
       " 206,\n",
       " 323,\n",
       " 341,\n",
       " 182,\n",
       " 100,\n",
       " 6,\n",
       " 343,\n",
       " 187,\n",
       " 202,\n",
       " 266,\n",
       " 8,\n",
       " 297,\n",
       " 23,\n",
       " 38,\n",
       " 246,\n",
       " 142,\n",
       " 129,\n",
       " 297,\n",
       " 203,\n",
       " 236,\n",
       " 6,\n",
       " 30,\n",
       " 332,\n",
       " 52,\n",
       " 173,\n",
       " 264,\n",
       " 307,\n",
       " 47,\n",
       " 242,\n",
       " 297,\n",
       " 111,\n",
       " 259,\n",
       " 63,\n",
       " 296,\n",
       " 151,\n",
       " 86,\n",
       " 165,\n",
       " 32,\n",
       " 48,\n",
       " 6,\n",
       " 227,\n",
       " 165,\n",
       " 20,\n",
       " 212,\n",
       " 211,\n",
       " 8,\n",
       " 60,\n",
       " 207,\n",
       " 54,\n",
       " 177,\n",
       " 140,\n",
       " 230,\n",
       " 307,\n",
       " 257,\n",
       " 6,\n",
       " 339,\n",
       " 159,\n",
       " 153,\n",
       " 233,\n",
       " 306,\n",
       " 297,\n",
       " 107,\n",
       " 121,\n",
       " 6,\n",
       " 34,\n",
       " 24,\n",
       " 149,\n",
       " 347,\n",
       " 118,\n",
       " 153,\n",
       " 63,\n",
       " 2,\n",
       " 318,\n",
       " 8,\n",
       " 232,\n",
       " 6,\n",
       " 297,\n",
       " 121,\n",
       " 93,\n",
       " 208,\n",
       " 283,\n",
       " 49,\n",
       " 169,\n",
       " 93,\n",
       " 208,\n",
       " 105,\n",
       " 6,\n",
       " 169,\n",
       " 2,\n",
       " 73,\n",
       " 6,\n",
       " 169,\n",
       " 250,\n",
       " 112,\n",
       " 34,\n",
       " 169,\n",
       " 119,\n",
       " 246,\n",
       " 252,\n",
       " 129,\n",
       " 203,\n",
       " 220,\n",
       " 170,\n",
       " 255,\n",
       " 6,\n",
       " 85,\n",
       " 20,\n",
       " 246,\n",
       " 75,\n",
       " 102,\n",
       " 34,\n",
       " 115,\n",
       " 307,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 78,\n",
       " 296,\n",
       " 62,\n",
       " 51,\n",
       " 169,\n",
       " 8,\n",
       " 223,\n",
       " 6,\n",
       " 34,\n",
       " 61,\n",
       " 297,\n",
       " 327,\n",
       " 6,\n",
       " 304,\n",
       " 168,\n",
       " 217,\n",
       " 20,\n",
       " 160,\n",
       " 228,\n",
       " 294,\n",
       " 275,\n",
       " 126,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 169,\n",
       " 2,\n",
       " 173,\n",
       " 235,\n",
       " 307,\n",
       " 183,\n",
       " 296,\n",
       " 327,\n",
       " 49,\n",
       " 278,\n",
       " 168,\n",
       " 35,\n",
       " 41,\n",
       " 296,\n",
       " 297,\n",
       " 134,\n",
       " 168,\n",
       " 284,\n",
       " 161,\n",
       " 341,\n",
       " 297,\n",
       " 174,\n",
       " 8,\n",
       " 169,\n",
       " 29,\n",
       " 344,\n",
       " 249,\n",
       " 314,\n",
       " 346,\n",
       " 27,\n",
       " 34,\n",
       " 149,\n",
       " 50,\n",
       " 273,\n",
       " 225,\n",
       " 297,\n",
       " 267,\n",
       " 109,\n",
       " 272,\n",
       " 8,\n",
       " 334,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 274,\n",
       " 169,\n",
       " 0,\n",
       " 336,\n",
       " 2,\n",
       " 171,\n",
       " 70,\n",
       " 130,\n",
       " 18,\n",
       " 20,\n",
       " 213,\n",
       " 220,\n",
       " 101,\n",
       " 288,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 56,\n",
       " 340,\n",
       " 10,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 80,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 80,\n",
       " 17,\n",
       " 258,\n",
       " 4,\n",
       " 13,\n",
       " 5,\n",
       " 7,\n",
       " 185,\n",
       " 156,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 195,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 229,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 7,\n",
       " 285,\n",
       " 220,\n",
       " 99,\n",
       " 4,\n",
       " 15,\n",
       " 5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset[0]['X']  # Label of the first review in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1\"></a>\n",
    "\n",
    "# 3.1 Word2Vec Training\n",
    "\n",
    "Word2Vec has two training variants:\n",
    "\n",
    " - **Continuous Bag of Words (CBOW)**: Predict center word from (bag of) context words.\n",
    " - **Skip-grams**: Predict context words given center word.\n",
    "  \n",
    "Visually, they look like this:\n",
    "\n",
    "<img src=\"https://ibin.co/4UIznsOEyH7t.png\" width=\"500\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-1\"></a>\n",
    "\n",
    "## 3.1.1. CBOW\n",
    "\n",
    "CBOW windows through the sentence and picks out the center word as the `Y` and the surrounding context words as the inputs `X`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (4, None, None)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lazyme import per_window, per_chunk\n",
    "\n",
    "xx =[1,2,3,4]\n",
    "list(per_window(xx, n=2))\n",
    "list(per_chunk(xx, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_window(sequence, n=1):\n",
    "    \"\"\"\n",
    "    From http://stackoverflow.com/q/42220614/610569\n",
    "        >>> list(per_window([1,2,3,4], n=2))\n",
    "        [(1, 2), (2, 3), (3, 4)]\n",
    "        >>> list(per_window([1,2,3,4], n=3))\n",
    "        [(1, 2, 3), (2, 3, 4)]\n",
    "    \"\"\"\n",
    "    start, stop = 0, n\n",
    "    seq = list(sequence)\n",
    "    while stop <= len(seq):\n",
    "        yield seq[start:stop]\n",
    "        start += 1\n",
    "        stop += 1\n",
    "\n",
    "def cbow_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1\n",
    "    for window in per_window(tokens, n):\n",
    "        target = window.pop(window_size)\n",
    "        yield window, target   # X = window ; Y = target. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent0 = ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', \n",
    "         'and', 'language', 'is', 'essentially', 'non-random', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'choose', 'words'], 'never'),\n",
       " (['users', 'never', 'words', 'randomly'], 'choose'),\n",
       " (['never', 'choose', 'randomly', ','], 'words'),\n",
       " (['choose', 'words', ',', 'and'], 'randomly'),\n",
       " (['words', 'randomly', 'and', 'language'], ','),\n",
       " (['randomly', ',', 'language', 'is'], 'and'),\n",
       " ([',', 'and', 'is', 'essentially'], 'language'),\n",
       " (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
       " (['language', 'is', 'non-random', '.'], 'essentially')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'never', 'words', 'randomly', ','], 'choose'),\n",
       " (['users', 'never', 'choose', 'randomly', ',', 'and'], 'words'),\n",
       " (['never', 'choose', 'words', ',', 'and', 'language'], 'randomly'),\n",
       " (['choose', 'words', 'randomly', 'and', 'language', 'is'], ','),\n",
       " (['words', 'randomly', ',', 'language', 'is', 'essentially'], 'and'),\n",
       " (['randomly', ',', 'and', 'is', 'essentially', 'non-random'], 'language'),\n",
       " ([',', 'and', 'language', 'essentially', 'non-random', '.'], 'is')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-2\"></a>\n",
    "\n",
    "## 3.1.2. Skipgram\n",
    "\n",
    "Skipgram training windows through the sentence and pictures out the center word as the input `X` and the context words as the outputs `Y`, additionally, it will randommly sample words not in the window as **negative samples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1 \n",
    "    for i, window in enumerate(per_window(tokens, n)):\n",
    "        target = window.pop(window_size)\n",
    "        # Generate positive samples.\n",
    "        for context_word in window:\n",
    "            yield target, context_word, 1\n",
    "        # Generate negative samples.\n",
    "        for _ in range(n-1):\n",
    "            leftovers = tokens[:i] + tokens[i+n:]\n",
    "            yield target, random.choice(leftovers), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('never', 'language', 1),\n",
       " ('never', 'users', 1),\n",
       " ('never', 'choose', 1),\n",
       " ('never', 'words', 1),\n",
       " ('never', 'essentially', 0),\n",
       " ('never', '.', 0),\n",
       " ('never', 'essentially', 0),\n",
       " ('never', 'essentially', 0),\n",
       " ('choose', 'users', 1),\n",
       " ('choose', 'never', 1),\n",
       " ('choose', 'words', 1),\n",
       " ('choose', 'randomly', 1),\n",
       " ('choose', 'and', 0),\n",
       " ('choose', 'essentially', 0),\n",
       " ('choose', 'non-random', 0),\n",
       " ('choose', 'is', 0),\n",
       " ('words', 'never', 1),\n",
       " ('words', 'choose', 1),\n",
       " ('words', 'randomly', 1),\n",
       " ('words', ',', 1),\n",
       " ('words', 'language', 0),\n",
       " ('words', 'essentially', 0),\n",
       " ('words', 'is', 0),\n",
       " ('words', 'language', 0),\n",
       " ('randomly', 'choose', 1),\n",
       " ('randomly', 'words', 1),\n",
       " ('randomly', ',', 1),\n",
       " ('randomly', 'and', 1),\n",
       " ('randomly', 'users', 0),\n",
       " ('randomly', 'essentially', 0),\n",
       " ('randomly', 'language', 0),\n",
       " ('randomly', 'essentially', 0),\n",
       " (',', 'words', 1),\n",
       " (',', 'randomly', 1),\n",
       " (',', 'and', 1),\n",
       " (',', 'language', 1),\n",
       " (',', 'users', 0),\n",
       " (',', 'essentially', 0),\n",
       " (',', 'users', 0),\n",
       " (',', 'choose', 0),\n",
       " ('and', 'randomly', 1),\n",
       " ('and', ',', 1),\n",
       " ('and', 'language', 1),\n",
       " ('and', 'is', 1),\n",
       " ('and', 'language', 0),\n",
       " ('and', 'never', 0),\n",
       " ('and', 'choose', 0),\n",
       " ('and', 'essentially', 0),\n",
       " ('language', ',', 1),\n",
       " ('language', 'and', 1),\n",
       " ('language', 'is', 1),\n",
       " ('language', 'essentially', 1),\n",
       " ('language', 'randomly', 0),\n",
       " ('language', 'users', 0),\n",
       " ('language', 'choose', 0),\n",
       " ('language', 'choose', 0),\n",
       " ('is', 'and', 1),\n",
       " ('is', 'language', 1),\n",
       " ('is', 'essentially', 1),\n",
       " ('is', 'non-random', 1),\n",
       " ('is', '.', 0),\n",
       " ('is', '.', 0),\n",
       " ('is', '.', 0),\n",
       " ('is', 'users', 0),\n",
       " ('essentially', 'language', 1),\n",
       " ('essentially', 'is', 1),\n",
       " ('essentially', 'non-random', 1),\n",
       " ('essentially', '.', 1),\n",
       " ('essentially', ',', 0),\n",
       " ('essentially', 'randomly', 0),\n",
       " ('essentially', 'choose', 0),\n",
       " ('essentially', 'language', 0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(skipgram_iterator(sent0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut-away: What is `partial`?\n",
    "\n",
    "The [`functools.partial`](https://docs.python.org/3.7/library/functools.html#functools.partial) function in Python is a mechanism to overload a function with preset arguments. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# Generates bigrams\n",
    "list(ngrams('this is a sentence'.split(), n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# You can create a new function that \"preset\" the `n` argument, e.g.\n",
    "bigrams = partial(ngrams, n=2)\n",
    "trigrams = partial(ngrams, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is', 'a'), ('is', 'a', 'sentence')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigrams('this is a sentence'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams('this is a sentence'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-3\"></a>\n",
    "\n",
    "## 3.1.3 Word2Vec Dataset\n",
    "\n",
    "Now that we know what are the inputs `X` and outputs `Y` of the Word2Vec task. \n",
    "\n",
    "Lets put everything together and modify the `Dataset` so that `__getitem__` retrieves CBOW or Skipgram formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "\n",
    "    def cbow_iterator(self,tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield window, target   # X = window ; Y = target. \n",
    "\n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            target = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield target, context_word, 1\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                yield target, random.choice(leftovers), 0\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-3-hint\"></a>\n",
    "## Hints for the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
    "##hint_word2vec_dataset()\n",
    "\n",
    "# Option 2: \"I give up just, run the code for me\" \n",
    "# Uncomment the next two lines, if you really gave up... \n",
    "##full_code_word2vec_dataset()\n",
    "##from tsundoku.word2vec import Word2VecText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-hint\"></a>\n",
    "\n",
    "## 3.1.4. Train a CBOW model\n",
    "\n",
    "### Lets Get Some Data\n",
    "\n",
    "Lets take Kilgarriff (2005) , \"Language is never ever, ever random\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io #codecs\n",
    "\n",
    "\n",
    "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
    "if os.path.isfile('language-never-random.txt'):\n",
    "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
    "        text = fin.read()\n",
    "else:\n",
    "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
    "    text = requests.get(url).content.decode('utf8')\n",
    "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
    "        fout.write(text)\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Language is never, ever, ever, random\n",
      "\n",
      "                                                               ADAM KILGARRIFF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Abstract\n",
      "Language users never choose words randomly, and language is essentially\n",
      "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
      "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
      "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
      "data, we shall (almost) always be able to establish that it is not true. In\n",
      "corpus studies, we frequently do have enough data, so the fact that a rela-\n",
      "tion between two phenomena is demonstrably non-random, does not sup-\n",
      "port the inference that it is not arbitrary. We present experimental evidence\n",
      "of how arbitrary associations between word frequencies and corpora are\n",
      "systematically non-random. We review literature in which hypothesis test-\n",
      "ing has been used, and show how it has often led to unhelpful or mislead-\n",
      "ing results.\n",
      "Keywords: 쎲쎲쎲\n",
      "\n",
      "1. Int\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check, lets take a look at the data.\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bulk of linguistic questions concern the dis- tinction between a and m. a linguistic account of a phenomenon gen- erally gives us reason to view the relation between , for example , a verb ’ s syntax and its semantics , as motivated rather than arbitrary .\n",
      "\u001b[92mof\u001b[0m \t\t the bulk \u001b[91m______\u001b[0m linguistic questions\n",
      "\u001b[92mlinguistic\u001b[0m \t bulk of \u001b[91m______\u001b[0m questions concern\n",
      "\u001b[92mquestions\u001b[0m \t of linguistic \u001b[91m______\u001b[0m concern the\n",
      "\u001b[92mconcern\u001b[0m \t linguistic questions \u001b[91m______\u001b[0m the dis-\n",
      "\u001b[92mthe\u001b[0m \t\t questions concern \u001b[91m______\u001b[0m dis- tinction\n",
      "\u001b[92mdis-\u001b[0m \t\t concern the \u001b[91m______\u001b[0m tinction between\n",
      "\u001b[92mtinction\u001b[0m \t the dis- \u001b[91m______\u001b[0m between a\n",
      "\u001b[92mbetween\u001b[0m \t dis- tinction \u001b[91m______\u001b[0m a and\n",
      "\u001b[92ma\u001b[0m \t\t tinction between \u001b[91m______\u001b[0m and m.\n",
      "\u001b[92mand\u001b[0m \t\t between a \u001b[91m______\u001b[0m m. a\n",
      "\u001b[92mm.\u001b[0m \t\t a and \u001b[91m______\u001b[0m a linguistic\n",
      "\u001b[92ma\u001b[0m \t\t and m. \u001b[91m______\u001b[0m linguistic account\n",
      "\u001b[92mlinguistic\u001b[0m \t m. a \u001b[91m______\u001b[0m account of\n",
      "\u001b[92maccount\u001b[0m \t a linguistic \u001b[91m______\u001b[0m of a\n",
      "\u001b[92mof\u001b[0m \t\t linguistic account \u001b[91m______\u001b[0m a phenomenon\n",
      "\u001b[92ma\u001b[0m \t\t account of \u001b[91m______\u001b[0m phenomenon gen-\n",
      "\u001b[92mphenomenon\u001b[0m \t of a \u001b[91m______\u001b[0m gen- erally\n",
      "\u001b[92mgen-\u001b[0m \t\t a phenomenon \u001b[91m______\u001b[0m erally gives\n",
      "\u001b[92merally\u001b[0m \t\t phenomenon gen- \u001b[91m______\u001b[0m gives us\n",
      "\u001b[92mgives\u001b[0m \t\t gen- erally \u001b[91m______\u001b[0m us reason\n",
      "\u001b[92mus\u001b[0m \t\t erally gives \u001b[91m______\u001b[0m reason to\n",
      "\u001b[92mreason\u001b[0m \t\t gives us \u001b[91m______\u001b[0m to view\n",
      "\u001b[92mto\u001b[0m \t\t us reason \u001b[91m______\u001b[0m view the\n",
      "\u001b[92mview\u001b[0m \t\t reason to \u001b[91m______\u001b[0m the relation\n",
      "\u001b[92mthe\u001b[0m \t\t to view \u001b[91m______\u001b[0m relation between\n",
      "\u001b[92mrelation\u001b[0m \t view the \u001b[91m______\u001b[0m between ,\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91m______\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91m______\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91m______\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91m______\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91m______\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91m______\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
      "\u001b[92mand\u001b[0m \t\t s syntax \u001b[91m______\u001b[0m its semantics\n",
      "\u001b[92mits\u001b[0m \t\t syntax and \u001b[91m______\u001b[0m semantics ,\n",
      "\u001b[92msemantics\u001b[0m \t and its \u001b[91m______\u001b[0m , as\n",
      "\u001b[92m,\u001b[0m \t\t its semantics \u001b[91m______\u001b[0m as motivated\n",
      "\u001b[92mas\u001b[0m \t\t semantics , \u001b[91m______\u001b[0m motivated rather\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91m______\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91m______\u001b[0m arbitrary .\n"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "def visualize_predictions(x, y, prediction, vocab, window_size, unk='<unk>'):\n",
    "    left = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[:window_size]])\n",
    "    right = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[window_size:]])\n",
    "    target = vocab.get(int(y), '<unk>')\n",
    "\n",
    "    if not prediction:\n",
    "        predicted_word = '______'\n",
    "    else:\n",
    "        predicted_word = vocab.get(int(prediction), '<unk>') \n",
    "    print(color_str(target, 'green'), '\\t' if len(target) > 6 else '\\t\\t', \n",
    "          left, color_str(predicted_word, 'green' if target == predicted_word else 'red'), right)\n",
    "    \n",
    "\n",
    "sent_idx = 10\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')\n",
    "print(' '.join(w2v_dataset.sents[sent_idx]))\n",
    "for w2v_io in w2v_dataset[sent_idx]:\n",
    "    context, target = w2v_io\n",
    "    context, target = tensor(context).to(device), tensor(target).to(device)\n",
    "    visualize_predictions(context, target, None, w2v_dataset.vocab, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-cbow-model\"></a>\n",
    "\n",
    "## Fill-in the code for the CBOW Model\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"500\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Image from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take a closer look from the inputs to the first `nn.Linear`\n",
    "\n",
    "Cos after it reach the first `nn.Linear` it's just the same as our multi-layered perceptron example =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  8,  0,  7])\n",
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "# Lets take a look at the first output.\n",
    "x, y = w2v_dataset[0][0][0],  w2v_dataset[0][0][1], \n",
    "\n",
    "x = tensor(x)\n",
    "y = autograd.Variable(tensor(y, dtype=torch.long))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[ 1.0259,  0.9284, -2.2316,  0.0754, -1.4768],\n",
       "                      [-1.6520, -1.4818, -0.4483,  0.3457, -0.3436],\n",
       "                      [-0.2531, -0.1992,  0.7915,  0.1961,  0.2940],\n",
       "                      ...,\n",
       "                      [-0.2093,  0.4285, -1.8079,  0.7984,  0.3629],\n",
       "                      [ 1.6770, -0.8758,  0.6344, -0.0887, -0.1260],\n",
       "                      [ 0.5117,  1.1197,  0.3272, -0.5028, -1.0432]]))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_size = 5\n",
    "emb = nn.Embedding(len(w2v_dataset.vocab), embd_size)\n",
    "emb.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1388, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0259,  0.9284, -2.2316,  0.0754, -1.4768],\n",
       "        [-1.6520, -1.4818, -0.4483,  0.3457, -0.3436],\n",
       "        [-0.2531, -0.1992,  0.7915,  0.1961,  0.2940],\n",
       "        ...,\n",
       "        [-0.2093,  0.4285, -1.8079,  0.7984,  0.3629],\n",
       "        [ 1.6770, -0.8758,  0.6344, -0.0887, -0.1260],\n",
       "        [ 0.5117,  1.1197,  0.3272, -0.5028, -1.0432]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(emb.state_dict()['weight'].shape)\n",
    "emb.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "tensor([[-0.7855,  1.9982,  1.8082, -1.2010,  1.1291],\n",
      "        [ 1.2505,  0.3433, -0.8396, -0.7779,  0.3282],\n",
      "        [ 1.0259,  0.9284, -2.2316,  0.0754, -1.4768],\n",
      "        [-0.2633,  1.4716, -0.6771, -0.3750, -0.0344]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(emb(x).shape)\n",
    "print(emb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7855,  1.9982,  1.8082, -1.2010,  1.1291,  1.2505,  0.3433, -0.8396,\n",
       "         -0.7779,  0.3282,  1.0259,  0.9284, -2.2316,  0.0754, -1.4768, -0.2633,\n",
       "          1.4716, -0.6771, -0.3750, -0.0344]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(emb(x).view(1, -1).shape)\n",
    "emb(x).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.1593, -0.1181,  0.0052,  ...,  0.0510, -0.0207,  0.1492],\n",
      "        [ 0.0397, -0.1941,  0.1248,  ..., -0.0058,  0.0773,  0.0470],\n",
      "        [ 0.1236, -0.0977,  0.1557,  ..., -0.2216, -0.1264, -0.2094],\n",
      "        ...,\n",
      "        [-0.0113, -0.1606,  0.1912,  ..., -0.0442,  0.1083, -0.0970],\n",
      "        [ 0.0369,  0.1836, -0.0503,  ...,  0.2101,  0.0778,  0.0710],\n",
      "        [-0.0342, -0.0441,  0.2212,  ..., -0.1659, -0.0135, -0.1570]])), ('bias', tensor([-0.2075,  0.1603,  0.0227,  0.1867, -0.1603, -0.0867,  0.0629,  0.0969,\n",
      "         0.0912, -0.1158,  0.0868, -0.0484,  0.2205,  0.1823,  0.0326,  0.1284,\n",
      "         0.0667,  0.0550,  0.0826,  0.0422, -0.2216, -0.1200, -0.0927,  0.1577,\n",
      "         0.0349,  0.1249, -0.0598,  0.1617, -0.0725,  0.1232, -0.0378,  0.0363,\n",
      "        -0.1645,  0.2122, -0.0617,  0.2212,  0.1321,  0.1082,  0.0190, -0.1899,\n",
      "        -0.0055,  0.0886, -0.1133,  0.0539,  0.0648,  0.1851, -0.1185,  0.1833,\n",
      "         0.2210,  0.0527,  0.0450,  0.0052,  0.0745, -0.0754, -0.0770,  0.2141,\n",
      "         0.0223,  0.0324,  0.0222, -0.0257, -0.1286, -0.0582,  0.1500,  0.0201,\n",
      "        -0.1081,  0.1804,  0.0674,  0.0948,  0.1752, -0.1807,  0.0419, -0.0412,\n",
      "         0.0937,  0.0319,  0.0885, -0.2090,  0.0896, -0.1374, -0.0393, -0.0551,\n",
      "        -0.1823, -0.1615,  0.1240,  0.0472, -0.1347, -0.1523,  0.0285,  0.1203,\n",
      "        -0.0308, -0.2063,  0.0002, -0.1950,  0.1601,  0.0621,  0.0236,  0.2164,\n",
      "         0.0367, -0.1775,  0.0617,  0.0971]))])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "lin1 = nn.Linear(len(x)*embd_size, hidden_size)\n",
    "print(lin1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 20])\n",
      "tensor([[-0.1593, -0.1181,  0.0052,  ...,  0.0510, -0.0207,  0.1492],\n",
      "        [ 0.0397, -0.1941,  0.1248,  ..., -0.0058,  0.0773,  0.0470],\n",
      "        [ 0.1236, -0.0977,  0.1557,  ..., -0.2216, -0.1264, -0.2094],\n",
      "        ...,\n",
      "        [-0.0113, -0.1606,  0.1912,  ..., -0.0442,  0.1083, -0.0970],\n",
      "        [ 0.0369,  0.1836, -0.0503,  ...,  0.2101,  0.0778,  0.0710],\n",
      "        [-0.0342, -0.0441,  0.2212,  ..., -0.1659, -0.0135, -0.1570]])\n"
     ]
    }
   ],
   "source": [
    "print(lin1.state_dict()['weight'].shape)\n",
    "print(lin1.state_dict()['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3817,  0.9909,  0.7012, -0.3593, -0.0812, -0.6965, -1.2010, -0.4626,\n",
       "         -0.6172, -0.7549,  0.1895,  0.4077,  1.5212,  0.1513,  0.2127, -0.3394,\n",
       "          0.7259,  0.7881, -0.1431, -0.6342,  0.4919, -0.1866, -0.0076, -0.6322,\n",
       "         -0.2157, -0.0477,  1.1727,  0.7505,  0.7963,  0.8121, -1.0275,  1.2920,\n",
       "          0.2828,  1.0255,  0.9426,  0.5819,  0.2563, -0.0383,  0.0477, -0.3911,\n",
       "         -0.5229, -0.2008, -0.0466,  0.2601, -0.3392, -0.1089,  0.2009,  0.2212,\n",
       "          0.7352, -0.2152, -0.3420, -0.4976, -0.4798, -0.8921, -1.1753,  0.5407,\n",
       "          1.2047, -0.7487,  0.1302,  0.0670,  0.3469, -0.8901,  0.8454, -0.0210,\n",
       "         -0.0111, -1.3168, -0.1614,  0.4426,  0.7137,  0.3536,  1.2130,  0.1277,\n",
       "          0.3944,  0.1174,  0.5066, -0.2962,  0.4578, -0.0253, -0.8236,  0.2473,\n",
       "         -0.0625,  0.3411, -0.1997, -0.3721,  1.3432, -0.0701,  0.5318, -0.4238,\n",
       "          0.7625, -0.2290, -0.1641, -0.8921,  0.7023, -0.1720,  0.9794, -0.3581,\n",
       "          0.6254,  0.3997,  0.1924,  0.1104]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lin1(emb(x).view(1, -1)).shape)\n",
    "lin1(emb(x).view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.9909, 0.7012, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.1895, 0.4077, 1.5212, 0.1513, 0.2127, 0.0000, 0.7259, 0.7881,\n",
       "         0.0000, 0.0000, 0.4919, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1727,\n",
       "         0.7505, 0.7963, 0.8121, 0.0000, 1.2920, 0.2828, 1.0255, 0.9426, 0.5819,\n",
       "         0.2563, 0.0000, 0.0477, 0.0000, 0.0000, 0.0000, 0.0000, 0.2601, 0.0000,\n",
       "         0.0000, 0.2009, 0.2212, 0.7352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.5407, 1.2047, 0.0000, 0.1302, 0.0670, 0.3469, 0.0000, 0.8454,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.4426, 0.7137, 0.3536, 1.2130, 0.1277,\n",
       "         0.3944, 0.1174, 0.5066, 0.0000, 0.4578, 0.0000, 0.0000, 0.2473, 0.0000,\n",
       "         0.3411, 0.0000, 0.0000, 1.3432, 0.0000, 0.5318, 0.0000, 0.7625, 0.0000,\n",
       "         0.0000, 0.0000, 0.7023, 0.0000, 0.9794, 0.0000, 0.6254, 0.3997, 0.1924,\n",
       "         0.1104]], grad_fn=<ThresholdBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = nn.ReLU()\n",
    "print(relu(lin1(emb(x).view(1, -1))).shape)\n",
    "relu(lin1(emb(x).view(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1388, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0361,  0.0369,  0.0532,  ...,  0.0423,  0.0221, -0.0308],\n",
       "        [-0.0447,  0.0967,  0.0583,  ...,  0.0839, -0.0752,  0.0878],\n",
       "        [ 0.0610,  0.0059, -0.0166,  ..., -0.0994, -0.0846,  0.0311],\n",
       "        ...,\n",
       "        [ 0.0064,  0.0385,  0.0630,  ..., -0.0633,  0.0055,  0.0551],\n",
       "        [ 0.0945, -0.0582,  0.0583,  ...,  0.0669, -0.0209,  0.0890],\n",
       "        [ 0.0218,  0.0901, -0.0164,  ..., -0.0478, -0.0856,  0.0349]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin2 = nn.Linear(hidden_size, len(w2v_dataset.vocab))\n",
    "print(lin2.state_dict()['weight'].shape)\n",
    "lin2.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1388])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0706, -0.0801, -0.1645,  ..., -0.2212,  0.1059,  0.0386]],\n",
       "       grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_x = relu(lin1(emb(x).view(1, -1)))\n",
    "print(lin2(h_x).shape)\n",
    "lin2(h_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-7.342565059661865,\n",
       "  -7.352034091949463,\n",
       "  -7.436441421508789,\n",
       "  -7.377696990966797,\n",
       "  -7.01069974899292,\n",
       "  -7.53016996383667,\n",
       "  -7.369412899017334,\n",
       "  -7.332258701324463,\n",
       "  -7.271763801574707,\n",
       "  -7.078455924987793,\n",
       "  -7.277397632598877,\n",
       "  -7.313171863555908,\n",
       "  -7.820171356201172,\n",
       "  -7.720254898071289,\n",
       "  -6.781040191650391,\n",
       "  -6.903045177459717,\n",
       "  -7.287822723388672,\n",
       "  -7.216547012329102,\n",
       "  -6.778151988983154,\n",
       "  -7.384675025939941,\n",
       "  -7.458211421966553,\n",
       "  -7.07975959777832,\n",
       "  -7.00247859954834,\n",
       "  -7.403648853302002,\n",
       "  -7.398122787475586,\n",
       "  -7.428200721740723,\n",
       "  -7.719222545623779,\n",
       "  -7.587791919708252,\n",
       "  -7.422340393066406,\n",
       "  -6.894408226013184,\n",
       "  -7.0969061851501465,\n",
       "  -6.842694282531738,\n",
       "  -7.381572723388672,\n",
       "  -7.4432244300842285,\n",
       "  -7.286935329437256,\n",
       "  -7.695927143096924,\n",
       "  -7.322376728057861,\n",
       "  -7.581524848937988,\n",
       "  -7.459885120391846,\n",
       "  -7.195405960083008,\n",
       "  -7.331758975982666,\n",
       "  -7.192982196807861,\n",
       "  -6.9072699546813965,\n",
       "  -7.411691665649414,\n",
       "  -7.513472080230713,\n",
       "  -7.732112884521484,\n",
       "  -7.011186599731445,\n",
       "  -7.344358921051025,\n",
       "  -7.13160514831543,\n",
       "  -7.44354248046875,\n",
       "  -7.766861438751221,\n",
       "  -7.950305461883545,\n",
       "  -7.640159606933594,\n",
       "  -7.332925796508789,\n",
       "  -7.4140238761901855,\n",
       "  -7.254846096038818,\n",
       "  -7.532375335693359,\n",
       "  -7.188139915466309,\n",
       "  -7.791329383850098,\n",
       "  -7.018855094909668,\n",
       "  -7.05949592590332,\n",
       "  -7.4662909507751465,\n",
       "  -6.721582889556885,\n",
       "  -6.649184703826904,\n",
       "  -7.247725009918213,\n",
       "  -7.054534435272217,\n",
       "  -7.448642253875732,\n",
       "  -6.870738506317139,\n",
       "  -7.659281253814697,\n",
       "  -7.421648979187012,\n",
       "  -7.755733966827393,\n",
       "  -7.3198957443237305,\n",
       "  -7.565158367156982,\n",
       "  -7.418535232543945,\n",
       "  -7.273451805114746,\n",
       "  -6.789364814758301,\n",
       "  -7.321164131164551,\n",
       "  -7.4639763832092285,\n",
       "  -7.55604362487793,\n",
       "  -6.623182773590088,\n",
       "  -7.824520111083984,\n",
       "  -7.081608772277832,\n",
       "  -8.048443794250488,\n",
       "  -7.909060001373291,\n",
       "  -7.244775772094727,\n",
       "  -7.346780300140381,\n",
       "  -7.131319999694824,\n",
       "  -7.695276260375977,\n",
       "  -7.104046821594238,\n",
       "  -7.2742919921875,\n",
       "  -7.758687496185303,\n",
       "  -7.261149883270264,\n",
       "  -7.302948951721191,\n",
       "  -7.363513469696045,\n",
       "  -7.3228230476379395,\n",
       "  -7.501919746398926,\n",
       "  -7.667947769165039,\n",
       "  -7.698672294616699,\n",
       "  -7.405638217926025,\n",
       "  -7.291654586791992,\n",
       "  -7.546875,\n",
       "  -7.752751350402832,\n",
       "  -7.3800835609436035,\n",
       "  -6.912816047668457,\n",
       "  -7.332239151000977,\n",
       "  -7.22028923034668,\n",
       "  -7.542148113250732,\n",
       "  -7.632672309875488,\n",
       "  -6.981409072875977,\n",
       "  -7.064556121826172,\n",
       "  -7.313798427581787,\n",
       "  -7.499289512634277,\n",
       "  -7.775838851928711,\n",
       "  -6.9411187171936035,\n",
       "  -7.420770168304443,\n",
       "  -7.187160968780518,\n",
       "  -7.44695520401001,\n",
       "  -7.412728309631348,\n",
       "  -7.109606742858887,\n",
       "  -7.147791862487793,\n",
       "  -6.872706890106201,\n",
       "  -7.408993721008301,\n",
       "  -6.954714775085449,\n",
       "  -7.75414514541626,\n",
       "  -7.21194314956665,\n",
       "  -6.729889869689941,\n",
       "  -7.434017658233643,\n",
       "  -7.865017890930176,\n",
       "  -7.049446105957031,\n",
       "  -7.295742511749268,\n",
       "  -7.150589466094971,\n",
       "  -7.233712673187256,\n",
       "  -7.255618095397949,\n",
       "  -7.685530185699463,\n",
       "  -7.571415901184082,\n",
       "  -7.277926921844482,\n",
       "  -7.732110977172852,\n",
       "  -7.521116256713867,\n",
       "  -7.005281448364258,\n",
       "  -7.45933723449707,\n",
       "  -7.48215389251709,\n",
       "  -7.294833183288574,\n",
       "  -7.366273880004883,\n",
       "  -7.475279331207275,\n",
       "  -7.102585315704346,\n",
       "  -7.289089202880859,\n",
       "  -7.138826847076416,\n",
       "  -7.0830912590026855,\n",
       "  -6.9372148513793945,\n",
       "  -7.153040885925293,\n",
       "  -7.380837440490723,\n",
       "  -7.370218753814697,\n",
       "  -7.692346572875977,\n",
       "  -6.785661220550537,\n",
       "  -7.216305255889893,\n",
       "  -7.568228244781494,\n",
       "  -6.892762660980225,\n",
       "  -7.284834861755371,\n",
       "  -7.084409713745117,\n",
       "  -7.3714470863342285,\n",
       "  -7.163286209106445,\n",
       "  -7.419120788574219,\n",
       "  -7.429417610168457,\n",
       "  -7.30228328704834,\n",
       "  -7.769510746002197,\n",
       "  -6.85922908782959,\n",
       "  -7.1763691902160645,\n",
       "  -7.251667499542236,\n",
       "  -7.236004829406738,\n",
       "  -7.526759624481201,\n",
       "  -7.14306640625,\n",
       "  -7.3663411140441895,\n",
       "  -7.237022399902344,\n",
       "  -7.539064884185791,\n",
       "  -7.588262557983398,\n",
       "  -7.816995620727539,\n",
       "  -6.999974250793457,\n",
       "  -7.08871603012085,\n",
       "  -6.992528915405273,\n",
       "  -6.995221138000488,\n",
       "  -7.73825740814209,\n",
       "  -7.367706775665283,\n",
       "  -7.528983116149902,\n",
       "  -7.424352169036865,\n",
       "  -7.627473831176758,\n",
       "  -6.8221516609191895,\n",
       "  -7.315023422241211,\n",
       "  -7.313411235809326,\n",
       "  -7.452640056610107,\n",
       "  -7.305228233337402,\n",
       "  -7.5728983879089355,\n",
       "  -7.914958953857422,\n",
       "  -7.108800411224365,\n",
       "  -7.609751224517822,\n",
       "  -7.320036888122559,\n",
       "  -7.185580730438232,\n",
       "  -7.263134479522705,\n",
       "  -7.0276103019714355,\n",
       "  -7.269124984741211,\n",
       "  -7.917411804199219,\n",
       "  -7.438372611999512,\n",
       "  -7.403571128845215,\n",
       "  -7.003180027008057,\n",
       "  -7.493887424468994,\n",
       "  -7.167213439941406,\n",
       "  -7.127851486206055,\n",
       "  -6.705868244171143,\n",
       "  -7.144240379333496,\n",
       "  -7.3614935874938965,\n",
       "  -6.833152770996094,\n",
       "  -6.754533767700195,\n",
       "  -6.615863800048828,\n",
       "  -6.940723419189453,\n",
       "  -7.457719326019287,\n",
       "  -7.529072284698486,\n",
       "  -7.328733921051025,\n",
       "  -6.938953876495361,\n",
       "  -7.495744228363037,\n",
       "  -7.998396873474121,\n",
       "  -7.382148742675781,\n",
       "  -6.9414381980896,\n",
       "  -7.090930938720703,\n",
       "  -7.0996856689453125,\n",
       "  -7.579165458679199,\n",
       "  -7.280555725097656,\n",
       "  -7.293018817901611,\n",
       "  -7.107183456420898,\n",
       "  -6.895219802856445,\n",
       "  -7.215695858001709,\n",
       "  -7.208438396453857,\n",
       "  -7.5532755851745605,\n",
       "  -7.329314231872559,\n",
       "  -6.86418342590332,\n",
       "  -7.196802616119385,\n",
       "  -6.988729476928711,\n",
       "  -7.336119651794434,\n",
       "  -6.726412773132324,\n",
       "  -7.276796817779541,\n",
       "  -6.914240837097168,\n",
       "  -7.022818088531494,\n",
       "  -7.269989967346191,\n",
       "  -7.349888801574707,\n",
       "  -7.518797397613525,\n",
       "  -6.994275093078613,\n",
       "  -6.668516159057617,\n",
       "  -7.646738529205322,\n",
       "  -7.366190433502197,\n",
       "  -7.198705673217773,\n",
       "  -7.36336088180542,\n",
       "  -7.276960372924805,\n",
       "  -6.834537506103516,\n",
       "  -7.115744113922119,\n",
       "  -6.629999160766602,\n",
       "  -6.741608142852783,\n",
       "  -7.481395244598389,\n",
       "  -7.109864711761475,\n",
       "  -7.272745132446289,\n",
       "  -7.755606651306152,\n",
       "  -7.794126510620117,\n",
       "  -7.597561836242676,\n",
       "  -7.533989906311035,\n",
       "  -7.354764461517334,\n",
       "  -6.734579086303711,\n",
       "  -7.301926136016846,\n",
       "  -7.243198394775391,\n",
       "  -7.585321426391602,\n",
       "  -7.423705101013184,\n",
       "  -6.995099067687988,\n",
       "  -6.982061862945557,\n",
       "  -7.522104740142822,\n",
       "  -7.140885353088379,\n",
       "  -7.138980865478516,\n",
       "  -7.598891258239746,\n",
       "  -7.1706438064575195,\n",
       "  -7.643066883087158,\n",
       "  -7.408329010009766,\n",
       "  -7.469080924987793,\n",
       "  -6.904324054718018,\n",
       "  -7.2736029624938965,\n",
       "  -7.2073163986206055,\n",
       "  -7.297232151031494,\n",
       "  -7.327799320220947,\n",
       "  -7.269506454467773,\n",
       "  -6.746786117553711,\n",
       "  -7.368542194366455,\n",
       "  -7.627527713775635,\n",
       "  -7.365596771240234,\n",
       "  -7.510872840881348,\n",
       "  -7.381655693054199,\n",
       "  -7.337571620941162,\n",
       "  -7.9102678298950195,\n",
       "  -7.09402322769165,\n",
       "  -7.521430969238281,\n",
       "  -6.771214008331299,\n",
       "  -6.9130778312683105,\n",
       "  -6.989999294281006,\n",
       "  -7.435117721557617,\n",
       "  -7.6623854637146,\n",
       "  -7.38300085067749,\n",
       "  -7.611526966094971,\n",
       "  -6.793886184692383,\n",
       "  -6.904873847961426,\n",
       "  -7.027676105499268,\n",
       "  -7.220104217529297,\n",
       "  -7.594999313354492,\n",
       "  -6.8802103996276855,\n",
       "  -6.937457084655762,\n",
       "  -7.45756196975708,\n",
       "  -6.912670612335205,\n",
       "  -7.364788055419922,\n",
       "  -7.911971569061279,\n",
       "  -8.444602966308594,\n",
       "  -7.074816703796387,\n",
       "  -7.0541300773620605,\n",
       "  -7.671759128570557,\n",
       "  -7.406467437744141,\n",
       "  -7.362970352172852,\n",
       "  -7.455897331237793,\n",
       "  -7.22401762008667,\n",
       "  -7.070666313171387,\n",
       "  -7.664070129394531,\n",
       "  -7.392106533050537,\n",
       "  -6.9329938888549805,\n",
       "  -7.321108341217041,\n",
       "  -7.4941911697387695,\n",
       "  -7.418755054473877,\n",
       "  -7.2940545082092285,\n",
       "  -6.825368404388428,\n",
       "  -7.016528606414795,\n",
       "  -7.15154504776001,\n",
       "  -7.277509689331055,\n",
       "  -7.32952880859375,\n",
       "  -7.031369686126709,\n",
       "  -7.124285697937012,\n",
       "  -7.366163730621338,\n",
       "  -7.324612617492676,\n",
       "  -7.206256866455078,\n",
       "  -6.717113971710205,\n",
       "  -7.375510215759277,\n",
       "  -7.772408485412598,\n",
       "  -7.562753200531006,\n",
       "  -7.50832986831665,\n",
       "  -7.537235736846924,\n",
       "  -7.559813022613525,\n",
       "  -7.537477970123291,\n",
       "  -7.843732833862305,\n",
       "  -7.535058498382568,\n",
       "  -7.316134452819824,\n",
       "  -6.993300914764404,\n",
       "  -7.475541114807129,\n",
       "  -7.007467746734619,\n",
       "  -7.701364994049072,\n",
       "  -7.244971752166748,\n",
       "  -7.546498775482178,\n",
       "  -7.275445461273193,\n",
       "  -7.146230220794678,\n",
       "  -6.684336185455322,\n",
       "  -7.613691329956055,\n",
       "  -7.46049690246582,\n",
       "  -6.81187105178833,\n",
       "  -7.225009441375732,\n",
       "  -7.167760848999023,\n",
       "  -6.803645610809326,\n",
       "  -7.39957332611084,\n",
       "  -7.324775218963623,\n",
       "  -7.0306620597839355,\n",
       "  -7.215296268463135,\n",
       "  -7.506420135498047,\n",
       "  -7.479597568511963,\n",
       "  -7.138192653656006,\n",
       "  -7.667192459106445,\n",
       "  -7.252482891082764,\n",
       "  -7.270402431488037,\n",
       "  -6.906991004943848,\n",
       "  -7.027338981628418,\n",
       "  -7.756340026855469,\n",
       "  -7.281421184539795,\n",
       "  -7.232121467590332,\n",
       "  -7.663069725036621,\n",
       "  -6.516870498657227,\n",
       "  -7.046401023864746,\n",
       "  -6.8760151863098145,\n",
       "  -7.176550388336182,\n",
       "  -6.800760269165039,\n",
       "  -7.283796787261963,\n",
       "  -7.626253128051758,\n",
       "  -7.032806873321533,\n",
       "  -7.368738651275635,\n",
       "  -7.0894646644592285,\n",
       "  -7.684166431427002,\n",
       "  -6.707188606262207,\n",
       "  -7.198296070098877,\n",
       "  -6.795511722564697,\n",
       "  -7.1065168380737305,\n",
       "  -7.062718868255615,\n",
       "  -7.70686149597168,\n",
       "  -7.202740669250488,\n",
       "  -6.896486759185791,\n",
       "  -7.885416507720947,\n",
       "  -7.328592300415039,\n",
       "  -7.6148762702941895,\n",
       "  -7.069987773895264,\n",
       "  -7.215329647064209,\n",
       "  -7.6105804443359375,\n",
       "  -7.845773220062256,\n",
       "  -7.166096210479736,\n",
       "  -7.5638108253479,\n",
       "  -7.465539455413818,\n",
       "  -6.899261951446533,\n",
       "  -7.224806785583496,\n",
       "  -7.429896831512451,\n",
       "  -7.402638912200928,\n",
       "  -7.111022472381592,\n",
       "  -7.445810794830322,\n",
       "  -6.781496524810791,\n",
       "  -7.337244987487793,\n",
       "  -7.530787467956543,\n",
       "  -7.72047758102417,\n",
       "  -7.269944667816162,\n",
       "  -7.278416156768799,\n",
       "  -6.69783353805542,\n",
       "  -7.35966682434082,\n",
       "  -6.746250629425049,\n",
       "  -7.671627044677734,\n",
       "  -7.272942066192627,\n",
       "  -7.452761650085449,\n",
       "  -7.10349702835083,\n",
       "  -7.200203895568848,\n",
       "  -7.3054633140563965,\n",
       "  -7.430575370788574,\n",
       "  -7.404758930206299,\n",
       "  -6.874661445617676,\n",
       "  -7.005904197692871,\n",
       "  -6.916507720947266,\n",
       "  -6.9974236488342285,\n",
       "  -7.643132209777832,\n",
       "  -7.189327716827393,\n",
       "  -7.204119682312012,\n",
       "  -6.849287509918213,\n",
       "  -7.182546138763428,\n",
       "  -6.992489814758301,\n",
       "  -7.30169677734375,\n",
       "  -7.414422512054443,\n",
       "  -7.1244964599609375,\n",
       "  -7.462353706359863,\n",
       "  -7.350437641143799,\n",
       "  -7.44608736038208,\n",
       "  -7.377982139587402,\n",
       "  -6.940378665924072,\n",
       "  -7.072391986846924,\n",
       "  -6.8453755378723145,\n",
       "  -6.9757304191589355,\n",
       "  -7.311065673828125,\n",
       "  -7.048783302307129,\n",
       "  -7.0365309715271,\n",
       "  -7.172735214233398,\n",
       "  -6.980831146240234,\n",
       "  -7.802964687347412,\n",
       "  -7.457393646240234,\n",
       "  -7.833160400390625,\n",
       "  -7.461478233337402,\n",
       "  -7.2249650955200195,\n",
       "  -7.301571369171143,\n",
       "  -7.063932418823242,\n",
       "  -7.022072792053223,\n",
       "  -7.926488876342773,\n",
       "  -7.489253520965576,\n",
       "  -7.124792575836182,\n",
       "  -7.5549139976501465,\n",
       "  -7.08787202835083,\n",
       "  -7.275861740112305,\n",
       "  -7.502763748168945,\n",
       "  -6.9382853507995605,\n",
       "  -7.523149490356445,\n",
       "  -6.8848371505737305,\n",
       "  -7.310749530792236,\n",
       "  -7.321018695831299,\n",
       "  -7.777126312255859,\n",
       "  -7.732624053955078,\n",
       "  -7.386718273162842,\n",
       "  -7.472094535827637,\n",
       "  -6.993090629577637,\n",
       "  -7.448188304901123,\n",
       "  -7.435852527618408,\n",
       "  -7.655395030975342,\n",
       "  -6.994107723236084,\n",
       "  -7.776130676269531,\n",
       "  -7.5051069259643555,\n",
       "  -7.8590545654296875,\n",
       "  -6.8542022705078125,\n",
       "  -7.706416130065918,\n",
       "  -7.247676849365234,\n",
       "  -7.250974655151367,\n",
       "  -7.063349723815918,\n",
       "  -7.401564598083496,\n",
       "  -7.50772762298584,\n",
       "  -7.013532638549805,\n",
       "  -7.547735691070557,\n",
       "  -7.563559055328369,\n",
       "  -7.322993755340576,\n",
       "  -7.252795696258545,\n",
       "  -7.374425888061523,\n",
       "  -7.3255534172058105,\n",
       "  -7.390781879425049,\n",
       "  -7.773882865905762,\n",
       "  -6.9658684730529785,\n",
       "  -7.348917484283447,\n",
       "  -7.038837432861328,\n",
       "  -7.3583550453186035,\n",
       "  -7.153871536254883,\n",
       "  -7.446378707885742,\n",
       "  -7.403281211853027,\n",
       "  -7.572136402130127,\n",
       "  -7.631246089935303,\n",
       "  -6.876864433288574,\n",
       "  -7.628512859344482,\n",
       "  -7.445290565490723,\n",
       "  -6.867120742797852,\n",
       "  -7.095217704772949,\n",
       "  -7.091185569763184,\n",
       "  -7.129709720611572,\n",
       "  -7.112361907958984,\n",
       "  -7.725095272064209,\n",
       "  -7.663609981536865,\n",
       "  -6.942230701446533,\n",
       "  -7.728233337402344,\n",
       "  -7.120335578918457,\n",
       "  -7.165091514587402,\n",
       "  -7.330891132354736,\n",
       "  -7.1589436531066895,\n",
       "  -7.219503879547119,\n",
       "  -7.337162017822266,\n",
       "  -7.394081115722656,\n",
       "  -6.859801769256592,\n",
       "  -7.790342330932617,\n",
       "  -7.007256031036377,\n",
       "  -6.9809250831604,\n",
       "  -7.507204055786133,\n",
       "  -6.963541030883789,\n",
       "  -7.151590347290039,\n",
       "  -7.524096965789795,\n",
       "  -7.224401950836182,\n",
       "  -7.254215717315674,\n",
       "  -6.732385158538818,\n",
       "  -7.826225280761719,\n",
       "  -7.11438512802124,\n",
       "  -7.0161452293396,\n",
       "  -7.202518939971924,\n",
       "  -6.965057373046875,\n",
       "  -7.068140983581543,\n",
       "  -7.292285919189453,\n",
       "  -7.237776279449463,\n",
       "  -7.224551200866699,\n",
       "  -7.165229320526123,\n",
       "  -7.444632053375244,\n",
       "  -7.567286968231201,\n",
       "  -7.683502197265625,\n",
       "  -7.450010299682617,\n",
       "  -7.350048542022705,\n",
       "  -6.90170431137085,\n",
       "  -6.987922191619873,\n",
       "  -7.016374111175537,\n",
       "  -7.368941783905029,\n",
       "  -7.5992536544799805,\n",
       "  -6.977142810821533,\n",
       "  -7.5815043449401855,\n",
       "  -7.1936936378479,\n",
       "  -7.091170787811279,\n",
       "  -6.942511558532715,\n",
       "  -6.738874912261963,\n",
       "  -7.5317816734313965,\n",
       "  -7.109158992767334,\n",
       "  -7.136721134185791,\n",
       "  -6.908953666687012,\n",
       "  -7.104117393493652,\n",
       "  -7.564173221588135,\n",
       "  -6.9320454597473145,\n",
       "  -7.733304023742676,\n",
       "  -7.641705513000488,\n",
       "  -7.8545308113098145,\n",
       "  -7.609691619873047,\n",
       "  -7.259505748748779,\n",
       "  -7.045719623565674,\n",
       "  -7.443587779998779,\n",
       "  -7.429831504821777,\n",
       "  -7.185054779052734,\n",
       "  -7.408022403717041,\n",
       "  -7.130431175231934,\n",
       "  -6.941194534301758,\n",
       "  -7.181407451629639,\n",
       "  -6.749831676483154,\n",
       "  -7.26469087600708,\n",
       "  -7.560708045959473,\n",
       "  -6.93300199508667,\n",
       "  -7.227470874786377,\n",
       "  -7.151060581207275,\n",
       "  -7.182284832000732,\n",
       "  -7.336628437042236,\n",
       "  -7.249540328979492,\n",
       "  -6.887758255004883,\n",
       "  -7.246616363525391,\n",
       "  -7.076268196105957,\n",
       "  -7.117690563201904,\n",
       "  -7.383800029754639,\n",
       "  -7.079592704772949,\n",
       "  -7.296486854553223,\n",
       "  -7.733968734741211,\n",
       "  -7.373895645141602,\n",
       "  -7.279116630554199,\n",
       "  -7.22833251953125,\n",
       "  -6.787823677062988,\n",
       "  -7.563908100128174,\n",
       "  -6.892696380615234,\n",
       "  -7.457522392272949,\n",
       "  -7.500964164733887,\n",
       "  -7.478036403656006,\n",
       "  -6.875889301300049,\n",
       "  -7.202282905578613,\n",
       "  -6.700801849365234,\n",
       "  -7.338911056518555,\n",
       "  -7.090187072753906,\n",
       "  -7.332836151123047,\n",
       "  -7.330267429351807,\n",
       "  -7.400913238525391,\n",
       "  -6.859579563140869,\n",
       "  -7.673948764801025,\n",
       "  -7.391711235046387,\n",
       "  -7.15350866317749,\n",
       "  -7.037961006164551,\n",
       "  -7.32826566696167,\n",
       "  -7.822186470031738,\n",
       "  -6.355149745941162,\n",
       "  -7.388801097869873,\n",
       "  -7.175313472747803,\n",
       "  -7.657102584838867,\n",
       "  -7.365084171295166,\n",
       "  -6.837454795837402,\n",
       "  -7.637314796447754,\n",
       "  -7.920080661773682,\n",
       "  -7.427596569061279,\n",
       "  -7.454284191131592,\n",
       "  -6.946552276611328,\n",
       "  -7.776351451873779,\n",
       "  -7.620631694793701,\n",
       "  -7.253060817718506,\n",
       "  -7.5318827629089355,\n",
       "  -7.507433891296387,\n",
       "  -7.745174407958984,\n",
       "  -7.095500469207764,\n",
       "  -7.01446533203125,\n",
       "  -7.179068565368652,\n",
       "  -7.336534023284912,\n",
       "  -7.300094127655029,\n",
       "  -7.030449390411377,\n",
       "  -6.743716239929199,\n",
       "  -7.4028520584106445,\n",
       "  -7.327149868011475,\n",
       "  -6.8302388191223145,\n",
       "  -7.200062274932861,\n",
       "  -7.252755641937256,\n",
       "  -7.44210147857666,\n",
       "  -7.085052967071533,\n",
       "  -7.639490127563477,\n",
       "  -6.981562614440918,\n",
       "  -7.456074237823486,\n",
       "  -7.503430366516113,\n",
       "  -7.349921226501465,\n",
       "  -7.073153495788574,\n",
       "  -7.845168590545654,\n",
       "  -7.780172348022461,\n",
       "  -7.126662254333496,\n",
       "  -7.336547374725342,\n",
       "  -7.288488864898682,\n",
       "  -7.469818115234375,\n",
       "  -7.450056076049805,\n",
       "  -7.481645584106445,\n",
       "  -7.226147174835205,\n",
       "  -7.111606597900391,\n",
       "  -7.288023948669434,\n",
       "  -7.8421101570129395,\n",
       "  -7.3953680992126465,\n",
       "  -6.873445510864258,\n",
       "  -6.881316661834717,\n",
       "  -6.99745512008667,\n",
       "  -6.846065521240234,\n",
       "  -7.44179630279541,\n",
       "  -6.883333683013916,\n",
       "  -7.010368347167969,\n",
       "  -7.208815097808838,\n",
       "  -6.864523410797119,\n",
       "  -7.35950231552124,\n",
       "  -7.826318264007568,\n",
       "  -7.635123252868652,\n",
       "  -7.578888893127441,\n",
       "  -6.874206066131592,\n",
       "  -7.189717769622803,\n",
       "  -8.03766918182373,\n",
       "  -7.108039379119873,\n",
       "  -6.981987953186035,\n",
       "  -7.163764476776123,\n",
       "  -7.412972927093506,\n",
       "  -7.459540367126465,\n",
       "  -7.201329231262207,\n",
       "  -6.97387170791626,\n",
       "  -7.558848857879639,\n",
       "  -7.2279486656188965,\n",
       "  -7.45003604888916,\n",
       "  -7.505661487579346,\n",
       "  -7.212707042694092,\n",
       "  -7.386593341827393,\n",
       "  -7.285557746887207,\n",
       "  -7.664431095123291,\n",
       "  -7.707221984863281,\n",
       "  -7.1455841064453125,\n",
       "  -7.554006099700928,\n",
       "  -7.275671005249023,\n",
       "  -6.953773498535156,\n",
       "  -6.790340423583984,\n",
       "  -6.794142246246338,\n",
       "  -6.938196659088135,\n",
       "  -7.6048736572265625,\n",
       "  -7.127894401550293,\n",
       "  -7.124716281890869,\n",
       "  -6.9617085456848145,\n",
       "  -6.61056661605835,\n",
       "  -7.124266624450684,\n",
       "  -7.510687828063965,\n",
       "  -7.257534027099609,\n",
       "  -7.25827693939209,\n",
       "  -7.169654369354248,\n",
       "  -7.541716575622559,\n",
       "  -7.027942657470703,\n",
       "  -7.610084056854248,\n",
       "  -7.2801713943481445,\n",
       "  -8.082901000976562,\n",
       "  -7.537815093994141,\n",
       "  -7.042580604553223,\n",
       "  -6.784124374389648,\n",
       "  -7.217372894287109,\n",
       "  -7.172996520996094,\n",
       "  -6.868606090545654,\n",
       "  -7.494089603424072,\n",
       "  -6.904828071594238,\n",
       "  -7.471760272979736,\n",
       "  -7.549132347106934,\n",
       "  -7.409234046936035,\n",
       "  -7.253552436828613,\n",
       "  -7.415284156799316,\n",
       "  -6.989514350891113,\n",
       "  -7.473537921905518,\n",
       "  -7.260258674621582,\n",
       "  -7.312020301818848,\n",
       "  -7.713156700134277,\n",
       "  -6.945180892944336,\n",
       "  -7.61812162399292,\n",
       "  -7.039781093597412,\n",
       "  -7.290689945220947,\n",
       "  -7.054896831512451,\n",
       "  -7.5205254554748535,\n",
       "  -7.625087261199951,\n",
       "  -7.277324199676514,\n",
       "  -7.3133111000061035,\n",
       "  -7.435722827911377,\n",
       "  -7.139792442321777,\n",
       "  -7.033107757568359,\n",
       "  -7.032423973083496,\n",
       "  -7.4580864906311035,\n",
       "  -7.6988067626953125,\n",
       "  -7.198246955871582,\n",
       "  -7.011642932891846,\n",
       "  -6.763094902038574,\n",
       "  -7.227375030517578,\n",
       "  -7.295984268188477,\n",
       "  -7.114225387573242,\n",
       "  -7.198017597198486,\n",
       "  -7.062535285949707,\n",
       "  -7.593044757843018,\n",
       "  -7.378256320953369,\n",
       "  -7.046135902404785,\n",
       "  -7.652467250823975,\n",
       "  -7.694798469543457,\n",
       "  -7.3286452293396,\n",
       "  -6.7408576011657715,\n",
       "  -7.339087009429932,\n",
       "  -7.325831890106201,\n",
       "  -7.38101053237915,\n",
       "  -7.399515151977539,\n",
       "  -7.900956630706787,\n",
       "  -7.17628812789917,\n",
       "  -7.43010950088501,\n",
       "  -7.255584716796875,\n",
       "  -7.6205153465271,\n",
       "  -7.408307075500488,\n",
       "  -6.723502159118652,\n",
       "  -7.0898261070251465,\n",
       "  -7.9322381019592285,\n",
       "  -7.592748641967773,\n",
       "  -6.860795974731445,\n",
       "  -7.3308210372924805,\n",
       "  -6.779940128326416,\n",
       "  -7.744483470916748,\n",
       "  -6.882345676422119,\n",
       "  -7.172725677490234,\n",
       "  -7.14694356918335,\n",
       "  -6.884965896606445,\n",
       "  -7.319926738739014,\n",
       "  -7.35477352142334,\n",
       "  -7.593694686889648,\n",
       "  -7.560860633850098,\n",
       "  -7.486804485321045,\n",
       "  -7.300540924072266,\n",
       "  -6.932495594024658,\n",
       "  -7.759702205657959,\n",
       "  -7.112460613250732,\n",
       "  -7.3948822021484375,\n",
       "  -7.298374652862549,\n",
       "  -7.417218208312988,\n",
       "  -6.682626247406006,\n",
       "  -6.605127811431885,\n",
       "  -7.644781589508057,\n",
       "  -7.426701068878174,\n",
       "  -7.1826066970825195,\n",
       "  -7.822803020477295,\n",
       "  -7.763153553009033,\n",
       "  -6.958465576171875,\n",
       "  -7.048401832580566,\n",
       "  -7.291678428649902,\n",
       "  -6.45983362197876,\n",
       "  -7.043432712554932,\n",
       "  -6.7934417724609375,\n",
       "  -7.123137950897217,\n",
       "  -7.260185718536377,\n",
       "  -7.149026870727539,\n",
       "  -7.521256446838379,\n",
       "  -7.952821254730225,\n",
       "  -7.260624885559082,\n",
       "  -7.6270670890808105,\n",
       "  -7.131175518035889,\n",
       "  -7.4800801277160645,\n",
       "  -7.579826831817627,\n",
       "  -7.444785118103027,\n",
       "  -7.6591033935546875,\n",
       "  -7.077716827392578,\n",
       "  -7.1882853507995605,\n",
       "  -7.596872329711914,\n",
       "  -7.057600498199463,\n",
       "  -7.642368793487549,\n",
       "  -7.307226181030273,\n",
       "  -7.259410858154297,\n",
       "  -7.474154472351074,\n",
       "  -6.919101238250732,\n",
       "  -7.591753005981445,\n",
       "  -7.21129846572876,\n",
       "  -7.275036334991455,\n",
       "  -7.208195686340332,\n",
       "  -6.988381385803223,\n",
       "  -7.417779922485352,\n",
       "  -6.6253767013549805,\n",
       "  -7.247653484344482,\n",
       "  -7.472597122192383,\n",
       "  -6.719795227050781,\n",
       "  -7.461304187774658,\n",
       "  -7.283395767211914,\n",
       "  -7.025635719299316,\n",
       "  -7.54007625579834,\n",
       "  -7.44100284576416,\n",
       "  -6.831541538238525,\n",
       "  -7.338146686553955,\n",
       "  -7.352335453033447,\n",
       "  -7.242012977600098,\n",
       "  -7.478127956390381,\n",
       "  -7.298892974853516,\n",
       "  -7.60573673248291,\n",
       "  -7.108908653259277,\n",
       "  -7.014320373535156,\n",
       "  -7.315193176269531,\n",
       "  -7.302489280700684,\n",
       "  -7.820532321929932,\n",
       "  -6.982687473297119,\n",
       "  -6.920811176300049,\n",
       "  -6.974045276641846,\n",
       "  -6.837212562561035,\n",
       "  -6.856856346130371,\n",
       "  -7.0819549560546875,\n",
       "  -7.2652268409729,\n",
       "  -7.639793872833252,\n",
       "  -6.755863189697266,\n",
       "  -7.587243556976318,\n",
       "  -7.357578754425049,\n",
       "  -7.553517818450928,\n",
       "  -7.177000045776367,\n",
       "  -6.952907562255859,\n",
       "  -7.254632472991943,\n",
       "  -7.476349353790283,\n",
       "  -7.63064432144165,\n",
       "  -7.41227912902832,\n",
       "  -7.201216697692871,\n",
       "  -6.907696723937988,\n",
       "  -7.6010260581970215,\n",
       "  -7.220673561096191,\n",
       "  -7.523873329162598,\n",
       "  -7.44284200668335,\n",
       "  -7.038978576660156,\n",
       "  -7.409326553344727,\n",
       "  -7.224865913391113,\n",
       "  -6.980230808258057,\n",
       "  -7.22709321975708,\n",
       "  -7.41564416885376,\n",
       "  -6.900612831115723,\n",
       "  -7.425025939941406,\n",
       "  -7.570137023925781,\n",
       "  -7.737504959106445,\n",
       "  -7.493899822235107,\n",
       "  -6.999817848205566,\n",
       "  -6.871204853057861,\n",
       "  -7.274578094482422,\n",
       "  -7.17930793762207,\n",
       "  -7.475224018096924,\n",
       "  -7.178646087646484,\n",
       "  -6.858139991760254,\n",
       "  -7.848759651184082,\n",
       "  -6.499496936798096,\n",
       "  -7.059116363525391,\n",
       "  -7.104677677154541,\n",
       "  -6.647297382354736,\n",
       "  -7.24139928817749,\n",
       "  -7.829359531402588,\n",
       "  -7.361724376678467,\n",
       "  -7.4863481521606445,\n",
       "  -7.439985752105713,\n",
       "  -6.804434299468994,\n",
       "  -7.121057033538818,\n",
       "  -7.7914605140686035,\n",
       "  -7.014050483703613,\n",
       "  -7.431624889373779,\n",
       "  -6.781638145446777,\n",
       "  -7.208512306213379,\n",
       "  -7.090919494628906,\n",
       "  -7.291939735412598,\n",
       "  -7.155921936035156,\n",
       "  -7.950047016143799,\n",
       "  -7.265888214111328,\n",
       "  -7.13953971862793,\n",
       "  -7.599184513092041,\n",
       "  -7.418667793273926,\n",
       "  -6.815933704376221,\n",
       "  -7.832486629486084,\n",
       "  -7.542263507843018,\n",
       "  -7.519629955291748,\n",
       "  -7.611638069152832,\n",
       "  -7.467029094696045,\n",
       "  -7.423983097076416,\n",
       "  -6.584983825683594,\n",
       "  -7.25335693359375,\n",
       "  -7.365310192108154,\n",
       "  -7.752492427825928,\n",
       "  -6.925881385803223,\n",
       "  -7.249834060668945,\n",
       "  -7.986162185668945,\n",
       "  -7.659649848937988,\n",
       "  -7.719338893890381,\n",
       "  -7.69366979598999,\n",
       "  -7.050897598266602,\n",
       "  -6.969481468200684,\n",
       "  -7.187069416046143,\n",
       "  -7.188048839569092,\n",
       "  -7.193037986755371,\n",
       "  -7.165302753448486,\n",
       "  -7.096136569976807,\n",
       "  -6.8579912185668945,\n",
       "  -6.410971641540527,\n",
       "  -7.718267440795898,\n",
       "  -7.21872615814209,\n",
       "  -7.391530513763428,\n",
       "  -6.754485130310059,\n",
       "  -7.309675693511963,\n",
       "  -7.527035236358643,\n",
       "  -7.3672943115234375,\n",
       "  -7.007488250732422,\n",
       "  -7.132826805114746,\n",
       "  -6.861506938934326,\n",
       "  -7.329662322998047,\n",
       "  -7.096203804016113,\n",
       "  -7.426604270935059,\n",
       "  -7.211907386779785,\n",
       "  -7.584606647491455,\n",
       "  -7.461597919464111,\n",
       "  -6.6236114501953125,\n",
       "  -7.673437595367432,\n",
       "  -7.184278964996338,\n",
       "  -7.078864574432373,\n",
       "  -7.4582719802856445,\n",
       "  -7.519623279571533,\n",
       "  -6.98124885559082,\n",
       "  -6.804260730743408,\n",
       "  -7.1518473625183105,\n",
       "  -6.86397123336792,\n",
       "  -7.0595479011535645,\n",
       "  -7.4108147621154785,\n",
       "  -7.487212181091309,\n",
       "  ...]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.LogSoftmax(dim=1)\n",
    "softmax(lin2(h_x)).detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-6.3551], grad_fn=<MaxBackward0>), tensor([631]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the index with highest softmax probabilities\n",
    "torch.max(softmax(lin2(h_x)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-train-cbow\"></a>\n",
    "\n",
    "# Now, we train the CBOW model for real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211, 24)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we split the data into training and testing.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenized_text_train, tokenized_text_test = train_test_split(tokenized_text, test_size=0.1, random_state=42)\n",
    "len(tokenized_text_train), len(tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:43<00:00,  4.04s/it]\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.003\n",
    "hidden_size = 100\n",
    "window_size = 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, embd_size, window_size, hidden_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x, y = w2v_io\n",
    "            x = tensor(x).to(device)\n",
    "            y = autograd.Variable(tensor(y, dtype=torch.long)).to(device)\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-evaluate-cbow\"></a>\n",
    "\n",
    "# Apply and Evaluate the CBOW Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[92mis\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mnot\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mmotivated\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91m______\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mwas\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91mmore\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[91mword\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91mas\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91mbritish\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91muncorrelated\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mto\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[92mis\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91min\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mχ2\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mbeing\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91mand\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mbetween\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mnever\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91mlanguage\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mgood\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[92mmotivated\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mp\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m______\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91m1977\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mthen\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mfrom\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mand\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91ma\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91mwe\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mare\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mwords\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mfor\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m______\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mlanguage\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[91mthat\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91menough\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91mlanguage\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mis\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mtrue\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91maccurate\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mrandom\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mgiven\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[91ma\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mto\u001b[0m out to\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mis\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mthe\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mthe\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mpurchases\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mof\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mis\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mrelative\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mword\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mby\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mtwo\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91m______\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91m(\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mused\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mwork\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mor\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[92mto\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mreject\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mlinguistically\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mthe\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91macquisition\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mchi-square\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mhypotheses\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mto\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mframework\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91m______\u001b[0m ⫺ 0.5\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mshown\u001b[0m greater than\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m0.5\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91mtwo\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mbe\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mand\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mfor\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mthat\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mtrips\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91mwe\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[92mwe\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mis\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mthe\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91m)\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mpossible\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[92mit\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mverb\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mnon-technical\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mmoral\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mthere\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91min\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91m)\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91m______\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m is overlooked\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[92mlinguistics\u001b[0m 16 (\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[92mis\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91mnot\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91mstatis-\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mjones\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mwith\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[92mnot\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mwe\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[91musing\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[91mthe\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mis\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mthere\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mstrong\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhere\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91mor\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mwhich\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mcorpus\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91mdoes\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[92mnot\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[91mpreclude\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91m______\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mby\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91min\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mthe\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mand\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mdoes\u001b[0m not randomly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mthe\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91m(\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mshould\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91m______\u001b[0m and the\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mthen\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[92mthe\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mdoes\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mnot\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mprobability\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mdifferences\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mfor\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mone\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mreject\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mis\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[92mfor\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mof\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mand\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91msophisticated\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91mhave\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mdefinition\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91mare\u001b[0m , as\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mrepresented\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[92mrather\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mnot\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[91mword\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91mare\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mand\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mcomparing\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91min\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91marbitrary\u001b[0m , be\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mapplied\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mor\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mmore\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[91mbrown\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mthat\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mto\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[91mis\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91mon\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91mestimate\u001b[0m of them\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91m______\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mevents\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mare\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mthese\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mframes\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91meach\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mpairs\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91m______\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mto\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91mshow\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mfor\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91mprobability\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91m1993\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x, y = w2v_io\n",
    "        x = tensor(x)\n",
    "        y = tensor(y)\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28085106382978725\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-load-model\"></a>\n",
    "\n",
    "# Go back to the 10th Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CBOW(\n",
       "    (embeddings): Embedding(1303, 100)\n",
       "    (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=1303, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
    "model_10 = torch.nn.DataParallel(model_10)\n",
    "model_10.load_state_dict(torch.load('cbow_checkpoint_10.pt'))\n",
    "model_10.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[92mis\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mnot\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91min\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91m______\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mwas\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91mmore\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[91mword\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91mand\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mplan\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mto\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[92mis\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91min\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mχ2\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mof\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91mand\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mbetween\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[92mrandom\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91mlanguage\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mand\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[91mto\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mp\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m______\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mrandom\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mrandom\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mfrom\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mand\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91ma\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91mwe\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[92mdo\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mwords\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mreject\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m______\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mlanguage\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[91mthat\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91menough\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91m:\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mis\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mtrue\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mrandom\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mrandom\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mis\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mto\u001b[0m out to\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91m______\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mthe\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91mthe\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91m)\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mof\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mis\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mword\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mword\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[92m(\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mas\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mtwo\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91m______\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91m(\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mused\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91m)\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mand\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[92mto\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mreject\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mable\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mthe\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91msubset\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mtwo\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mhypotheses\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mis\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mbritish\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91m______\u001b[0m ⫺ 0.5\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91ma.\u001b[0m greater than\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[92mthe\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91mbnc\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mbe\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mand\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mfor\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mprobability\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mnever\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91mto\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[92mwe\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mof\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mis\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mthe\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mthat\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91m)\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mpossible\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mand\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mverb\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mfollowing\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mmathematics\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mthe\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mand\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91m)\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91m______\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m is overlooked\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[92mlinguistics\u001b[0m 16 (\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91mand\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91m______\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91mrandom\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mrandom\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mas\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[91mthen\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[91mnot\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[92mnot\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mwe\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mfrom\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[91mthe\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[91m______\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91mtesting\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mis\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mthere\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mstrong\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mthe\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91mwas\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mrandom\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mcorpus\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91mis\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[91mfiltering\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[92mrandom\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91m______\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mof\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91min\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mthe\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mand\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[92mare\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mthe\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91m(\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mrandom\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91mlinguistics\u001b[0m and the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mthen\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[92mthe\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mof\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mrandom\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mprobability\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mlinguistics\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mfor\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91msubset\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mreject\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91mrandom\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m)\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mof\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mand\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mwould\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91mhave\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mdefinition\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91menglish\u001b[0m , as\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91ma\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91ma\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mthe\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[91msame\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[91mword\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91mare\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mand\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mlanguage\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91min\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91marbitrary\u001b[0m , be\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mapplied\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mwhere\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mmore\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[91mnull\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mthat\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mto\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[91mis\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91m______\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91mpairs\u001b[0m of them\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[92mof\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mevents\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mand\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[92mthe\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91ma\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mpairs\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mthe\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mto\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91mas\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mfor\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91m______\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mas\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x, y = w2v_io\n",
    "        x = tensor(x)\n",
    "        y = tensor(y)\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model_10(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26382978723404255\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [optional] How to Handle Unknown Words? \n",
    "\n",
    "This is not the best way to handle unknown words, but we can simply assign an index for unknown words.\n",
    "\n",
    "**Hint:** Ensure that you have `gensim` version 3.7.0 first. Otherwise this part of the code won't work. Try `python -m pip install -U pip` and then `python -m pip install -U gensim==3.7.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a', 1: 'bar', 2: 'foo', 3: 'is', 4: 'sentence', 5: 'this'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a', 1: 'bar', 2: 'foo', 3: 'is', 4: 'sentence', 5: 'this'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.patch_with_special_tokens\n",
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "\n",
    "try:\n",
    "    special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "    vocab.patch_with_special_tokens(special_tokens)\n",
    "except: # If gensim is not 3.7.0\n",
    "    pass\n",
    "    \n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [optional] Lets Rewrite the `Word2VecText` Object\n",
    "\n",
    "Now with the (i) unknown word patch in the vocabulary as well as (ii) `skipgram_iterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Add the unknown word patch here.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        try:\n",
    "            special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "            self.vocab.patch_with_special_tokens(special_tokens)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = self.cbow_iterator\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = self.skipgram_iterator\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent, self.window_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                if leftovers:\n",
    "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5\"></a>\n",
    "\n",
    "# Lets try the skipgram task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5-foward\"></a>\n",
    "\n",
    "# Take a closer look at what's in the `forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx1 = torch.rand(1,20)\n",
    "xx2 = torch.rand(1,20)\n",
    "\n",
    "xx1_numpy = xx1.detach().numpy()\n",
    "xx2_numpy = xx2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "(20, 1)\n",
      "[[4.799898]]\n"
     ]
    }
   ],
   "source": [
    "print(xx1_numpy.shape)\n",
    "print(xx2_numpy.T.shape)\n",
    "print(np.dot(xx1_numpy, xx2_numpy.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "torch.Size([20, 1])\n",
      "tensor([[6.4539]])\n"
     ]
    }
   ],
   "source": [
    "print(xx1.shape)\n",
    "print(torch.t(xx2).shape) \n",
    "\n",
    "print(torch.mm(xx1, torch.t(xx2))) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5-train\"></a>\n",
    "\n",
    "# Train a Skipgram model (for real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [02:02<48:51, 30.54s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-6fa26cd78ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# This unsqueeze thing is really a feature/bug... -_-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepcoh_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "embd_size = 100\n",
    "learning_rate = 0.03\n",
    "hidden_size = 300\n",
    "window_size = 3\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=3, variant='skipgram')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = SkipGram(vocab_size, embd_size,).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epcoh_loss = 0\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x1, x2 = w2v_io['x']\n",
    "            x1, x2 = tensor(x1).to(device), tensor(x2).to(device)\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.float)).to(device)\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x1, x2)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs[0], y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epcoh_loss += float(loss)\n",
    "    torch.save(model.state_dict(), 'skipgram_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(epcoh_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5-evaluate\"></a>\n",
    "\n",
    "# Evaluate the model on the skipgram task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "skipgram_iterator() missing 1 required positional argument: 'window_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-687508993e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Extract all the CBOW contexts (X) and targets (Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mw2v_io\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Retrieve the inputs and outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: skipgram_iterator() missing 1 required positional argument: 'window_size'"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        _, prediction =  torch.max(model(x1, x2), 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Collobert and Weston SENNA Embeddings\n",
    "\n",
    "\n",
    "If you're on a Mac or Linux, you can use the `!` bang commands in the next cell to get the data.\n",
    "\n",
    "```\n",
    "!pip install kaggle\n",
    "!mkdir -p .kaggle\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > .kaggle/kaggle.json\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!kaggle datasets download -d alvations/vegetables-senna-embeddings --force -p ./\n",
    "```\n",
    "\n",
    "If you're on windows go to https://www.kaggle.com/alvations/vegetables-senna-embeddings and download the data files. \n",
    "\n",
    "What's most important are the \n",
    " - `.txt` file that contains the vocabulary list\n",
    " - `.npy` file that contains the binarized numpy array\n",
    " \n",
    "The rows of the numpy array corresponds to the vocabulary in the order from the `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /anaconda3/lib/python3.6/site-packages (1.4.7.1)\n",
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.6/site-packages (from kaggle) (4.23.3)\n",
      "Requirement already satisfied: python-dateutil in /anaconda3/lib/python3.6/site-packages (from kaggle) (2.6.1)\n",
      "Requirement already satisfied: urllib3<1.23.0,>=1.15 in /anaconda3/lib/python3.6/site-packages (from kaggle) (1.22)\n",
      "Requirement already satisfied: six>=1.10 in /anaconda3/lib/python3.6/site-packages (from kaggle) (1.11.0)\n",
      "Requirement already satisfied: python-slugify in /anaconda3/lib/python3.6/site-packages (from kaggle) (1.2.5)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.6/site-packages (from kaggle) (2.18.4)\n",
      "Requirement already satisfied: certifi in /anaconda3/lib/python3.6/site-packages (from kaggle) (2018.11.29)\n",
      "Requirement already satisfied: Unidecode>=0.04.16 in /anaconda3/lib/python3.6/site-packages (from python-slugify->kaggle) (1.0.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests->kaggle) (2.6)\n",
      "Downloading vegetables-senna-embeddings.zip to .\n",
      "100%|█████████████████████████████████████▉| 64.0M/64.1M [00:07<00:00, 7.11MB/s]\n",
      "100%|██████████████████████████████████████| 64.1M/64.1M [00:07<00:00, 9.08MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle\n",
    "!mkdir -p .kaggle\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > .kaggle/kaggle.json\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!kaggle datasets download -d alvations/vegetables-senna-embeddings --force -p ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-vocab\"></a>\n",
    "\n",
    "\n",
    "## 3.1.6. Loading Pre-trained Embeddings\n",
    "\n",
    "Lets overwrite the `Word2VecText` object with the pretrained embeddings. \n",
    "\n",
    "Most important thing is the overwrite the `Dictionary` from `gensim` with the vocabulary of the pre-trained embeddings, as such:\n",
    "\n",
    "```python\n",
    "        # Loads the pretrained keys. \n",
    "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
    "        self.vocab = Dictionary({})\n",
    "        self.vocab.token2id = pretrained_keys\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Loads the pretrained keys. \n",
    "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
    "        self.vocab = Dictionary({})\n",
    "        self.vocab.token2id = pretrained_keys\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=-1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                if leftovers:\n",
    "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-pretrained\"></a>\n",
    "\n",
    "## Override the embeddings layer with the pre-trained weights.\n",
    "\n",
    "In PyTorch, the weights of the `nn.Embedding` object can be easily overwritten with `from_pretrained` function, see https://pytorch.org/docs/stable/nn.html#embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, pretrained_npy):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='skipgram')\n",
    "pretrained_npy = torch.tensor(np.load('senna.wiki-reuters.lm2.50d.npy'))\n",
    "pretrained_model = SkipGram(pretrained_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-eval-skipgram\"></a>\n",
    "## Test Pretrained Embeddings on the Skipgram Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        pretrained_model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        if -1 in (x1, x2): # Skip unknown words.\n",
    "            continue\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        with torch.no_grad():\n",
    "            logprobs = pretrained_model(x1, x2)\n",
    "            _, prediction =  torch.max(logprobs, 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "    pretrained_keys = {line.strip():i for i, line in enumerate(fin)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4959667562943046\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-eval-cbow\"></a>\n",
    "## Test Pretrained Embeddings on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m:\u001b[0m \t\t the problem is essentially this \u001b[91mmahler\u001b[0m if a word ( or\n",
      "\u001b[92mre-\u001b[0m \t\t the web is a vast \u001b[91mluckless\u001b[0m source for many languages .\n",
      "\u001b[92mrandom\u001b[0m \t\t is that the association is \u001b[91mknocked\u001b[0m , arbitrary , motivated or\n",
      "\u001b[92m,\u001b[0m \t\t that the association is random \u001b[91mmansion\u001b[0m arbitrary , motivated or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t arbitrary , motivated or pre- \u001b[91mpegmatite\u001b[0m ( r , a ,\n",
      "\u001b[92minevitably\u001b[0m \t however , their methods are \u001b[91mnew-home\u001b[0m noisy , suffering , for\n",
      "\u001b[92mnoisy\u001b[0m \t\t , their methods are inevitably \u001b[91mtobago\u001b[0m , suffering , for example\n",
      "\u001b[92m,\u001b[0m \t\t their methods are inevitably noisy \u001b[91mjustice\u001b[0m suffering , for example ,\n",
      "\u001b[92msuffering\u001b[0m \t methods are inevitably noisy , \u001b[91msidereal\u001b[0m , for example , from\n",
      "\u001b[92m,\u001b[0m \t\t are inevitably noisy , suffering \u001b[91mhomesteading\u001b[0m for example , from just\n",
      "\u001b[92mfor\u001b[0m \t\t inevitably noisy , suffering , \u001b[91mj.c.\u001b[0m example , from just those\n",
      "\u001b[92mexample\u001b[0m \t noisy , suffering , for \u001b[91mrelegation\u001b[0m , from just those parser\n",
      "\u001b[92m,\u001b[0m \t\t , suffering , for example \u001b[91mcallers\u001b[0m from just those parser errors\n",
      "\u001b[92mfrom\u001b[0m \t\t suffering , for example , \u001b[91mcondottieri\u001b[0m just those parser errors that\n",
      "\u001b[92mjust\u001b[0m \t\t , for example , from \u001b[91mpatronising\u001b[0m those parser errors that the\n",
      "\u001b[92mthose\u001b[0m \t\t for example , from just \u001b[91mfriendship\u001b[0m parser errors that the whole\n",
      "\u001b[92mparser\u001b[0m \t\t example , from just those \u001b[91mcomorian\u001b[0m errors that the whole process\n",
      "\u001b[92merrors\u001b[0m \t\t , from just those parser \u001b[91mjahn\u001b[0m that the whole process is\n",
      "\u001b[92mthat\u001b[0m \t\t from just those parser errors \u001b[91msherwin\u001b[0m the whole process is designed\n",
      "\u001b[92mthe\u001b[0m \t\t just those parser errors that \u001b[91mrethink\u001b[0m whole process is designed to\n",
      "\u001b[92mwhole\u001b[0m \t\t those parser errors that the \u001b[91mcushion\u001b[0m process is designed to address\n",
      "\u001b[92mprocess\u001b[0m \t parser errors that the whole \u001b[91mfibres\u001b[0m is designed to address ,\n",
      "\u001b[92mis\u001b[0m \t\t errors that the whole process \u001b[91mtobago\u001b[0m designed to address , and\n",
      "\u001b[92mdesigned\u001b[0m \t that the whole process is \u001b[91mgores\u001b[0m to address , and they\n",
      "\u001b[92mto\u001b[0m \t\t the whole process is designed \u001b[91msidi\u001b[0m address , and they do\n",
      "\u001b[92maddress\u001b[0m \t whole process is designed to \u001b[91mpumas\u001b[0m , and they do not\n",
      "\u001b[92m,\u001b[0m \t\t process is designed to address \u001b[91mpegmatite\u001b[0m and they do not wish\n",
      "\u001b[92mand\u001b[0m \t\t is designed to address , \u001b[91msomaliland\u001b[0m they do not wish to\n",
      "\u001b[92mthey\u001b[0m \t\t designed to address , and \u001b[91mtaylor\u001b[0m do not wish to accept\n",
      "\u001b[92mdo\u001b[0m \t\t to address , and they \u001b[91msamanda\u001b[0m not wish to accept any\n",
      "\u001b[92mnot\u001b[0m \t\t address , and they do \u001b[91mdeviating\u001b[0m wish to accept any scf\n",
      "\u001b[92mwish\u001b[0m \t\t , and they do not \u001b[91minversora\u001b[0m to accept any scf for\n",
      "\u001b[92mto\u001b[0m \t\t and they do not wish \u001b[91mcorrientes\u001b[0m accept any scf for which\n",
      "\u001b[92maccept\u001b[0m \t\t they do not wish to \u001b[91msnowman\u001b[0m any scf for which there\n",
      "\u001b[92many\u001b[0m \t\t do not wish to accept \u001b[91mfrogmen\u001b[0m scf for which there is\n",
      "\u001b[92mscf\u001b[0m \t\t not wish to accept any \u001b[91mfloor\u001b[0m for which there is any\n",
      "\u001b[92mfor\u001b[0m \t\t wish to accept any scf \u001b[91mafire\u001b[0m which there is any evidence\n",
      "\u001b[92mwhich\u001b[0m \t\t to accept any scf for \u001b[91mfaysal\u001b[0m there is any evidence as\n",
      "\u001b[92mthere\u001b[0m \t\t accept any scf for which \u001b[91mlaissez-faire\u001b[0m is any evidence as a\n",
      "\u001b[92mis\u001b[0m \t\t any scf for which there \u001b[91mtisbury\u001b[0m any evidence as a true\n",
      "\u001b[92many\u001b[0m \t\t scf for which there is \u001b[91mhella\u001b[0m evidence as a true scf\n",
      "\u001b[92mevidence\u001b[0m \t for which there is any \u001b[91mstrathspey\u001b[0m as a true scf for\n",
      "\u001b[92mas\u001b[0m \t\t which there is any evidence \u001b[91madventist\u001b[0m a true scf for the\n",
      "\u001b[92ma\u001b[0m \t\t there is any evidence as \u001b[91mbeliever\u001b[0m true scf for the verb\n",
      "\u001b[92mtrue\u001b[0m \t\t is any evidence as a \u001b[91mfertimport\u001b[0m scf for the verb .\n",
      "\u001b[92mthat\u001b[0m \t\t while it might seem plausible \u001b[91msmits\u001b[0m oddities would in some way\n",
      "\u001b[92moddities\u001b[0m \t it might seem plausible that \u001b[91mlalumiere\u001b[0m would in some way balance\n",
      "\u001b[92mwould\u001b[0m \t\t might seem plausible that oddities \u001b[91mbrookwood\u001b[0m in some way balance out\n",
      "\u001b[92min\u001b[0m \t\t seem plausible that oddities would \u001b[91mkumwanza\u001b[0m some way balance out to\n",
      "\u001b[92msome\u001b[0m \t\t plausible that oddities would in \u001b[91mknutsford\u001b[0m way balance out to give\n",
      "\u001b[92mway\u001b[0m \t\t that oddities would in some \u001b[91mcallers\u001b[0m balance out to give a\n",
      "\u001b[92m<unk>\u001b[0m \t\t balance out to give a \u001b[91mbte\u001b[0m tion that was indistinguishable from\n",
      "\u001b[92mone\u001b[0m \t\t tion that was indistinguishable from \u001b[91mphone-tapping\u001b[0m where the individual words (\n",
      "\u001b[92mwhere\u001b[0m \t\t that was indistinguishable from one \u001b[91mgobain\u001b[0m the individual words ( as\n",
      "\u001b[92mthe\u001b[0m \t\t was indistinguishable from one where \u001b[91mshintom\u001b[0m individual words ( as opposed\n",
      "\u001b[92mindividual\u001b[0m \t indistinguishable from one where the \u001b[91mm/0\u001b[0m words ( as opposed to\n",
      "\u001b[92mwords\u001b[0m \t\t from one where the individual \u001b[91mcorrupts\u001b[0m ( as opposed to the\n",
      "\u001b[92m(\u001b[0m \t\t one where the individual words \u001b[91medema\u001b[0m as opposed to the texts\n",
      "\u001b[92mas\u001b[0m \t\t where the individual words ( \u001b[91msnowman\u001b[0m opposed to the texts )\n",
      "\u001b[92mopposed\u001b[0m \t the individual words ( as \u001b[91mpropagation\u001b[0m to the texts ) had\n",
      "\u001b[92mto\u001b[0m \t\t individual words ( as opposed \u001b[91mardal\u001b[0m the texts ) had been\n",
      "\u001b[92mthe\u001b[0m \t\t words ( as opposed to \u001b[91mtenseiga\u001b[0m texts ) had been randomly\n",
      "\u001b[92mtexts\u001b[0m \t\t ( as opposed to the \u001b[91mcushion\u001b[0m ) had been randomly selected\n",
      "\u001b[92m)\u001b[0m \t\t as opposed to the texts \u001b[91mabcl\u001b[0m had been randomly selected ,\n",
      "\u001b[92mhad\u001b[0m \t\t opposed to the texts ) \u001b[91mnoses\u001b[0m been randomly selected , this\n",
      "\u001b[92mbeen\u001b[0m \t\t to the texts ) had \u001b[91mburghley\u001b[0m randomly selected , this turns\n",
      "\u001b[92mrandomly\u001b[0m \t the texts ) had been \u001b[91mhirshhorn\u001b[0m selected , this turns out\n",
      "\u001b[92mselected\u001b[0m \t texts ) had been randomly \u001b[91mdemolitions\u001b[0m , this turns out not\n",
      "\u001b[92m,\u001b[0m \t\t ) had been randomly selected \u001b[91md.a.\u001b[0m this turns out not to\n",
      "\u001b[92mthis\u001b[0m \t\t had been randomly selected , \u001b[91mm/0\u001b[0m turns out not to be\n",
      "\u001b[92mturns\u001b[0m \t\t been randomly selected , this \u001b[91mcotangent\u001b[0m out not to be the\n",
      "\u001b[92mout\u001b[0m \t\t randomly selected , this turns \u001b[91mrmh\u001b[0m not to be the case\n",
      "\u001b[92mnot\u001b[0m \t\t selected , this turns out \u001b[91mstrangled\u001b[0m to be the case .\n",
      "\u001b[92mvaries\u001b[0m \t\t however where the sample size \u001b[91mtammi\u001b[0m by an order of magnitude\n",
      "\u001b[92mby\u001b[0m \t\t where the sample size varies \u001b[91mgallen\u001b[0m an order of magnitude ,\n",
      "\u001b[92man\u001b[0m \t\t the sample size varies by \u001b[91msaharanpur\u001b[0m order of magnitude , or\n",
      "\u001b[92morder\u001b[0m \t\t sample size varies by an \u001b[91mmajority\u001b[0m of magnitude , or where\n",
      "\u001b[92mof\u001b[0m \t\t size varies by an order \u001b[91mrochon\u001b[0m magnitude , or where it\n",
      "\u001b[92mmagnitude\u001b[0m \t varies by an order of \u001b[91mhomesteading\u001b[0m , or where it is\n",
      "\u001b[92m,\u001b[0m \t\t by an order of magnitude \u001b[91mpegmatite\u001b[0m or where it is enormous\n",
      "\u001b[92mor\u001b[0m \t\t an order of magnitude , \u001b[91mdivas\u001b[0m where it is enormous ,\n",
      "\u001b[92mwhere\u001b[0m \t\t order of magnitude , or \u001b[91mdeacon\u001b[0m it is enormous , it\n",
      "\u001b[92mit\u001b[0m \t\t of magnitude , or where \u001b[91mrelegation\u001b[0m is enormous , it is\n",
      "\u001b[92mis\u001b[0m \t\t magnitude , or where it \u001b[91mperugino\u001b[0m enormous , it is wrong\n",
      "\u001b[92menormous\u001b[0m \t , or where it is \u001b[91mcocktail\u001b[0m , it is wrong to\n",
      "\u001b[92m,\u001b[0m \t\t or where it is enormous \u001b[91mregenerator\u001b[0m it is wrong to identify\n",
      "\u001b[92mit\u001b[0m \t\t where it is enormous , \u001b[91mashanti\u001b[0m is wrong to identify the\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of the conference of \u001b[91mimprudent\u001b[0m south-central sas users group ,\n",
      "\u001b[92man\u001b[0m \t\t making false assumptions is often \u001b[91mboggs\u001b[0m ingenious way to proceed ;\n",
      "\u001b[92mingenious\u001b[0m \t false assumptions is often an \u001b[91mreactive\u001b[0m way to proceed ; the\n",
      "\u001b[92mway\u001b[0m \t\t assumptions is often an ingenious \u001b[91minversora\u001b[0m to proceed ; the problem\n",
      "\u001b[92mto\u001b[0m \t\t is often an ingenious way \u001b[91mmarket-orientated\u001b[0m proceed ; the problem arises\n",
      "\u001b[92mproceed\u001b[0m \t often an ingenious way to \u001b[91mgombos\u001b[0m ; the problem arises where\n",
      "\u001b[92m;\u001b[0m \t\t an ingenious way to proceed \u001b[91mheadphones\u001b[0m the problem arises where the\n",
      "\u001b[92mthe\u001b[0m \t\t ingenious way to proceed ; \u001b[91maustro-hungarian\u001b[0m problem arises where the literal\n",
      "\u001b[92mproblem\u001b[0m \t way to proceed ; the \u001b[91mcristovao\u001b[0m arises where the literal falsity\n",
      "\u001b[92marises\u001b[0m \t\t to proceed ; the problem \u001b[91mamavisca\u001b[0m where the literal falsity of\n",
      "\u001b[92mwhere\u001b[0m \t\t proceed ; the problem arises \u001b[91my.\u001b[0m the literal falsity of the\n",
      "\u001b[92mthe\u001b[0m \t\t ; the problem arises where \u001b[91mlebowa\u001b[0m literal falsity of the assumption\n",
      "\u001b[92mliteral\u001b[0m \t the problem arises where the \u001b[91mcotangent\u001b[0m falsity of the assumption is\n",
      "\u001b[92mfalsity\u001b[0m \t problem arises where the literal \u001b[91msidi\u001b[0m of the assumption is overlooked\n",
      "\u001b[92mof\u001b[0m \t\t arises where the literal falsity \u001b[91msuperheros\u001b[0m the assumption is overlooked ,\n",
      "\u001b[92mthe\u001b[0m \t\t where the literal falsity of \u001b[91mlandmass\u001b[0m assumption is overlooked , and\n",
      "\u001b[92m<unk>\u001b[0m \t\t assumption is overlooked , and \u001b[91msomaliland\u001b[0m ate inferences are drawn .\n",
      "\u001b[92m<unk>\u001b[0m \t\t when we look at linguistic \u001b[91mprofitable\u001b[0m ena in corpora , the\n",
      "\u001b[92mnull\u001b[0m \t\t ena in corpora , the \u001b[91mammianus\u001b[0m hypothesis will never be true\n",
      "\u001b[92mhypothesis\u001b[0m \t in corpora , the null \u001b[91mgreatest\u001b[0m will never be true .\n",
      "\u001b[92menough\u001b[0m \t\t we do not always have \u001b[91meurope/africa\u001b[0m data to reject the null\n",
      "\u001b[92mdata\u001b[0m \t\t do not always have enough \u001b[91mreactive\u001b[0m to reject the null hypothesis\n",
      "\u001b[92mto\u001b[0m \t\t not always have enough data \u001b[91mcanyons\u001b[0m reject the null hypothesis ,\n",
      "\u001b[92mreject\u001b[0m \t\t always have enough data to \u001b[91mgrammophon\u001b[0m the null hypothesis , but\n",
      "\u001b[92mthe\u001b[0m \t\t have enough data to reject \u001b[91moctober-december\u001b[0m null hypothesis , but that\n",
      "\u001b[92mnull\u001b[0m \t\t enough data to reject the \u001b[91mcollision\u001b[0m hypothesis , but that is\n",
      "\u001b[92mhypothesis\u001b[0m \t data to reject the null \u001b[91mconsonant\u001b[0m , but that is a\n",
      "\u001b[92m,\u001b[0m \t\t to reject the null hypothesis \u001b[91munspoken\u001b[0m but that is a distinct\n",
      "\u001b[92mbut\u001b[0m \t\t reject the null hypothesis , \u001b[91mtoothbrush\u001b[0m that is a distinct issue\n",
      "\u001b[92mthat\u001b[0m \t\t the null hypothesis , but \u001b[91msidi\u001b[0m is a distinct issue :\n",
      "\u001b[92mis\u001b[0m \t\t null hypothesis , but that \u001b[91mdavin\u001b[0m a distinct issue : wherever\n",
      "\u001b[92ma\u001b[0m \t\t hypothesis , but that is \u001b[91mcarbon0\u001b[0m distinct issue : wherever there\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mdistinct\u001b[0m \t , but that is a \u001b[91mhausmann\u001b[0m issue : wherever there is\n",
      "\u001b[92missue\u001b[0m \t\t but that is a distinct \u001b[91mschools.he\u001b[0m : wherever there is enough\n",
      "\u001b[92m:\u001b[0m \t\t that is a distinct issue \u001b[91mracially\u001b[0m wherever there is enough data\n",
      "\u001b[92mwherever\u001b[0m \t is a distinct issue : \u001b[91mstews\u001b[0m there is enough data ,\n",
      "\u001b[92mthere\u001b[0m \t\t a distinct issue : wherever \u001b[91mhoogovens\u001b[0m is enough data , it\n",
      "\u001b[92mis\u001b[0m \t\t distinct issue : wherever there \u001b[91mtracers\u001b[0m enough data , it is\n",
      "\u001b[92menough\u001b[0m \t\t issue : wherever there is \u001b[91mingersoll\u001b[0m data , it is rejected\n",
      "\u001b[92mdata\u001b[0m \t\t : wherever there is enough \u001b[91mnewts\u001b[0m , it is rejected .\n",
      "\u001b[92mare\u001b[0m \t\t since words in a text \u001b[91mayacucho\u001b[0m not random , we know\n",
      "\u001b[92mnot\u001b[0m \t\t words in a text are \u001b[91magrippa\u001b[0m random , we know that\n",
      "\u001b[92mrandom\u001b[0m \t\t in a text are not \u001b[91msidi\u001b[0m , we know that our\n",
      "\u001b[92m,\u001b[0m \t\t a text are not random \u001b[91mzarb\u001b[0m we know that our corpora\n",
      "\u001b[92mwe\u001b[0m \t\t text are not random , \u001b[91mindigenisation\u001b[0m know that our corpora are\n",
      "\u001b[92mknow\u001b[0m \t\t are not random , we \u001b[91mhiddink\u001b[0m that our corpora are not\n",
      "\u001b[92mthat\u001b[0m \t\t not random , we know \u001b[91mfarragut\u001b[0m our corpora are not randomly\n",
      "\u001b[92mour\u001b[0m \t\t random , we know that \u001b[91msurendra\u001b[0m corpora are not randomly generated\n",
      "\u001b[92mcorpora\u001b[0m \t , we know that our \u001b[91mgores\u001b[0m are not randomly generated ,\n",
      "\u001b[92mare\u001b[0m \t\t we know that our corpora \u001b[91mdisparaging\u001b[0m not randomly generated , and\n",
      "\u001b[92mnot\u001b[0m \t\t know that our corpora are \u001b[91mwesteros\u001b[0m randomly generated , and the\n",
      "\u001b[92mrandomly\u001b[0m \t that our corpora are not \u001b[91mburghley\u001b[0m generated , and the hypothesis\n",
      "\u001b[92mgenerated\u001b[0m \t our corpora are not randomly \u001b[91m0------------------\u001b[0m , and the hypothesis test\n",
      "\u001b[92mthe\u001b[0m \t\t gives us reason to view \u001b[91mcorps\u001b[0m relation between , for example\n",
      "\u001b[92mrelation\u001b[0m \t us reason to view the \u001b[91mannoyances\u001b[0m between , for example ,\n",
      "\u001b[92mbetween\u001b[0m \t reason to view the relation \u001b[91mkapiti\u001b[0m , for example , a\n",
      "\u001b[92m,\u001b[0m \t\t to view the relation between \u001b[91mmarcion\u001b[0m for example , a verb\n",
      "\u001b[92m<unk>\u001b[0m \t\t for example , a verb \u001b[91msamanda\u001b[0m s syntax and its semantics\n",
      "\u001b[92m,\u001b[0m \t\t s syntax and its semantics \u001b[91me-class\u001b[0m as motivated rather than arbitrary\n",
      "\u001b[92mas\u001b[0m \t\t syntax and its semantics , \u001b[91mrehearse\u001b[0m motivated rather than arbitrary .\n",
      "\u001b[92merror\u001b[0m \t\t the average value of the \u001b[91malli\u001b[0m term , language is never\n",
      "\u001b[92mterm\u001b[0m \t\t average value of the error \u001b[91mantonelli\u001b[0m , language is never ,\n",
      "\u001b[92m,\u001b[0m \t\t value of the error term \u001b[91mrenovation\u001b[0m language is never , ever\n",
      "\u001b[92mlanguage\u001b[0m \t of the error term , \u001b[91msecant\u001b[0m is never , ever ,\n",
      "\u001b[92mis\u001b[0m \t\t the error term , language \u001b[91mayacucho\u001b[0m never , ever , ever\n",
      "\u001b[92mnever\u001b[0m \t\t error term , language is \u001b[91mtoothbrush\u001b[0m , ever , ever ,\n",
      "\u001b[92m,\u001b[0m \t\t term , language is never \u001b[91mcomorian\u001b[0m ever , ever , random\n",
      "\u001b[92m,\u001b[0m \t\t the hypothesis can , therefore \u001b[91mangustifolia\u001b[0m be couched as : are\n",
      "\u001b[92mbe\u001b[0m \t\t hypothesis can , therefore , \u001b[91mstottlemeyer\u001b[0m couched as : are the\n",
      "\u001b[92mcouched\u001b[0m \t can , therefore , be \u001b[91md.a.\u001b[0m as : are the error\n",
      "\u001b[92mas\u001b[0m \t\t , therefore , be couched \u001b[91mheihachi\u001b[0m : are the error terms\n",
      "\u001b[92m:\u001b[0m \t\t therefore , be couched as \u001b[91mfarragut\u001b[0m are the error terms systematically\n",
      "\u001b[92mare\u001b[0m \t\t , be couched as : \u001b[91mranting\u001b[0m the error terms systematically greater\n",
      "\u001b[92mthe\u001b[0m \t\t be couched as : are \u001b[91mdescribes\u001b[0m error terms systematically greater than\n",
      "\u001b[92mbecomes\u001b[0m \t % of them , devastate \u001b[91mdeviating\u001b[0m one of the verbs for\n",
      "\u001b[92mone\u001b[0m \t\t of them , devastate becomes \u001b[91mdeceit\u001b[0m of the verbs for which\n",
      "\u001b[92mof\u001b[0m \t\t them , devastate becomes one \u001b[91mshudder\u001b[0m the verbs for which we\n",
      "\u001b[92mthe\u001b[0m \t\t , devastate becomes one of \u001b[91mecuadorians\u001b[0m verbs for which we have\n",
      "\u001b[92mverbs\u001b[0m \t\t devastate becomes one of the \u001b[91mpegmatite\u001b[0m for which we have plenty\n",
      "\u001b[92mfor\u001b[0m \t\t becomes one of the verbs \u001b[91magrippa\u001b[0m which we have plenty of\n",
      "\u001b[92mwhich\u001b[0m \t\t one of the verbs for \u001b[91mshakes\u001b[0m we have plenty of data\n",
      "\u001b[92mwe\u001b[0m \t\t of the verbs for which \u001b[91my.\u001b[0m have plenty of data ,\n",
      "\u001b[92mhave\u001b[0m \t\t the verbs for which we \u001b[91mmetalworking\u001b[0m plenty of data , and\n",
      "\u001b[92mplenty\u001b[0m \t\t verbs for which we have \u001b[91mfloor\u001b[0m of data , and crude\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-unfreeze-finetune\"></a>\n",
    "## Unfreeze the Embedddings and Tune it on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy, freeze=False)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2 \n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-4f84802592bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Zero gradient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Calculate the log probability of the context embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_cbow_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshare_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.003\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(pretrained_cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(pretrained_cbow_model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x = tensor(w2v_io['x'])\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.long))\n",
    "            \n",
    "            if -1 in x or int(y) == -1:\n",
    "                continue\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = pretrained_cbow_model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_finetuning_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-reval-cbow\"></a>\n",
    "\n",
    "## Re-Test Pretrained Embeddings on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
