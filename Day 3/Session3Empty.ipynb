{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "- **3.0. Data Preparation**\n",
    "  - 3.0.1. *Vocabulary*\n",
    "  - 3.0.2. *Dataset*\n",
    "<br><br>  \n",
    "\n",
    "- **3.1. Word2Vec from Scratch**\n",
    "  - 3.1.1. *CBOW*\n",
    "  - 3.1.2. *Skipgram*\n",
    "  - 3.1.3. *Word2Vec Dataset*\n",
    "  - 3.1.4. *Train a CBOW model*\n",
    "  - 3.1.5. *Train a Skipgram model*\n",
    "  - 3.1.6. *Loading Pre-trained Embeddings*\n",
    "  \n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: torch in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (1.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (4.28.1)\n",
      "Requirement already satisfied: nltk in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (3.4)\n",
      "Requirement already satisfied: lazyme in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (0.0.22)\n",
      "Requirement already satisfied: requests in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (2.21.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (from sklearn) (0.20.1)\n",
      "Requirement already satisfied: six in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (from requests) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (from requests) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /Users/davidleonardi/anaconda3/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "/Users/davidleonardi/anaconda3/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/davidleonardi/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn torch tqdm nltk lazyme requests\n",
    "!python -m nltk.downloader movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0. Data Preparation\n",
    "\n",
    "Before we train our own embeddings, lets first understand how to read text data into pytorch.\n",
    "The native pytorch way to load datasets is to use the `torch.utils.data.Dataset` object.\n",
    "\n",
    "There are already several other libraries that help with loading text datasets, e.g. \n",
    "\n",
    " - FastAI https://docs.fast.ai/text.data.html\n",
    " - AllenNLP https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset.html\n",
    " - Torch Text https://github.com/pytorch/text#data\n",
    " - Texar https://texar.readthedocs.io/en/latest/code/data.html#id4 \n",
    " - SpaCy https://github.com/explosion/thinc\n",
    " \n",
    "\n",
    "But to truly understand and use it for the custom datasets you'll see at work, lets learn it the native way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0.1  Vocabulary\n",
    "\n",
    "Given a text, the first thing to do is to build a vocabulary (i.e. a dictionary of unique words) and assign an index to each unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"Language users never choose words randomly, and language is essentially\n",
    "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
    "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
    "the null hypothesis will never be true. Moreover, where there is enough\n",
    "data, we shall (almost) always be able to establish that it is not true. In\n",
    "corpus studies, we frequently do have enough data, so the fact that a relation \n",
    "between two phenomena is demonstrably non-random, does not support the inference \n",
    "that it is not arbitrary. We present experimental evidence\n",
    "of how arbitrary associations between word frequencies and corpora are\n",
    "systematically non-random. We review literature in which hypothesis testing \n",
    "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
    "\n",
    "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "uniq_tokens = set(chain(*tokenized_text))\n",
    "\n",
    "vocab = {}   # Assign indices to every word.\n",
    "idx2tok = {} # Also keep an dict of index to words.\n",
    "for i, token in enumerate(uniq_tokens):\n",
    "    vocab[token] = i\n",
    "    idx2tok[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'how': 0,\n",
       " 'review': 1,\n",
       " 'it': 2,\n",
       " 'led': 3,\n",
       " 'words': 4,\n",
       " 'hypothesis': 5,\n",
       " 'associations': 6,\n",
       " 'or': 7,\n",
       " 'null': 8,\n",
       " 'we': 9,\n",
       " 'corpus': 10,\n",
       " 'look': 11,\n",
       " 'a': 12,\n",
       " 'used': 13,\n",
       " 'statistical': 14,\n",
       " 'always': 15,\n",
       " ')': 16,\n",
       " 'frequencies': 17,\n",
       " 'randomly': 18,\n",
       " 'true': 19,\n",
       " 'frequently': 20,\n",
       " 'enough': 21,\n",
       " 'have': 22,\n",
       " '(': 23,\n",
       " 'has': 24,\n",
       " 'arbitrary': 25,\n",
       " 'two': 26,\n",
       " 'choose': 27,\n",
       " 'been': 28,\n",
       " 'fact': 29,\n",
       " 'relation': 30,\n",
       " 'studies': 31,\n",
       " 'phenomena': 32,\n",
       " 'unhelpful': 33,\n",
       " 'the': 34,\n",
       " 'that': 35,\n",
       " 'support': 36,\n",
       " 'where': 37,\n",
       " 'non-random': 38,\n",
       " 'at': 39,\n",
       " 'does': 40,\n",
       " 'experimental': 41,\n",
       " 'so': 42,\n",
       " 'to': 43,\n",
       " 'between': 44,\n",
       " 'demonstrably': 45,\n",
       " 'do': 46,\n",
       " 'posits': 47,\n",
       " 'of': 48,\n",
       " ',': 49,\n",
       " 'and': 50,\n",
       " 'language': 51,\n",
       " 'results': 52,\n",
       " '.': 53,\n",
       " 'essentially': 54,\n",
       " 'inference': 55,\n",
       " 'misleading': 56,\n",
       " 'not': 57,\n",
       " 'often': 58,\n",
       " 'moreover': 59,\n",
       " 'users': 60,\n",
       " 'will': 61,\n",
       " 'data': 62,\n",
       " 'when': 63,\n",
       " 'able': 64,\n",
       " 'never': 65,\n",
       " 'word': 66,\n",
       " 'hence': 67,\n",
       " 'almost': 68,\n",
       " 'which': 69,\n",
       " 'uses': 70,\n",
       " 'testing': 71,\n",
       " 'shall': 72,\n",
       " 'corpora': 73,\n",
       " 'literature': 74,\n",
       " 'there': 75,\n",
       " 'establish': 76,\n",
       " 'in': 77,\n",
       " 'present': 78,\n",
       " 'be': 79,\n",
       " 'are': 80,\n",
       " 'show': 81,\n",
       " 'systematically': 82,\n",
       " 'evidence': 83,\n",
       " 'linguistic': 84,\n",
       " 'is': 85,\n",
       " 'randomness': 86}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the index of the word 'corpora'\n",
    "vocab['corpora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51, 60, 65, 27, 4, 18, 49, 50, 51, 85, 54, 38, 53]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The indexed representation of the first sentence.\n",
    "\n",
    "sent0 = tokenized_text[0]\n",
    "\n",
    "[vocab[token] for token in sent0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language',\n",
       " 'users',\n",
       " 'never',\n",
       " 'choose',\n",
       " 'words',\n",
       " 'randomly',\n",
       " ',',\n",
       " 'and',\n",
       " 'language',\n",
       " 'is',\n",
       " 'essentially',\n",
       " 'non-random',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pet Peeve\n",
    "\n",
    "I (Liling) don't really like to write my own vectorizer the `gensim` has functions that are optimized for such operations. In fact, I've written a [whole preprocessing pipeline library for me to use for language modelling and machine translation purposes](https://github.com/alvations/komorebi/blob/master/komorebi/text.py) =)\n",
    "\n",
    "Using `gensim`, I would have written the above as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "vocab = Dictionary(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ',',\n",
       " 1: '.',\n",
       " 2: 'and',\n",
       " 3: 'choose',\n",
       " 4: 'essentially',\n",
       " 5: 'is',\n",
       " 6: 'language',\n",
       " 7: 'never',\n",
       " 8: 'non-random',\n",
       " 9: 'randomly',\n",
       " 10: 'users',\n",
       " 11: 'words',\n",
       " 12: 'a',\n",
       " 13: 'hypothesis',\n",
       " 14: 'null',\n",
       " 15: 'posits',\n",
       " 16: 'randomness',\n",
       " 17: 'statistical',\n",
       " 18: 'testing',\n",
       " 19: 'uses',\n",
       " 20: 'which',\n",
       " 21: 'at',\n",
       " 22: 'be',\n",
       " 23: 'corpora',\n",
       " 24: 'hence',\n",
       " 25: 'in',\n",
       " 26: 'linguistic',\n",
       " 27: 'look',\n",
       " 28: 'phenomena',\n",
       " 29: 'the',\n",
       " 30: 'true',\n",
       " 31: 'we',\n",
       " 32: 'when',\n",
       " 33: 'will',\n",
       " 34: '(',\n",
       " 35: ')',\n",
       " 36: 'able',\n",
       " 37: 'almost',\n",
       " 38: 'always',\n",
       " 39: 'data',\n",
       " 40: 'enough',\n",
       " 41: 'establish',\n",
       " 42: 'it',\n",
       " 43: 'moreover',\n",
       " 44: 'not',\n",
       " 45: 'shall',\n",
       " 46: 'that',\n",
       " 47: 'there',\n",
       " 48: 'to',\n",
       " 49: 'where',\n",
       " 50: 'arbitrary',\n",
       " 51: 'between',\n",
       " 52: 'corpus',\n",
       " 53: 'demonstrably',\n",
       " 54: 'do',\n",
       " 55: 'does',\n",
       " 56: 'fact',\n",
       " 57: 'frequently',\n",
       " 58: 'have',\n",
       " 59: 'inference',\n",
       " 60: 'relation',\n",
       " 61: 'so',\n",
       " 62: 'studies',\n",
       " 63: 'support',\n",
       " 64: 'two',\n",
       " 65: 'are',\n",
       " 66: 'associations',\n",
       " 67: 'evidence',\n",
       " 68: 'experimental',\n",
       " 69: 'frequencies',\n",
       " 70: 'how',\n",
       " 71: 'of',\n",
       " 72: 'present',\n",
       " 73: 'systematically',\n",
       " 74: 'word',\n",
       " 75: 'been',\n",
       " 76: 'has',\n",
       " 77: 'led',\n",
       " 78: 'literature',\n",
       " 79: 'misleading',\n",
       " 80: 'often',\n",
       " 81: 'or',\n",
       " 82: 'results',\n",
       " 83: 'review',\n",
       " 84: 'show',\n",
       " 85: 'unhelpful',\n",
       " 86: 'used'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the key-value order is different of gensim from the native Python's\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token2id['corpora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.doc2idx(sent0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"indexed form\" of the tokens in the sentence forms the ***vectorized*** input to the `nn.Embedding` layer in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0.2 Dataset\n",
    "\n",
    "Lets try creating a `torch.utils.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Text(Dataset):\n",
    "    def __init__(self, tokenized_texts):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.vocab = Dictionary(tokenized_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        return self.vectorize(self.sents[index])\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return {'x': self.vocab.doc2idx(tokens)}\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = Text(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset[0] # First sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language',\n",
       " 'users',\n",
       " 'never',\n",
       " 'choose',\n",
       " 'words',\n",
       " 'randomly',\n",
       " ',',\n",
       " 'and',\n",
       " 'language',\n",
       " 'is',\n",
       " 'essentially',\n",
       " 'non-random',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset.unvectorize(text_dataset[0]['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return `dict` in `__getitem__()`\n",
    "\n",
    "This is nice if we're just representing sentences/documents by their indices but when we're doing machine learning, we usually have `X` and `Y`. \n",
    "\n",
    "If we have labels for the each sentence, we can also put it into to `__getitem__()` by having it return a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LabeledText(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.labels = labels # Sentence level labels.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        return {'X': self.vectorize(self.sents[index]), 'Y': self.labels[index]}\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try the `LabeledDataset` on a movie review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:13<00:00, 145.99it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "for fileid in tqdm(movie_reviews.fileids()):\n",
    "    label = fileid.split('/')[0]\n",
    "    doc = word_tokenize(movie_reviews.open(fileid).read())\n",
    "    documents.append(doc)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.',\n",
       " 'they',\n",
       " 'get',\n",
       " 'into',\n",
       " 'an',\n",
       " 'accident',\n",
       " '.',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guys',\n",
       " 'dies',\n",
       " ',',\n",
       " 'but',\n",
       " 'his',\n",
       " 'girlfriend',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'see',\n",
       " 'him',\n",
       " 'in',\n",
       " 'her',\n",
       " 'life',\n",
       " ',',\n",
       " 'and',\n",
       " 'has',\n",
       " 'nightmares',\n",
       " '.',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'deal',\n",
       " '?',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'and',\n",
       " '``',\n",
       " 'sorta',\n",
       " '``',\n",
       " 'find',\n",
       " 'out',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'critique',\n",
       " ':',\n",
       " 'a',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'for',\n",
       " 'the',\n",
       " 'teen',\n",
       " 'generation',\n",
       " 'that',\n",
       " 'touches',\n",
       " 'on',\n",
       " 'a',\n",
       " 'very',\n",
       " 'cool',\n",
       " 'idea',\n",
       " ',',\n",
       " 'but',\n",
       " 'presents',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'package',\n",
       " '.',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'review',\n",
       " 'an',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'one',\n",
       " 'to',\n",
       " 'write',\n",
       " ',',\n",
       " 'since',\n",
       " 'i',\n",
       " 'generally',\n",
       " 'applaud',\n",
       " 'films',\n",
       " 'which',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'break',\n",
       " 'the',\n",
       " 'mold',\n",
       " ',',\n",
       " 'mess',\n",
       " 'with',\n",
       " 'your',\n",
       " 'head',\n",
       " 'and',\n",
       " 'such',\n",
       " '(',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '&',\n",
       " 'memento',\n",
       " ')',\n",
       " ',',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'good',\n",
       " 'and',\n",
       " 'bad',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'making',\n",
       " 'all',\n",
       " 'types',\n",
       " 'of',\n",
       " 'films',\n",
       " ',',\n",
       " 'and',\n",
       " 'these',\n",
       " 'folks',\n",
       " 'just',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'snag',\n",
       " 'this',\n",
       " 'one',\n",
       " 'correctly',\n",
       " '.',\n",
       " 'they',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'this',\n",
       " 'pretty',\n",
       " 'neat',\n",
       " 'concept',\n",
       " ',',\n",
       " 'but',\n",
       " 'executed',\n",
       " 'it',\n",
       " 'terribly',\n",
       " '.',\n",
       " 'so',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'well',\n",
       " ',',\n",
       " 'its',\n",
       " 'main',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'simply',\n",
       " 'too',\n",
       " 'jumbled',\n",
       " '.',\n",
       " 'it',\n",
       " 'starts',\n",
       " 'off',\n",
       " '``',\n",
       " 'normal',\n",
       " '``',\n",
       " 'but',\n",
       " 'then',\n",
       " 'downshifts',\n",
       " 'into',\n",
       " 'this',\n",
       " '``',\n",
       " 'fantasy',\n",
       " '``',\n",
       " 'world',\n",
       " 'in',\n",
       " 'which',\n",
       " 'you',\n",
       " ',',\n",
       " 'as',\n",
       " 'an',\n",
       " 'audience',\n",
       " 'member',\n",
       " ',',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'on',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'dreams',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'characters',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'from',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'others',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'strange',\n",
       " 'apparitions',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'disappearances',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'looooot',\n",
       " 'of',\n",
       " 'chase',\n",
       " 'scenes',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tons',\n",
       " 'of',\n",
       " 'weird',\n",
       " 'things',\n",
       " 'that',\n",
       " 'happen',\n",
       " ',',\n",
       " 'and',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'not',\n",
       " 'explained',\n",
       " '.',\n",
       " 'now',\n",
       " 'i',\n",
       " 'personally',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mind',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'unravel',\n",
       " 'a',\n",
       " 'film',\n",
       " 'every',\n",
       " 'now',\n",
       " 'and',\n",
       " 'then',\n",
       " ',',\n",
       " 'but',\n",
       " 'when',\n",
       " 'all',\n",
       " 'it',\n",
       " 'does',\n",
       " 'is',\n",
       " 'give',\n",
       " 'me',\n",
       " 'the',\n",
       " 'same',\n",
       " 'clue',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " ',',\n",
       " 'i',\n",
       " 'get',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'fed',\n",
       " 'up',\n",
       " 'after',\n",
       " 'a',\n",
       " 'while',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'this',\n",
       " 'film',\n",
       " \"'s\",\n",
       " 'biggest',\n",
       " 'problem',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'obviously',\n",
       " 'got',\n",
       " 'this',\n",
       " 'big',\n",
       " 'secret',\n",
       " 'to',\n",
       " 'hide',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'it',\n",
       " 'completely',\n",
       " 'until',\n",
       " 'its',\n",
       " 'final',\n",
       " 'five',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'and',\n",
       " 'do',\n",
       " 'they',\n",
       " 'make',\n",
       " 'things',\n",
       " 'entertaining',\n",
       " ',',\n",
       " 'thrilling',\n",
       " 'or',\n",
       " 'even',\n",
       " 'engaging',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meantime',\n",
       " '?',\n",
       " 'not',\n",
       " 'really',\n",
       " '.',\n",
       " 'the',\n",
       " 'sad',\n",
       " 'part',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'arrow',\n",
       " 'and',\n",
       " 'i',\n",
       " 'both',\n",
       " 'dig',\n",
       " 'on',\n",
       " 'flicks',\n",
       " 'like',\n",
       " 'this',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'actually',\n",
       " 'figured',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'out',\n",
       " 'by',\n",
       " 'the',\n",
       " 'half-way',\n",
       " 'point',\n",
       " ',',\n",
       " 'so',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'after',\n",
       " 'that',\n",
       " 'did',\n",
       " 'start',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'sense',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'still',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'the',\n",
       " 'make',\n",
       " 'the',\n",
       " 'film',\n",
       " 'all',\n",
       " 'that',\n",
       " 'more',\n",
       " 'entertaining',\n",
       " '.',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'the',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'with',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'this',\n",
       " 'is',\n",
       " 'that',\n",
       " 'you',\n",
       " 'should',\n",
       " 'always',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'the',\n",
       " 'audience',\n",
       " 'is',\n",
       " '``',\n",
       " 'into',\n",
       " 'it',\n",
       " '``',\n",
       " 'even',\n",
       " 'before',\n",
       " 'they',\n",
       " 'are',\n",
       " 'given',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'password',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'your',\n",
       " 'world',\n",
       " 'of',\n",
       " 'understanding',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'showing',\n",
       " 'melissa',\n",
       " 'sagemiller',\n",
       " 'running',\n",
       " 'away',\n",
       " 'from',\n",
       " 'visions',\n",
       " 'for',\n",
       " 'about',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'just',\n",
       " 'plain',\n",
       " 'lazy',\n",
       " '!',\n",
       " '!',\n",
       " 'okay',\n",
       " ',',\n",
       " 'we',\n",
       " 'get',\n",
       " 'it',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'chasing',\n",
       " 'her',\n",
       " 'and',\n",
       " 'we',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'who',\n",
       " 'they',\n",
       " 'are',\n",
       " '.',\n",
       " 'do',\n",
       " 'we',\n",
       " 'really',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " '?',\n",
       " 'how',\n",
       " 'about',\n",
       " 'giving',\n",
       " 'us',\n",
       " 'different',\n",
       " 'scenes',\n",
       " 'offering',\n",
       " 'further',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'going',\n",
       " 'down',\n",
       " 'in',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'apparently',\n",
       " ',',\n",
       " 'the',\n",
       " 'studio',\n",
       " 'took',\n",
       " 'this',\n",
       " 'film',\n",
       " 'away',\n",
       " 'from',\n",
       " 'its',\n",
       " 'director',\n",
       " 'and',\n",
       " 'chopped',\n",
       " 'it',\n",
       " 'up',\n",
       " 'themselves',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'shows',\n",
       " '.',\n",
       " 'there',\n",
       " 'might',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'decent',\n",
       " 'teen',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'in',\n",
       " 'here',\n",
       " 'somewhere',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'guess',\n",
       " '``',\n",
       " 'the',\n",
       " 'suits',\n",
       " '``',\n",
       " 'decided',\n",
       " 'that',\n",
       " 'turning',\n",
       " 'it',\n",
       " 'into',\n",
       " 'a',\n",
       " 'music',\n",
       " 'video',\n",
       " 'with',\n",
       " 'little',\n",
       " 'edge',\n",
       " ',',\n",
       " 'would',\n",
       " 'make',\n",
       " 'more',\n",
       " 'sense',\n",
       " '.',\n",
       " 'the',\n",
       " 'actors',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'for',\n",
       " 'the',\n",
       " 'most',\n",
       " 'part',\n",
       " ',',\n",
       " 'although',\n",
       " 'wes',\n",
       " 'bentley',\n",
       " 'just',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'playing',\n",
       " 'the',\n",
       " 'exact',\n",
       " 'same',\n",
       " 'character',\n",
       " 'that',\n",
       " 'he',\n",
       " 'did',\n",
       " 'in',\n",
       " 'american',\n",
       " 'beauty',\n",
       " ',',\n",
       " 'only',\n",
       " 'in',\n",
       " 'a',\n",
       " 'new',\n",
       " 'neighborhood',\n",
       " '.',\n",
       " 'but',\n",
       " 'my',\n",
       " 'biggest',\n",
       " 'kudos',\n",
       " 'go',\n",
       " 'out',\n",
       " 'to',\n",
       " 'sagemiller',\n",
       " ',',\n",
       " 'who',\n",
       " 'holds',\n",
       " 'her',\n",
       " 'own',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'film',\n",
       " ',',\n",
       " 'and',\n",
       " 'actually',\n",
       " 'has',\n",
       " 'you',\n",
       " 'feeling',\n",
       " 'her',\n",
       " 'character',\n",
       " \"'s\",\n",
       " 'unraveling',\n",
       " '.',\n",
       " 'overall',\n",
       " ',',\n",
       " 'the',\n",
       " 'film',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'stick',\n",
       " 'because',\n",
       " 'it',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'entertain',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'confusing',\n",
       " ',',\n",
       " 'it',\n",
       " 'rarely',\n",
       " 'excites',\n",
       " 'and',\n",
       " 'it',\n",
       " 'feels',\n",
       " 'pretty',\n",
       " 'redundant',\n",
       " 'for',\n",
       " 'most',\n",
       " 'of',\n",
       " 'its',\n",
       " 'runtime',\n",
       " ',',\n",
       " 'despite',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'ending',\n",
       " 'and',\n",
       " 'explanation',\n",
       " 'to',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'craziness',\n",
       " 'that',\n",
       " 'came',\n",
       " 'before',\n",
       " 'it',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'and',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'horror',\n",
       " 'or',\n",
       " 'teen',\n",
       " 'slasher',\n",
       " 'flick',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'just',\n",
       " 'packaged',\n",
       " 'to',\n",
       " 'look',\n",
       " 'that',\n",
       " 'way',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'is',\n",
       " 'apparently',\n",
       " 'assuming',\n",
       " 'that',\n",
       " 'the',\n",
       " 'genre',\n",
       " 'is',\n",
       " 'still',\n",
       " 'hot',\n",
       " 'with',\n",
       " 'the',\n",
       " 'kids',\n",
       " '.',\n",
       " 'it',\n",
       " 'also',\n",
       " 'wrapped',\n",
       " 'production',\n",
       " 'two',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'has',\n",
       " 'been',\n",
       " 'sitting',\n",
       " 'on',\n",
       " 'the',\n",
       " 'shelves',\n",
       " 'ever',\n",
       " 'since',\n",
       " '.',\n",
       " 'whatever',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'skip',\n",
       " 'it',\n",
       " '!',\n",
       " 'where',\n",
       " \"'s\",\n",
       " 'joblo',\n",
       " 'coming',\n",
       " 'from',\n",
       " '?',\n",
       " 'a',\n",
       " 'nightmare',\n",
       " 'of',\n",
       " 'elm',\n",
       " 'street',\n",
       " '3',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'blair',\n",
       " 'witch',\n",
       " '2',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " ':',\n",
       " 'salvation',\n",
       " '(',\n",
       " '4/10',\n",
       " ')',\n",
       " '-',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'memento',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'others',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'stir',\n",
       " 'of',\n",
       " 'echoes',\n",
       " '(',\n",
       " '8/10',\n",
       " ')']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = LabeledText(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[243,\n",
       " 17,\n",
       " 314,\n",
       " 294,\n",
       " 77,\n",
       " 140,\n",
       " 307,\n",
       " 20,\n",
       " 68,\n",
       " 237,\n",
       " 6,\n",
       " 97,\n",
       " 34,\n",
       " 299,\n",
       " 98,\n",
       " 8,\n",
       " 302,\n",
       " 135,\n",
       " 167,\n",
       " 33,\n",
       " 22,\n",
       " 8,\n",
       " 226,\n",
       " 220,\n",
       " 297,\n",
       " 145,\n",
       " 87,\n",
       " 6,\n",
       " 60,\n",
       " 158,\n",
       " 136,\n",
       " 74,\n",
       " 307,\n",
       " 262,\n",
       " 157,\n",
       " 165,\n",
       " 153,\n",
       " 179,\n",
       " 6,\n",
       " 34,\n",
       " 149,\n",
       " 214,\n",
       " 8,\n",
       " 333,\n",
       " 2,\n",
       " 297,\n",
       " 82,\n",
       " 18,\n",
       " 326,\n",
       " 297,\n",
       " 204,\n",
       " 34,\n",
       " 19,\n",
       " 280,\n",
       " 19,\n",
       " 124,\n",
       " 230,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 79,\n",
       " 17,\n",
       " 20,\n",
       " 199,\n",
       " 204,\n",
       " 129,\n",
       " 297,\n",
       " 294,\n",
       " 133,\n",
       " 296,\n",
       " 311,\n",
       " 225,\n",
       " 20,\n",
       " 322,\n",
       " 75,\n",
       " 164,\n",
       " 6,\n",
       " 60,\n",
       " 245,\n",
       " 169,\n",
       " 165,\n",
       " 20,\n",
       " 322,\n",
       " 46,\n",
       " 234,\n",
       " 8,\n",
       " 337,\n",
       " 168,\n",
       " 333,\n",
       " 188,\n",
       " 304,\n",
       " 253,\n",
       " 33,\n",
       " 108,\n",
       " 148,\n",
       " 226,\n",
       " 307,\n",
       " 345,\n",
       " 6,\n",
       " 272,\n",
       " 163,\n",
       " 132,\n",
       " 37,\n",
       " 122,\n",
       " 337,\n",
       " 42,\n",
       " 307,\n",
       " 59,\n",
       " 297,\n",
       " 201,\n",
       " 6,\n",
       " 196,\n",
       " 341,\n",
       " 348,\n",
       " 152,\n",
       " 34,\n",
       " 290,\n",
       " 4,\n",
       " 185,\n",
       " 156,\n",
       " 1,\n",
       " 195,\n",
       " 5,\n",
       " 6,\n",
       " 60,\n",
       " 300,\n",
       " 38,\n",
       " 142,\n",
       " 34,\n",
       " 46,\n",
       " 328,\n",
       " 220,\n",
       " 189,\n",
       " 28,\n",
       " 315,\n",
       " 220,\n",
       " 122,\n",
       " 6,\n",
       " 34,\n",
       " 301,\n",
       " 128,\n",
       " 173,\n",
       " 86,\n",
       " 208,\n",
       " 276,\n",
       " 304,\n",
       " 226,\n",
       " 76,\n",
       " 8,\n",
       " 302,\n",
       " 263,\n",
       " 307,\n",
       " 150,\n",
       " 293,\n",
       " 304,\n",
       " 246,\n",
       " 209,\n",
       " 72,\n",
       " 6,\n",
       " 60,\n",
       " 113,\n",
       " 169,\n",
       " 295,\n",
       " 8,\n",
       " 277,\n",
       " 333,\n",
       " 38,\n",
       " 297,\n",
       " 248,\n",
       " 341,\n",
       " 297,\n",
       " 204,\n",
       " 18,\n",
       " 331,\n",
       " 6,\n",
       " 170,\n",
       " 186,\n",
       " 247,\n",
       " 168,\n",
       " 296,\n",
       " 169,\n",
       " 2,\n",
       " 271,\n",
       " 309,\n",
       " 172,\n",
       " 8,\n",
       " 169,\n",
       " 282,\n",
       " 221,\n",
       " 19,\n",
       " 216,\n",
       " 19,\n",
       " 60,\n",
       " 299,\n",
       " 95,\n",
       " 167,\n",
       " 304,\n",
       " 19,\n",
       " 116,\n",
       " 19,\n",
       " 342,\n",
       " 165,\n",
       " 337,\n",
       " 347,\n",
       " 6,\n",
       " 40,\n",
       " 33,\n",
       " 43,\n",
       " 194,\n",
       " 6,\n",
       " 150,\n",
       " 215,\n",
       " 164,\n",
       " 333,\n",
       " 2,\n",
       " 141,\n",
       " 225,\n",
       " 8,\n",
       " 300,\n",
       " 38,\n",
       " 96,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 64,\n",
       " 70,\n",
       " 45,\n",
       " 130,\n",
       " 297,\n",
       " 81,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 229,\n",
       " 339,\n",
       " 183,\n",
       " 180,\n",
       " 297,\n",
       " 81,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 286,\n",
       " 36,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 91,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 20,\n",
       " 184,\n",
       " 220,\n",
       " 65,\n",
       " 260,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 308,\n",
       " 220,\n",
       " 330,\n",
       " 303,\n",
       " 296,\n",
       " 147,\n",
       " 6,\n",
       " 34,\n",
       " 203,\n",
       " 220,\n",
       " 169,\n",
       " 168,\n",
       " 271,\n",
       " 217,\n",
       " 114,\n",
       " 8,\n",
       " 218,\n",
       " 163,\n",
       " 240,\n",
       " 92,\n",
       " 208,\n",
       " 198,\n",
       " 312,\n",
       " 307,\n",
       " 317,\n",
       " 20,\n",
       " 121,\n",
       " 110,\n",
       " 218,\n",
       " 34,\n",
       " 299,\n",
       " 6,\n",
       " 60,\n",
       " 335,\n",
       " 28,\n",
       " 169,\n",
       " 93,\n",
       " 168,\n",
       " 137,\n",
       " 190,\n",
       " 297,\n",
       " 259,\n",
       " 69,\n",
       " 231,\n",
       " 34,\n",
       " 231,\n",
       " 26,\n",
       " 6,\n",
       " 163,\n",
       " 135,\n",
       " 175,\n",
       " 220,\n",
       " 117,\n",
       " 320,\n",
       " 25,\n",
       " 20,\n",
       " 338,\n",
       " 6,\n",
       " 337,\n",
       " 168,\n",
       " 304,\n",
       " 121,\n",
       " 2,\n",
       " 54,\n",
       " 247,\n",
       " 8,\n",
       " 169,\n",
       " 2,\n",
       " 219,\n",
       " 143,\n",
       " 304,\n",
       " 53,\n",
       " 261,\n",
       " 307,\n",
       " 155,\n",
       " 6,\n",
       " 60,\n",
       " 169,\n",
       " 265,\n",
       " 307,\n",
       " 325,\n",
       " 307,\n",
       " 155,\n",
       " 169,\n",
       " 71,\n",
       " 319,\n",
       " 170,\n",
       " 123,\n",
       " 125,\n",
       " 200,\n",
       " 8,\n",
       " 34,\n",
       " 92,\n",
       " 302,\n",
       " 187,\n",
       " 303,\n",
       " 106,\n",
       " 6,\n",
       " 305,\n",
       " 228,\n",
       " 108,\n",
       " 103,\n",
       " 6,\n",
       " 165,\n",
       " 297,\n",
       " 192,\n",
       " 18,\n",
       " 217,\n",
       " 251,\n",
       " 8,\n",
       " 297,\n",
       " 256,\n",
       " 236,\n",
       " 168,\n",
       " 296,\n",
       " 297,\n",
       " 39,\n",
       " 34,\n",
       " 163,\n",
       " 57,\n",
       " 89,\n",
       " 225,\n",
       " 127,\n",
       " 180,\n",
       " 304,\n",
       " 6,\n",
       " 277,\n",
       " 329,\n",
       " 24,\n",
       " 120,\n",
       " 203,\n",
       " 220,\n",
       " 169,\n",
       " 230,\n",
       " 61,\n",
       " 297,\n",
       " 146,\n",
       " 244,\n",
       " 6,\n",
       " 277,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 287,\n",
       " 25,\n",
       " 296,\n",
       " 86,\n",
       " 281,\n",
       " 307,\n",
       " 187,\n",
       " 20,\n",
       " 182,\n",
       " 55,\n",
       " 220,\n",
       " 266,\n",
       " 6,\n",
       " 60,\n",
       " 169,\n",
       " 284,\n",
       " 86,\n",
       " 208,\n",
       " 297,\n",
       " 187,\n",
       " 297,\n",
       " 121,\n",
       " 28,\n",
       " 296,\n",
       " 202,\n",
       " 106,\n",
       " 8,\n",
       " 163,\n",
       " 144,\n",
       " 297,\n",
       " 58,\n",
       " 181,\n",
       " 341,\n",
       " 205,\n",
       " 180,\n",
       " 304,\n",
       " 168,\n",
       " 296,\n",
       " 347,\n",
       " 268,\n",
       " 31,\n",
       " 187,\n",
       " 292,\n",
       " 296,\n",
       " 297,\n",
       " 43,\n",
       " 168,\n",
       " 19,\n",
       " 167,\n",
       " 169,\n",
       " 19,\n",
       " 108,\n",
       " 51,\n",
       " 302,\n",
       " 38,\n",
       " 138,\n",
       " 297,\n",
       " 261,\n",
       " 238,\n",
       " 307,\n",
       " 104,\n",
       " 348,\n",
       " 342,\n",
       " 220,\n",
       " 316,\n",
       " 8,\n",
       " 163,\n",
       " 191,\n",
       " 6,\n",
       " 269,\n",
       " 193,\n",
       " 257,\n",
       " 254,\n",
       " 44,\n",
       " 130,\n",
       " 324,\n",
       " 129,\n",
       " 21,\n",
       " 11,\n",
       " 200,\n",
       " 306,\n",
       " 297,\n",
       " 204,\n",
       " 168,\n",
       " 173,\n",
       " 241,\n",
       " 178,\n",
       " 0,\n",
       " 0,\n",
       " 224,\n",
       " 6,\n",
       " 329,\n",
       " 135,\n",
       " 169,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 300,\n",
       " 38,\n",
       " 239,\n",
       " 66,\n",
       " 153,\n",
       " 34,\n",
       " 329,\n",
       " 92,\n",
       " 208,\n",
       " 176,\n",
       " 339,\n",
       " 302,\n",
       " 38,\n",
       " 8,\n",
       " 92,\n",
       " 329,\n",
       " 251,\n",
       " 210,\n",
       " 307,\n",
       " 262,\n",
       " 169,\n",
       " 231,\n",
       " 34,\n",
       " 231,\n",
       " 26,\n",
       " 18,\n",
       " 162,\n",
       " 21,\n",
       " 139,\n",
       " 321,\n",
       " 88,\n",
       " 260,\n",
       " 222,\n",
       " 131,\n",
       " 166,\n",
       " 167,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 287,\n",
       " 141,\n",
       " 94,\n",
       " 165,\n",
       " 297,\n",
       " 204,\n",
       " 18,\n",
       " 35,\n",
       " 6,\n",
       " 297,\n",
       " 289,\n",
       " 310,\n",
       " 304,\n",
       " 121,\n",
       " 44,\n",
       " 130,\n",
       " 170,\n",
       " 90,\n",
       " 34,\n",
       " 67,\n",
       " 169,\n",
       " 320,\n",
       " 298,\n",
       " 6,\n",
       " 34,\n",
       " 169,\n",
       " 270,\n",
       " 8,\n",
       " 300,\n",
       " 197,\n",
       " 3,\n",
       " 50,\n",
       " 20,\n",
       " 246,\n",
       " 83,\n",
       " 294,\n",
       " 199,\n",
       " 204,\n",
       " 165,\n",
       " 154,\n",
       " 279,\n",
       " 6,\n",
       " 60,\n",
       " 163,\n",
       " 144,\n",
       " 19,\n",
       " 297,\n",
       " 291,\n",
       " 19,\n",
       " 84,\n",
       " 296,\n",
       " 313,\n",
       " 169,\n",
       " 167,\n",
       " 20,\n",
       " 206,\n",
       " 323,\n",
       " 341,\n",
       " 182,\n",
       " 100,\n",
       " 6,\n",
       " 343,\n",
       " 187,\n",
       " 202,\n",
       " 266,\n",
       " 8,\n",
       " 297,\n",
       " 23,\n",
       " 38,\n",
       " 246,\n",
       " 142,\n",
       " 129,\n",
       " 297,\n",
       " 203,\n",
       " 236,\n",
       " 6,\n",
       " 30,\n",
       " 332,\n",
       " 52,\n",
       " 173,\n",
       " 264,\n",
       " 307,\n",
       " 47,\n",
       " 242,\n",
       " 297,\n",
       " 111,\n",
       " 259,\n",
       " 63,\n",
       " 296,\n",
       " 151,\n",
       " 86,\n",
       " 165,\n",
       " 32,\n",
       " 48,\n",
       " 6,\n",
       " 227,\n",
       " 165,\n",
       " 20,\n",
       " 212,\n",
       " 211,\n",
       " 8,\n",
       " 60,\n",
       " 207,\n",
       " 54,\n",
       " 177,\n",
       " 140,\n",
       " 230,\n",
       " 307,\n",
       " 257,\n",
       " 6,\n",
       " 339,\n",
       " 159,\n",
       " 153,\n",
       " 233,\n",
       " 306,\n",
       " 297,\n",
       " 107,\n",
       " 121,\n",
       " 6,\n",
       " 34,\n",
       " 24,\n",
       " 149,\n",
       " 347,\n",
       " 118,\n",
       " 153,\n",
       " 63,\n",
       " 2,\n",
       " 318,\n",
       " 8,\n",
       " 232,\n",
       " 6,\n",
       " 297,\n",
       " 121,\n",
       " 93,\n",
       " 208,\n",
       " 283,\n",
       " 49,\n",
       " 169,\n",
       " 93,\n",
       " 208,\n",
       " 105,\n",
       " 6,\n",
       " 169,\n",
       " 2,\n",
       " 73,\n",
       " 6,\n",
       " 169,\n",
       " 250,\n",
       " 112,\n",
       " 34,\n",
       " 169,\n",
       " 119,\n",
       " 246,\n",
       " 252,\n",
       " 129,\n",
       " 203,\n",
       " 220,\n",
       " 170,\n",
       " 255,\n",
       " 6,\n",
       " 85,\n",
       " 20,\n",
       " 246,\n",
       " 75,\n",
       " 102,\n",
       " 34,\n",
       " 115,\n",
       " 307,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 78,\n",
       " 296,\n",
       " 62,\n",
       " 51,\n",
       " 169,\n",
       " 8,\n",
       " 223,\n",
       " 6,\n",
       " 34,\n",
       " 61,\n",
       " 297,\n",
       " 327,\n",
       " 6,\n",
       " 304,\n",
       " 168,\n",
       " 217,\n",
       " 20,\n",
       " 160,\n",
       " 228,\n",
       " 294,\n",
       " 275,\n",
       " 126,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 169,\n",
       " 2,\n",
       " 173,\n",
       " 235,\n",
       " 307,\n",
       " 183,\n",
       " 296,\n",
       " 327,\n",
       " 49,\n",
       " 278,\n",
       " 168,\n",
       " 35,\n",
       " 41,\n",
       " 296,\n",
       " 297,\n",
       " 134,\n",
       " 168,\n",
       " 284,\n",
       " 161,\n",
       " 341,\n",
       " 297,\n",
       " 174,\n",
       " 8,\n",
       " 169,\n",
       " 29,\n",
       " 344,\n",
       " 249,\n",
       " 314,\n",
       " 346,\n",
       " 27,\n",
       " 34,\n",
       " 149,\n",
       " 50,\n",
       " 273,\n",
       " 225,\n",
       " 297,\n",
       " 267,\n",
       " 109,\n",
       " 272,\n",
       " 8,\n",
       " 334,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 274,\n",
       " 169,\n",
       " 0,\n",
       " 336,\n",
       " 2,\n",
       " 171,\n",
       " 70,\n",
       " 130,\n",
       " 18,\n",
       " 20,\n",
       " 213,\n",
       " 220,\n",
       " 101,\n",
       " 288,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 56,\n",
       " 340,\n",
       " 10,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 80,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 80,\n",
       " 17,\n",
       " 258,\n",
       " 4,\n",
       " 13,\n",
       " 5,\n",
       " 7,\n",
       " 185,\n",
       " 156,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 195,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 229,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 7,\n",
       " 285,\n",
       " 220,\n",
       " 99,\n",
       " 4,\n",
       " 15,\n",
       " 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset[0]['X']  # First review in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset[0]['Y']  # Label of the first review in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Word2Vec Training\n",
    "\n",
    "Word2Vec has two training variants:\n",
    "\n",
    " - **Continuous Bag of Words (CBOW)**: Predict center word from (bag of) context words.\n",
    " - **Skip-grams**: Predict context words given center word.\n",
    "  \n",
    "Visually, they look like this:\n",
    "\n",
    "<img src=\"https://ibin.co/4UIznsOEyH7t.png\" width=\"500\">\n",
    "\n",
    "## 3.1.1. CBOW\n",
    "\n",
    "CBOW windows through the sentence and picks out the center word as the `Y` and the surrounding context words as the inputs `X`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_window(sequence, n=1):\n",
    "    \"\"\"\n",
    "    From http://stackoverflow.com/q/42220614/610569\n",
    "        >>> list(per_window([1,2,3,4], n=2))\n",
    "        [(1, 2), (2, 3), (3, 4)]\n",
    "        >>> list(per_window([1,2,3,4], n=3))\n",
    "        [(1, 2, 3), (2, 3, 4)]\n",
    "    \"\"\"\n",
    "    start, stop = 0, n\n",
    "    seq = list(sequence)\n",
    "    while stop <= len(seq):\n",
    "        yield seq[start:stop]\n",
    "        start += 1\n",
    "        stop += 1\n",
    "\n",
    "def cbow_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1\n",
    "    for window in per_window(tokens, n):\n",
    "        target = window.pop(window_size)\n",
    "        yield window, target   # X = window ; Y = target. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent0 = ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', \n",
    "         'and', 'language', 'is', 'essentially', 'non-random', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'choose', 'words'], 'never'),\n",
       " (['users', 'never', 'words', 'randomly'], 'choose'),\n",
       " (['never', 'choose', 'randomly', ','], 'words'),\n",
       " (['choose', 'words', ',', 'and'], 'randomly'),\n",
       " (['words', 'randomly', 'and', 'language'], ','),\n",
       " (['randomly', ',', 'language', 'is'], 'and'),\n",
       " ([',', 'and', 'is', 'essentially'], 'language'),\n",
       " (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
       " (['language', 'is', 'non-random', '.'], 'essentially')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'never', 'words', 'randomly', ','], 'choose'),\n",
       " (['users', 'never', 'choose', 'randomly', ',', 'and'], 'words'),\n",
       " (['never', 'choose', 'words', ',', 'and', 'language'], 'randomly'),\n",
       " (['choose', 'words', 'randomly', 'and', 'language', 'is'], ','),\n",
       " (['words', 'randomly', ',', 'language', 'is', 'essentially'], 'and'),\n",
       " (['randomly', ',', 'and', 'is', 'essentially', 'non-random'], 'language'),\n",
       " ([',', 'and', 'language', 'essentially', 'non-random', '.'], 'is')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.2. Skipgram\n",
    "\n",
    "Skipgram training windows through the sentence and pictures out the center word as the input `X` and the context words as the outputs `Y`, additionally, it will randommly sample words not in the window as **negative samples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1 \n",
    "    for i, window in enumerate(per_window(tokens, n)):\n",
    "        target = window.pop(window_size)\n",
    "        # Generate positive samples.\n",
    "        for context_word in window:\n",
    "            yield target, context_word, 1\n",
    "        # Generate negative samples.\n",
    "        for _ in range(n-1):\n",
    "            leftovers = tokens[:i] + tokens[i+n:]\n",
    "            yield target, random.choice(leftovers), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language users never choose words randomly , and language is essentially non-random .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('never', 'language', 1),\n",
       " ('never', 'users', 1),\n",
       " ('never', 'choose', 1),\n",
       " ('never', 'words', 1),\n",
       " ('never', 'non-random', 0),\n",
       " ('never', 'essentially', 0),\n",
       " ('never', 'and', 0),\n",
       " ('never', 'essentially', 0),\n",
       " ('choose', 'users', 1),\n",
       " ('choose', 'never', 1),\n",
       " ('choose', 'words', 1),\n",
       " ('choose', 'randomly', 1),\n",
       " ('choose', 'and', 0),\n",
       " ('choose', 'language', 0),\n",
       " ('choose', ',', 0),\n",
       " ('choose', 'non-random', 0),\n",
       " ('words', 'never', 1),\n",
       " ('words', 'choose', 1),\n",
       " ('words', 'randomly', 1),\n",
       " ('words', ',', 1),\n",
       " ('words', 'users', 0),\n",
       " ('words', '.', 0),\n",
       " ('words', 'essentially', 0),\n",
       " ('words', 'language', 0),\n",
       " ('randomly', 'choose', 1),\n",
       " ('randomly', 'words', 1),\n",
       " ('randomly', ',', 1),\n",
       " ('randomly', 'and', 1),\n",
       " ('randomly', 'language', 0),\n",
       " ('randomly', 'language', 0),\n",
       " ('randomly', 'is', 0),\n",
       " ('randomly', 'never', 0),\n",
       " (',', 'words', 1),\n",
       " (',', 'randomly', 1),\n",
       " (',', 'and', 1),\n",
       " (',', 'language', 1),\n",
       " (',', 'is', 0),\n",
       " (',', 'non-random', 0),\n",
       " (',', 'non-random', 0),\n",
       " (',', '.', 0),\n",
       " ('and', 'randomly', 1),\n",
       " ('and', ',', 1),\n",
       " ('and', 'language', 1),\n",
       " ('and', 'is', 1),\n",
       " ('and', 'users', 0),\n",
       " ('and', 'non-random', 0),\n",
       " ('and', 'language', 0),\n",
       " ('and', 'users', 0),\n",
       " ('language', ',', 1),\n",
       " ('language', 'and', 1),\n",
       " ('language', 'is', 1),\n",
       " ('language', 'essentially', 1),\n",
       " ('language', 'users', 0),\n",
       " ('language', 'users', 0),\n",
       " ('language', '.', 0),\n",
       " ('language', 'never', 0),\n",
       " ('is', 'and', 1),\n",
       " ('is', 'language', 1),\n",
       " ('is', 'essentially', 1),\n",
       " ('is', 'non-random', 1),\n",
       " ('is', 'never', 0),\n",
       " ('is', 'choose', 0),\n",
       " ('is', 'words', 0),\n",
       " ('is', 'randomly', 0),\n",
       " ('essentially', 'language', 1),\n",
       " ('essentially', 'is', 1),\n",
       " ('essentially', 'non-random', 1),\n",
       " ('essentially', '.', 1),\n",
       " ('essentially', 'choose', 0),\n",
       " ('essentially', 'words', 0),\n",
       " ('essentially', 'and', 0),\n",
       " ('essentially', ',', 0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(' '.join(sent0))\n",
    "list(skipgram_iterator(sent0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.3 Word2Vec Dataset\n",
    "\n",
    "Now that we know what are the inputs `X` and outputs `Y` of the Word2Vec task. \n",
    "\n",
    "Lets put everything together and modify the `Dataset` so that `__getitem__` retrieves CBOW or Skipgram formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import functional as F\n",
    "\n",
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        \"\"\"\n",
    "            >>> tokens = ['language', 'users', 'never', 'choose', 'words', 'randomly', \n",
    "            ...           ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n",
    "            >>> cbow_iterator(tokens, 2)\n",
    "            [(['language', 'users', 'choose', 'words'], 'never'),\n",
    "            (['users', 'never', 'words', 'randomly'], 'choose'),\n",
    "            (['never', 'choose', 'randomly', ','], 'words'),\n",
    "            (['choose', 'words', ',', 'and'], 'randomly'),\n",
    "            (['words', 'randomly', 'and', 'language'], ','),\n",
    "            (['randomly', ',', 'language', 'is'], 'and'),\n",
    "            ([',', 'and', 'is', 'essentially'], 'language'),\n",
    "            (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
    "            (['language', 'is', 'non-random', '.'], 'essentially')]\n",
    "        \"\"\"\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        \"\"\"\n",
    "            >>> tokens = ['language', 'users', 'never', 'choose', 'words', 'randomly', \n",
    "            ...           ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n",
    "            >>> list(skipgram_iterator(tokens, 2))[:10]\n",
    "            [('never', 'language', 1),\n",
    "             ('never', 'users', 1),\n",
    "             ('never', 'choose', 1),\n",
    "             ('never', 'words', 1),\n",
    "             ('never', 'non-random', 0),\n",
    "             ('never', 'is', 0),\n",
    "             ('never', 'and', 0),\n",
    "             ('never', 'and', 0),\n",
    "             ('choose', 'users', 1),\n",
    "             ('choose', 'never', 1)]\n",
    "        \"\"\"\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.5. Train a CBOW model\n",
    "\n",
    "### Lets Get Some Data\n",
    "\n",
    "Lets take Kilgarriff (2005) , \"Language is never ever, ever random\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
    "if os.path.isfile('language-never-random.txt'):\n",
    "    with open('language-never-random.txt') as fin:\n",
    "        text = fin.read()\n",
    "else:\n",
    "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
    "    text = requests.get(url).content.decode('utf8')\n",
    "    with open('language-never-random.txt', 'w') as fout:\n",
    "        fout.write(text)\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check, lets take a look at the data.\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bulk of linguistic questions concern the dis- tinction between a and m. a linguistic account of a phenomenon gen- erally gives us reason to view the relation between , for example , a verb ’ s syntax and its semantics , as motivated rather than arbitrary .\n",
      "\u001b[92mof\u001b[0m \t\t the bulk \u001b[91m______\u001b[0m linguistic questions\n",
      "\u001b[92mlinguistic\u001b[0m \t bulk of \u001b[91m______\u001b[0m questions concern\n",
      "\u001b[92mquestions\u001b[0m \t of linguistic \u001b[91m______\u001b[0m concern the\n",
      "\u001b[92mconcern\u001b[0m \t linguistic questions \u001b[91m______\u001b[0m the dis-\n",
      "\u001b[92mthe\u001b[0m \t\t questions concern \u001b[91m______\u001b[0m dis- tinction\n",
      "\u001b[92mdis-\u001b[0m \t\t concern the \u001b[91m______\u001b[0m tinction between\n",
      "\u001b[92mtinction\u001b[0m \t the dis- \u001b[91m______\u001b[0m between a\n",
      "\u001b[92mbetween\u001b[0m \t dis- tinction \u001b[91m______\u001b[0m a and\n",
      "\u001b[92ma\u001b[0m \t\t tinction between \u001b[91m______\u001b[0m and m.\n",
      "\u001b[92mand\u001b[0m \t\t between a \u001b[91m______\u001b[0m m. a\n",
      "\u001b[92mm.\u001b[0m \t\t a and \u001b[91m______\u001b[0m a linguistic\n",
      "\u001b[92ma\u001b[0m \t\t and m. \u001b[91m______\u001b[0m linguistic account\n",
      "\u001b[92mlinguistic\u001b[0m \t m. a \u001b[91m______\u001b[0m account of\n",
      "\u001b[92maccount\u001b[0m \t a linguistic \u001b[91m______\u001b[0m of a\n",
      "\u001b[92mof\u001b[0m \t\t linguistic account \u001b[91m______\u001b[0m a phenomenon\n",
      "\u001b[92ma\u001b[0m \t\t account of \u001b[91m______\u001b[0m phenomenon gen-\n",
      "\u001b[92mphenomenon\u001b[0m \t of a \u001b[91m______\u001b[0m gen- erally\n",
      "\u001b[92mgen-\u001b[0m \t\t a phenomenon \u001b[91m______\u001b[0m erally gives\n",
      "\u001b[92merally\u001b[0m \t\t phenomenon gen- \u001b[91m______\u001b[0m gives us\n",
      "\u001b[92mgives\u001b[0m \t\t gen- erally \u001b[91m______\u001b[0m us reason\n",
      "\u001b[92mus\u001b[0m \t\t erally gives \u001b[91m______\u001b[0m reason to\n",
      "\u001b[92mreason\u001b[0m \t\t gives us \u001b[91m______\u001b[0m to view\n",
      "\u001b[92mto\u001b[0m \t\t us reason \u001b[91m______\u001b[0m view the\n",
      "\u001b[92mview\u001b[0m \t\t reason to \u001b[91m______\u001b[0m the relation\n",
      "\u001b[92mthe\u001b[0m \t\t to view \u001b[91m______\u001b[0m relation between\n",
      "\u001b[92mrelation\u001b[0m \t view the \u001b[91m______\u001b[0m between ,\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91m______\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91m______\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91m______\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91m______\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91m______\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91m______\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[91m______\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
      "\u001b[92mand\u001b[0m \t\t s syntax \u001b[91m______\u001b[0m its semantics\n",
      "\u001b[92mits\u001b[0m \t\t syntax and \u001b[91m______\u001b[0m semantics ,\n",
      "\u001b[92msemantics\u001b[0m \t and its \u001b[91m______\u001b[0m , as\n",
      "\u001b[92m,\u001b[0m \t\t its semantics \u001b[91m______\u001b[0m as motivated\n",
      "\u001b[92mas\u001b[0m \t\t semantics , \u001b[91m______\u001b[0m motivated rather\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91m______\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91m______\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91m______\u001b[0m arbitrary .\n"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "def visualize_predictions(x, y, prediction, vocab, window_size, unk='<unk>'):\n",
    "    left = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[:window_size]])\n",
    "    right = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[window_size:]])\n",
    "    target = vocab.get(int(y), '<unk>')\n",
    "\n",
    "    if not prediction:\n",
    "        predicted_word = '______'\n",
    "    else:\n",
    "        predicted_word = vocab.get(int(prediction), '<unk>') \n",
    "    print(color_str(target, 'green'), '\\t' if len(target) > 6 else '\\t\\t', \n",
    "          left, color_str(predicted_word, 'green' if target == predicted_word else 'red'), right)\n",
    "    \n",
    "\n",
    "sent_idx = 10\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')\n",
    "print(' '.join(w2v_dataset.sents[sent_idx]))\n",
    "for w2v_io in w2v_dataset[sent_idx]:\n",
    "    context = x = tensor(w2v_io['x'])\n",
    "    target = y = tensor(w2v_io['y'])\n",
    "    visualize_predictions(context, target, None, w2v_dataset.vocab, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: tensor([ 35, 118,  31, 129])\n",
      "embedding: torch.Size([4, 500])\n",
      "flatten: torch.Size([1, 2000])\n"
     ]
    }
   ],
   "source": [
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')\n",
    "# print(' '.join(w2v_dataset.sents[sent_idx]))\n",
    "\n",
    "embedding = nn.Embedding(300, 500)\n",
    "for w2v_io in w2v_dataset[sent_idx]:\n",
    "    context = x = tensor(w2v_io['x'])\n",
    "    target = y = tensor(w2v_io['y'])\n",
    "    print(\"context:\", context)\n",
    "    print(\"embedding:\", embedding(context).shape)\n",
    "    print(\"flatten:\", embedding(context).view((1, -1)).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill-in the code for the CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Flatten the embeddings from multiple context words\n",
    "        # into a single vector.\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        # Apply the first linear layer and activation.\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        # Apply a second linear and predict\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        # Return the weights\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211, 24)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenized_text_train, tokenized_text_test = train_test_split(tokenized_text, test_size=0.1, random_state=42)\n",
    "len(tokenized_text_train), len(tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:15<00:00,  5.89s/it]\n"
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.003\n",
    "hidden_size = 100\n",
    "window_size = 2\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x = tensor(w2v_io['x'])\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.long))\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply and Evaluate the CBOW Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[92mis\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mto\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91m______\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91massociation\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mis\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91msimple\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[92m(\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91msocial\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91muncorrelated\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mdefined\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[91mof\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91msample\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mthe\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91maccurate\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91m’\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mof\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91malso\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91mdistribution\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[92marbitrary\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91mand\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[92mmotivated\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mf\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m)\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91m1977\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mrens\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91massociated\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mto\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91m______\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91mwe\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mare\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mthey\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91msee\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m______\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91m(\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mh0\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mno\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91mto\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mand\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mword\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91msimple\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91minappropriate\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mgiven\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[91meach\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mhave\u001b[0m out to\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mis\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mtwo\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91m______\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mtrue\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mthat\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mwords\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mrelative\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mcorpus\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[91mare\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mof\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mprobability\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mthey\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91ma\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mby\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91msubcorpus\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91m______\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[92mto\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mreject\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mlinguistically\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mx\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91macquisition\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mchi-square\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mlexicon\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mprobability\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mmutual\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91mat\u001b[0m ⫺ 0.5\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mrandom\u001b[0m greater than\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m0.5\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91muse\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91my\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91maccurately\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mof\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mdata\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mtest\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mnever\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91mlikely\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[92mwe\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mor\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mevidence\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mnever\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mthat\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91m______\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mverb\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mfollowing\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mcells\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mthe\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mis\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91mestimate\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91mis\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m is overlooked\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[92mlinguistics\u001b[0m 16 (\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91mor\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91mnot\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91mby\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91marbitrary\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mas\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mof\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[92mnot\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mone\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[92mhave\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[91mto\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mso\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[92mthat\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mrole\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhere\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[92mit\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mof\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91mfrom\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mand\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mthey\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91msubcategorization\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[91m______\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[92mrandom\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91mbecause\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mthe\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mshall\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91min\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mit\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mthey\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mdoes\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mvery\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mcapable\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mcatf\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91mdatasets\u001b[0m and the\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mwith\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[92mthe\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mare\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mmore\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mcells\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91m:\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91min\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mnumber\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mthat\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91msubcorpora\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mcorpus\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mfrom\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mthe\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mand\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91mstrategy\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mdefinition\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91marbitrary\u001b[0m , as\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mhere\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mthat\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mand\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[92merror\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[91msize\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91mkilgarriff\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mthere\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91m______\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91miff\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91marbitrary\u001b[0m , be\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mare\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mreject\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mhigh\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[91m(\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91m______\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mor\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91mthe\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[92m%\u001b[0m of them\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mthe\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91m)\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mjust\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[92mthe\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mfilter\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91mthese\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91meach\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91ma\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mwhere\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91m______\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mprobabilities\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91mtwo\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91m…\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2723404255319149\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go back to the 90th Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CBOW(\n",
       "    (embeddings): Embedding(1303, 100)\n",
       "    (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=1303, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_90 = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
    "model_90 = torch.nn.DataParallel(model_90)\n",
    "model_90.load_state_dict(torch.load('cbow_checkpoint_90.pt'))\n",
    "model_90.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[92mis\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mto\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91m______\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91massociation\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mis\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91msimple\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[92m(\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91msocial\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91muncorrelated\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mdefined\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[91mof\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91msample\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mthe\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91maccurate\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91m’\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mof\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91malso\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91mdistribution\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[92marbitrary\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91mand\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[92mmotivated\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mf\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m)\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91m1977\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mrens\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91massociated\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mto\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91m______\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91mwe\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91mare\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mthey\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91msee\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m______\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91m(\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mh0\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mno\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91mto\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mand\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mword\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91msimple\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91minappropriate\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mgiven\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[91meach\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mhave\u001b[0m out to\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91mis\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mtwo\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91m______\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mtrue\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mthat\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mwords\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mrelative\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mcorpus\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[91mare\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mof\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mprobability\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mthey\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91ma\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mby\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91msubcorpus\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91m______\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[92mto\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mreject\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mlinguistically\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mx\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91macquisition\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mchi-square\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mlexicon\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mprobability\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mmutual\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91mat\u001b[0m ⫺ 0.5\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mrandom\u001b[0m greater than\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m0.5\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91muse\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91my\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91maccurately\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mof\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mdata\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mtest\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mnever\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91mlikely\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[92mwe\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mor\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mevidence\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mnever\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mthat\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91m______\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mverb\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mfollowing\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mcells\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mthe\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mis\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91mestimate\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91mis\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m is overlooked\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[92mlinguistics\u001b[0m 16 (\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91mor\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91mnot\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91mby\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91marbitrary\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mas\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91malthough\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[92mnot\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mone\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[92mhave\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[91mto\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mso\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[92mthat\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mrole\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhere\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[92mit\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mof\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91mfrom\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mand\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mthey\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91msubcategorization\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[91m______\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[92mrandom\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91mbecause\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mthere\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mshall\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91min\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mit\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mthey\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mdoes\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mvery\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mcapable\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mcatf\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91mdatasets\u001b[0m and the\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mwith\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[92mthe\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mare\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mmore\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mcells\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91m:\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91min\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mnumber\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mthat\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91msubcorpora\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mcorpus\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mfrom\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mthe\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mand\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91mstrategy\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mdefinition\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91marbitrary\u001b[0m , as\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mhere\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mthat\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mand\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[92merror\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[91msize\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91mkilgarriff\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mthere\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91m______\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91miff\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91marbitrary\u001b[0m , be\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mare\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mreject\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mhigh\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[91m(\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91m______\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mor\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91mthe\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[92m%\u001b[0m of them\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mthe\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91m)\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mjust\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[92mthe\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mfilter\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91mthese\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91meach\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91ma\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mwhere\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91m______\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mprobabilities\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91mtwo\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91m…\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model_90(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2723404255319149\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go back to the 10th Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CBOW(\n",
       "    (embeddings): Embedding(1303, 100)\n",
       "    (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=1303, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
    "model_10 = torch.nn.DataParallel(model_10)\n",
    "model_10.load_state_dict(torch.load('cbow_checkpoint_10.pt'))\n",
    "model_10.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[92mis\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mto\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91m______\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91mused\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mfor\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[92ma\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91msimple\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[92m(\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91mat\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91m)\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mrandom\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91m______\u001b[0m etc .\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[91mof\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91mvery\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91mthe\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91maccurate\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91m______\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mof\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91malso\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91mand\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[91mand\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91mand\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[92mmotivated\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mlanguage\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91m)\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mrandom\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mand\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91m:\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mto\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91m______\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[92mthey\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[91m(\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mthey\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91msee\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91m______\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91m(\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mh0\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mno\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91mto\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mand\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mand\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91msimple\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mb\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[92mfor\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91mbe\u001b[0m out to\n",
      "\u001b[92mwas\u001b[0m \t\t tion that \u001b[91m______\u001b[0m indistinguishable from\n",
      "\u001b[92mindistinguishable\u001b[0m \t that was \u001b[91mtwo\u001b[0m from one\n",
      "\u001b[92mfrom\u001b[0m \t\t was indistinguishable \u001b[91m______\u001b[0m one where\n",
      "\u001b[92mone\u001b[0m \t\t indistinguishable from \u001b[91mtrue\u001b[0m where the\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mthat\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mof\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mnull\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mfrequency\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[91mare\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[91mof\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mprobability\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mthey\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[91ma\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91mlanguage\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mcorpora\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91m______\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[91mcan\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mreject\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mthat\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mand\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91mtrue\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mlinguistics\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mof\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91mand\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mnull\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t ⫺ e \u001b[91mat\u001b[0m ⫺ 0.5\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mrandom\u001b[0m greater than\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[92mthe\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91msame\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mwill\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mand\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[91mof\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mrandom\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mrandom\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91mwill\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mand\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mor\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mevidence\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[92mis\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mnever\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mh0\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mnull\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mwith\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mprobability\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mthe\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mis\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91mlikelihood\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91mholds\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mprobability\u001b[0m is overlooked\n",
      "\u001b[92mlinguistics\u001b[0m \t compu- tational \u001b[91mand\u001b[0m 16 (\n",
      "\u001b[92m16\u001b[0m \t\t tational linguistics \u001b[91m19\u001b[0m ( 1\n",
      "\u001b[92m(\u001b[0m \t\t linguistics 16 \u001b[92m(\u001b[0m 1 )\n",
      "\u001b[92m1\u001b[0m \t\t 16 ( \u001b[92m1\u001b[0m ) ,\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91mor\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91mnot\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91mby\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91marbitrary\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mas\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[91mhave\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mof\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[91marbitrary\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mbe\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mdata\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[91mas\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91m______\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mso\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[92mthat\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91mstrong\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhere\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[92mit\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mof\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91mfor\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91mand\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mthey\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91msubcategorization\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[91mmore\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[92mrandom\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91mfor\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mlanguage\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91min\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mit\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mand\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[91mdoes\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mvery\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91mrandom\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mrandom\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91marbitrary\u001b[0m and the\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mwith\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[92mthe\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[92mhypothesis\u001b[0m test con-\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mfor\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mrandom\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mword\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91m:\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91min\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91mnumber\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mthat\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91maverage\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91mwith\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[92mexample\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mfor\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mthe\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mrandom\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[91mlarge\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n",
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91mfor\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91mas\u001b[0m , as\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91mrandom\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mis\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mand\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[91mconference\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[92merror\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[91msize\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91mkilgarriff\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[91mthere\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t ) 2 \u001b[91mand\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mby\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mleech\u001b[0m , be\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mrandom\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mfor\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mvery\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[91m(\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91m______\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mor\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91mthe\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[92m%\u001b[0m of them\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mthe\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91m)\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mrandom\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[92mthe\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mfilter\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91mthese\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91meach\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91ma\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mwhere\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91m______\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mprobabilities\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91mtwo\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91m…\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model_10(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2553191489361702\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Handle Unknown Words? \n",
    "\n",
    "This is not the best way to handle unknown words, but we can simply assign an index for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a', 1: 'bar', 2: 'foo', 3: 'is', 4: 'sentence', 5: 'this'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dictionary' object has no attribute 'patch_with_special_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e9d9ce063650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'this is a foo bar sentence'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspecial_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_with_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dictionary' object has no attribute 'patch_with_special_tokens'"
     ]
    }
   ],
   "source": [
    "# See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.patch_with_special_tokens\n",
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "vocab.patch_with_special_tokens(special_tokens)\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Rewrite the `Word2VecText` Object\n",
    "\n",
    "Now with the (i) unknown word patch in the vocabulary as well as (ii) `skipgram_iterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Add the unknown word patch here.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "        self.vocab.???(special_tokens)\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.???(tokens, unknown_word_index=1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.03\n",
    "hidden_size = 300\n",
    "window_size = 3\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='skipgram')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.MSELoss() # binary classification.\n",
    "model = SkipGram(vocab_size, embd_size,)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epcoh_loss = 0\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x1, x2 = w2v_io['x']\n",
    "            x1, x2 = tensor(x1), tensor(x2)\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.float))\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x1, x2)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epcoh_loss += float(loss)\n",
    "    torch.save(model.state_dict(), 'skipgram_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(epcoh_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        _, prediction =  torch.max(model(x1, x2), 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Collobert and Weston SENNA Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!mkdir -p .kaggle\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > .kaggle/kaggle.json\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!kaggle datasets download -d alvations/vegetables-senna-embeddings --force -p ./\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Rewrite the Word2VecText object with the pretrained embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Loads the pretrained keys. \n",
    "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
    "        self.vocab = Dictionary({})\n",
    "        self.vocab.token2id = pretrained_keys\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=-1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                if leftovers:\n",
    "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pretrained Embeddings on the Skipgram Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, pretrained_npy):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding.???(pretrained_npy)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = ???\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='skipgram')\n",
    "pretrained_npy = torch.tensor(np.load('senna.wiki-reuters.lm2.50d.npy'))\n",
    "pretrained_model = SkipGram(pretrained_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        pretrained_model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        if -1 in (x1, x2): # Skip unknown words.\n",
    "            continue\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        with torch.no_grad():\n",
    "            logprobs = pretrained_model(x1, x2)\n",
    "            _, prediction =  torch.max(logprobs, 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "    pretrained_keys = {line.strip():i for i, line in enumerate(fin)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pretrained Embeddings on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        # See https://pytorch.org/docs/stable/nn.html?highlight=from_pretrained#torch.nn.Embedding.from_pretrained\n",
    "        self.embeddings = nn.Embedding.???(pretrained_npy)\n",
    "        self.linear1 = nn.Linear(???, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, ???)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2 \n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze the Embedddings and Tune it on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        # https://pytorch.org/docs/stable/nn.html?highlight=from_pretrained#torch.nn.Embedding.from_pretrained\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy, ???=False)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2 \n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(pretrained_cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(pretrained_cbow_model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x = tensor(w2v_io['x'])\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.long))\n",
    "            \n",
    "            if -1 in x or int(y) == -1:\n",
    "                continue\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = pretrained_cbow_model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_finetuning_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
